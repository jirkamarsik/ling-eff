\chapter{Properties}
\label{chap:properties}

We have given a formal definition of $\banana{\lambda}$ in
Chapter~\ref{chap:definitions} and then demonstrated it on examples in
Chapter~\ref{chap:examples}. We now turn back to a study of
$\banana{\lambda}$ itself and we derive some of its formal properties.

\minitoc

\section{Derived Rules}
\label{sec:derived-rules}

At the end of Chapter~\ref{chap:definitions},
in~\ref{sec:common-combinators}, we have introduced some new syntax for
$\banana{\lambda}$ terms and we have translated that syntax into terms of
the core $\banana{\lambda}$ calculus. However, in
Chapter~\ref{chap:examples}, we have then seen that expanding these
syntactic extensions during the reduction of a term is a tedious process
and we have introduced some shortcuts to allow us to proceed faster and at
a higher level of abstraction (e.g.\ the $\eta.\hsbind$ rule). In this
section, we will give typing rules and reduction rules to these new
constructions and prove their correctness.


\subsection{Function Composition\texorpdfstring{ ($\compop$)}{}}
\label{ssec:function-composition}

The first piece of syntactic sugar we have introduced was an infix symbol
for function composition.

$$
f \compop g = \lam{x}{\ap{f}{(\ap{g}{x})}}
$$

In order to type terms containing this symbol, it will be useful to have a
typing rule.

\begin{proposition}
  The following typing rule is derivable in $\banana{\lambda}$:

  \begin{prooftree}
    \AxiomC{$\Gamma \vdash M : \beta \to \gamma$}
    \AxiomC{$\Gamma \vdash N : \alpha \to \beta$}
    \RightLabel{$[\compop]$}
    \BinaryInfC{$\Gamma \vdash M \compop N : \alpha \to \gamma$}
  \end{prooftree}
\end{proposition}

\begin{proof}
  Since $M \compop N = \lam{x}{\ap{M}{(\ap{N}{x})}}$, we can prove the
  validity of this rule with the typing rule below:
  
  \begin{prooftree}
    \AxiomC{$\Gamma, x : \alpha \vdash M : \beta \to \gamma$}
    \AxiomC{$\Gamma, x : \alpha \vdash N : \alpha \to \beta$}
    \AxiomC{$\Gamma, x : \alpha \vdash x : \alpha$}
    \RightLabel{[app]}
    \BinaryInfC{$\Gamma, x : \alpha \vdash \ap{N}{x} : \beta$}
    \RightLabel{[app]}
    \BinaryInfC{$\Gamma, x : \alpha \vdash \ap{M}{(\ap{N}{x})} : \gamma$}
    \RightLabel{[abs]}
    \UnaryInfC{$\Gamma \vdash \lam{x}{\ap{M}{(\ap{N}{x})}} : \alpha \to \gamma$}
  \end{prooftree}

  $x$ is presumed to be fresh for $M$ and $N$ and so we can equate
  $\Gamma, x : \alpha \vdash M : \beta \to \gamma$ with
  $\Gamma \vdash M : \beta \to \gamma$ and the same for $N$.
\end{proof}

The result of function composition is another function and functions can be
applied to arguments. We can derive a reduction rule for this kind of
function.

\begin{proposition}
  The following reduction is derivable in $\banana{\lambda}$:

  \vspace{2mm}
  \begin{tabular}{>{$}r<{$} >{$}c<{$} >{$}l<{$}}
    \ap{(M_1 \compop M_2)}{N} & \to_\compop & \ap{M_1}{(\ap{M_2}{N})} \\
  \end{tabular}
  \vspace{2mm}
\end{proposition}

\begin{proof}
  \begin{align*}
    \ap{(M_1 \compop M_2)}{N}
    &= \ap{(\lam{x}{\ap{M_1}{(\ap{M_2}{x})}})}{N} \\
    &\to_\beta \ap{M_1}{(\ap{M_2}{N})}
  \end{align*}
\end{proof}


\subsection{Monadic Bind\texorpdfstring{ ($\hsbind$)}{}}
\label{ssec:bind}

As a reminder, we give the definition of $\hsbind$
from~\ref{ssec:composing-functions}.

\begin{align*}
  M \hsbind N &= \ap{N^*}{M} \\
              &= \ap{\banana{\onto{\eta}{N}}}{M}
\end{align*}

First, we will prove the correct typing for $\hsbind$.

\begin{proposition}
  The following typing rule is derivable in $\banana{\lambda}$:

  \begin{prooftree}
    \AxiomC{$\Gamma \vdash M : \FF_E(\alpha)$}
    \AxiomC{$\Gamma \vdash N : \alpha \to \FF_E(\beta)$}
    \RightLabel{$[\hsbind]$}
    \BinaryInfC{$\Gamma \vdash M \hsbind N : \FF_E(\beta)$}
  \end{prooftree}
\end{proposition}

\begin{proof}
  We note that $M \hsbind N = \ap{\banana{\onto{\eta}{N}}}{M}$ and
  construct the following typing derivation in $\banana{\lambda}$:
  
  \begin{prooftree}
    \AxiomC{$\Gamma \vdash N : \alpha \to \FF_E(\beta)$}
    \RightLabel{[$\banana{}$]}
    \UnaryInfC{$\Gamma \vdash \banana{\onto{\eta}{N}} : \FF_E(\alpha) \to \FF_E(\beta)$}
    \AxiomC{$\Gamma \vdash M : \FF_E(\alpha)$}
    \RightLabel{[app]}
    \BinaryInfC{$\Gamma \vdash \ap{\banana{\onto{\eta}{N}}}{M} : \FF_E(\beta)$}
  \end{prooftree}
\end{proof}

Next, we will prove the validity of two reduction rules for $\hsbind$.

\begin{proposition}\label{prop:bind-rules}
  The following reductions are derivable in $\banana{\lambda}$:

  \vspace{2mm}
  \begin{tabular}{>{$}r<{$} >{$}c<{$} >{$}l<{$}}
    \etaE{M} \hsbind N & \to_{\eta.\hsbind} & \ap{N}{M} \\
    \app{\op{op}}{M_{\mathrm{p}}}{(\lam{x}{M_{\mathrm{c}}})} \hsbind N & \to_{\op{op}.\hsbind} & \app{\op{op}}{M_{\mathrm{p}}}{(\lam{x}{M_{\mathrm{c}} \hsbind N})}
  \end{tabular}
  \vspace{2mm}
\end{proposition}

\begin{proof}
  \begin{align*}
    \etaE{M} \hsbind N
    &= \ap{\banana{\onto{\eta}{N}}}{(\etaE{M})} \\
    &\to_{\banana{\eta}} \ap{N}{M}
  \end{align*}
  \begin{align*}
    \app{\op{op}}{M_\petitp}{(\lam{x}{M_\petitc})} \hsbind N
    &= \ap{\banana{\onto{\eta}{N}}}{(\app{\op{op}}{M_\petitp}{(\lam{x}{M_\petitc})})} \\
    &\to_{\banana{\op{op}'}} \app{\op{op}}{M_\petitp}{(\lam{x}{\ap{\banana{\onto{\eta}{N}}}{M_\petitc}})} \\
    &= \app{\op{op}}{M_\petitp}{(\lam{x}{M_\petitc \hsbind N})}
  \end{align*}
\end{proof}


\subsection{Closed Handlers\texorpdfstring{ ($\bbanana{}$)}{}}
\label{ssec:closed-handlers}

In~\ref{ssec:operations-and-handlers}, we introduced a notation for closed
handlers. Even though we define closed handlers in terms of (open)
handlers, their typing and reduction rules are actually simpler, since they
do not have to go out of their way to support openness (i.e.\ passing
through uninterpreted operations).

$$
\ap{\cibbanana}{N} = \ap{\cherry}{(\ap{\banana{(\onto{\op{op}_i}{(\lam{x k}{\ap{\eta}{(\ap{\ap{M_i}{x}}{(\cherry \compop k)})}})})_{i \in I},\ \onto{\eta}{(\lam{x}{\etaE{(\ap{M_\eta}{x})}})}}}{N})}
$$

We will first go through the typing rule.

\begin{proposition}
  The following typing rule is derivable in $\banana{\lambda}$:

  \begin{prooftree}
  \AxiomC{$E = \{\typedopg{\op{op}_i}{\alpha_i}{\beta_i}\}_{i \in I}$}
  \def\extraVskip{0pt}
  \noLine
  \UnaryInfC{$[\Gamma \vdash M_i : \alpha_i \to (\beta_i \to \delta) \to \delta]_{i \in I}$}
  \noLine
  \UnaryInfC{$\Gamma \vdash M_\eta : \gamma \to \delta$}
  \noLine
  \UnaryInfC{$\Gamma \vdash N : \FF_E(\gamma)$}
  \def\extraVskip{2pt}
  \RightLabel{$[\bbanana{}]$}
  \UnaryInfC{$\Gamma \vdash \ap{\cibbanana}{N} : \delta$}
  \end{prooftree}
\end{proposition}

\begin{proof}
  We have
  $\ap{\cibbanana}{N} = \ap{\cherry}{(\ap{\banana{(\onto{\op{op}_i}{(\lam{x
            k}{\ap{\eta}{(\ap{\ap{M_i}{x}}{(\cherry \compop k)})}})})_{i \in
          I},\ \onto{\eta}{(\lam{x}{\etaE{(\ap{M_\eta}{x})}})}}}{N})}$ and
  we will proceed by building a typing derivation for this term.

  \begin{prooftree}
    \AxiomC{$E = \{\typedopg{\op{op}_i}{\alpha_i}{\beta_i}\}_{i \in I}$}
    \def\extraVskip{0pt}
    \noLine
    \UnaryInfC{$[\Gamma \vdash \lam{x k}{\ap{\eta}{(\ap{\ap{M_i}{x}}{(\cherry \compop k)})}} : \alpha_i \to (\beta_i \to
      \FF_\emptyset(\delta)) \to \FF_\emptyset(\delta)]_{i \in I}$}
    \noLine
    \UnaryInfC{$\Gamma \vdash \lam{x}{\etaE{(\ap{M_\eta}{x})}} : \gamma \to \FF_\emptyset(\delta)$}
    \noLine
    \UnaryInfC{$\Gamma \vdash N : \FF_E(\gamma)$}
    \def\extraVskip{2pt}
    \RightLabel{[$\banana{}$]}
    \UnaryInfC{$\Gamma \vdash \ap{\banana{(\onto{\op{op}_i}{(\lam{x k}{\ap{\eta}{(\ap{\ap{M_i}{x}}{(\cherry \compop k)})}})})_{i \in I},\ \onto{\eta}{(\lam{x}{\etaE{(\ap{M_\eta}{x})}})}}}{N} : \FF_\emptyset(\delta)$} 
    \RightLabel{[$\cherry$]}
    \UnaryInfC{$\Gamma \vdash \ap{\cherry}{(\ap{\banana{(\onto{\op{op}_i}{(\lam{x k}{\ap{\eta}{(\ap{\ap{M_i}{x}}{(\cherry \compop k)})}})})_{i \in I},\ \onto{\eta}{(\lam{x}{\etaE{(\ap{M_\eta}{x})}})}}}{N})} : \delta$} 
  \end{prooftree}

  We still need to prove both
  $\Gamma \vdash \lam{x k}{\ap{\eta}{(\ap{\ap{M_i}{x}}{(\cherry \compop
        k)})}} : \alpha_i \to (\beta_i \to \FF_\emptyset(\delta)) \to
  \FF_\emptyset(\delta)$ for every $i \in I$ and
  $\Gamma \vdash \lam{x}{\etaE{(\ap{M_\eta}{x})}} : \gamma \to
  \FF_\emptyset(\delta)$.

\begin{adjustwidth}[]{-1cm}{-1cm}
  \begin{prooftree}
    \AxiomC{$\Gamma \vdash M_i : \alpha_i \to (\beta_i \to \delta) \to \delta$}
    \AxiomC{$\Gamma, x : \alpha_i \vdash x : \alpha_i$}
    \RightLabel{[app]}
    \BinaryInfC{$\Gamma, x : \alpha_i \vdash \ap{M_i}{x} : (\beta_i \to \delta) \to \delta$}
    \AxiomC{$\Gamma \vdash \cherry : \FF_\emptyset(\delta) \to \delta$}
    \AxiomC{$\Gamma, k : \beta_i \to \FF_\emptyset(\delta) \vdash k : \beta_i \to \FF_\emptyset(\delta)$}
    \RightLabel{[$\compop$]}
    \BinaryInfC{$\Gamma, k : \beta_i \to \FF_\emptyset(\delta) \vdash \cherry \compop k : \beta_i \to \delta$}
    \RightLabel{[app]}
    \BinaryInfC{$\Gamma, x : \alpha_i, k : \beta_i \to \FF_\emptyset(\delta) \vdash \ap{\ap{M_i}{x}}{(\cherry \compop k)} : \delta$}
    \RightLabel{[$\eta$]}
    \UnaryInfC{$\Gamma, x : \alpha_i, k : \beta_i \to \FF_\emptyset(\delta) \vdash \ap{\eta}{(\ap{\ap{M_i}{x}}{(\cherry \compop k)})} : \FF_\emptyset(\delta)$}
    \RightLabel{[abs]}
    \UnaryInfC{$\Gamma, x : \alpha_i \vdash \lam{k}{\ap{\eta}{(\ap{\ap{M_i}{x}}{(\cherry \compop k)})}} : (\beta_i \to \FF_\emptyset(\delta)) \to \FF_\emptyset(\delta)$}
    \RightLabel{[abs]}
    \UnaryInfC{$\Gamma \vdash \lam{x k}{\ap{\eta}{(\ap{\ap{M_i}{x}}{(\cherry \compop k)})}} : \alpha_i \to (\beta_i \to \FF_\emptyset(\delta)) \to \FF_\emptyset(\delta)$}
  \end{prooftree}
  \end{adjustwidth}

  $x$ and $k$ are assumed to be fresh for $M_i$.

  \begin{prooftree}
    \AxiomC{$\Gamma, x : \gamma \vdash M_\eta : \gamma \to \delta$}
    \AxiomC{$\Gamma, x : \gamma \vdash x : \gamma$}
    \RightLabel{[app]}
    \BinaryInfC{$\Gamma, x : \gamma \vdash \ap{M_\eta}{x} : \delta$}
    \RightLabel{[$\eta$]}
    \UnaryInfC{$\Gamma, x : \gamma \vdash \etaE{(\ap{M_\eta}{x})} : \FF_\emptyset(\delta)$}
    \RightLabel{[abs]}
    \UnaryInfC{$\Gamma \vdash \lam{x}{\etaE{(\ap{M_\eta}{x})}} : \gamma \to \FF_\emptyset(\delta)$}
  \end{prooftree}

  $x$ is assumed to be fresh for $M_\eta$.
\end{proof}

We can also have reduction rules for closed handlers, which work exactly
the same way as the open handler reduction rules (only they do not include
cases for uninterpreted operations).

\begin{proposition}
  The following reductions are derivable in $\banana{\lambda}$:

  \vspace{2mm}
  \begin{tabular}{>{$}r<{$} >{$}c<{$} >{$}l<{$}}
    \ap{\cibbanana}{(\etaE{N})} & \to_{\bbanana{\eta}} & \ap{M_\eta}{N} \\
    \ap{\cibbanana}{(\app{\op{op}_i}{N_\petitp}{(\lam{x}{N_\petitc})})}
    & \to_{\bbanana{\op{op}}}
    & \app{M_i}{N_\petitp}{(\lam{x}{\ap{\cibbanana}{N_\petitc}})}
  \end{tabular}
  \vspace{2mm}
\end{proposition}

\begin{proof}
  \begin{align*}
    \ap{\cibbanana}{(\etaE{N})}
    &= \ap{\cherry}{(\ap{\banana{(\onto{\op{op}_i}{(\lam{x k}{\ap{\eta}{(\ap{\ap{M_i}{x}}{(\cherry \compop k)})}})})_{i \in I},\ \onto{\eta}{(\lam{x}{\etaE{(\ap{M_\eta}{x})}})}}}{(\etaE{N})})} \\
    &\to_{\banana{\eta}} \ap{\cherry}{(\ap{(\lam{x}{\etaE{(\ap{M_\eta}{x})}})}{N})} \\
    &\to_\beta \ap{\cherry}{({\etaE{(\ap{M_\eta}{N})}})} \\
    &\to_\cherry \ap{M_\eta}{N}
  \end{align*}

  \begin{align*}
    \ap{\cibbanana}{(\app{\op{op}_i}{N_\petitp}{(\lam{x}{N_\petitc})})}
    &= \ap{\cherry}{(\ap{\banana{(\onto{\op{op}_i}{(\lam{x k}{\ap{\eta}{(\ap{\ap{M_i}{x}}{(\cherry \compop k)})}})})_{i \in I},\ \onto{\eta}{(\lam{x}{\etaE{(\ap{M_\eta}{x})}})}}}{(\app{\op{op}_i}{N_\petitp}{(\lam{x}{N_\petitc})})})} \\
    &\to_{\banana{\op{op}}} \ap{\cherry}{(\app{(\lam{x k}{\ap{\eta}{(\ap{\ap{M_i}{x}}{(\cherry \compop k)})}})}{N_\petitp}{(\lam{x}{\ap{\banana{\ldots}}{N_\petitc}})})} \\
    &\to_\beta \ap{\cherry}{(\ap{(\lam{k}{\ap{\eta}{(\ap{\ap{M_i}{N_\petitp}}{(\cherry \compop k)})}})}{(\lam{x}{\ap{\banana{\ldots}}{N_\petitc}})})} \\
    &\to_\beta \ap{\cherry}{(\etaE{(\ap{\ap{M_i}{N_\petitp}}{(\cherry \compop (\lam{x}{\ap{\banana{\ldots}}{N_\petitc}}))})})} \\
    &\to_\cherry \ap{\ap{M_i}{N_\petitp}}{(\cherry \compop (\lam{x}{\ap{\banana{\ldots}}{N_\petitc}}))} \\
    &= \ap{\ap{M_i}{N_\petitp}}{(\lam{x}{\ap{\cherry}{(\ap{(\lam{x}{\ap{\banana{\ldots}}{N_\petitc}})}{x})}})} \\
    &\to_\beta \ap{\ap{M_i}{N_\petitp}}{(\lam{x}{\ap{\cherry}{(\ap{\banana{\ldots}}{N_\petitc})}})} \\
    &= \ap{\ap{M_i}{N_\petitp}}{(\lam{x}{\ap{\cibbanana}{N_\petitc}})}
  \end{align*}

  In the above, $\banana{\ldots}$ is taken to be a shortcut for
  $\banana{(\onto{\op{op}_i}{(\lam{x
        k}{\ap{\eta}{(\ap{\ap{M_i}{x}}{(\cherry \compop k)})}})})_{i \in I},\
    \onto{\eta}{(\lam{x}{\etaE{(\ap{M_\eta}{x})}})}}$.
\end{proof}


\section{Type Soundness}
\label{sec:type-soundness}

In Chapter~\ref{chap:definitions}, we have introduced both a type system
and a reduction semantics for $\banana{\lambda}$. Now we will give more
substance to these two definitions by proving properties which outline the
relationship between them.

Types can give us two guarantees: typed terms do not get stuck and typed
terms always terminate. The former property is known as \emph{progress}
and, in Subsection~\ref{ssec:progress}, we will show that it holds for
$\calc$ as long as we abstain from using the partial function $\CC$. The
latter property is known as \emph{termination} and its proofs tend to be
more involved, so we delay its discussion until
Section~\ref{sec:termination}.

For both of these properties to hold, it will be essential to prove that a
typed term stays typed after performing a reduction. This will be the
object of the next subsection.


\subsection{Subject Reduction}
\label{ssec:subject-reduction}

We now turn our attention to the subject reduction property. We can
summarize subject reduction with the slogan ``reduction preserves
types''. The rest of this section will consider a formal proof of this
property for $\banana{\lambda}$, but before we begin, we present a small
lemma.

\begin{lemma}\label{lem:substitution-types}
  \demph{Substitution and types}
  
  Whenever we have $\Gamma, x : \alpha \vdash M : \tau$ and
  $\Gamma \vdash N : \alpha$, we also have
  $\Gamma \vdash \subst{M}{x}{N} : \tau$ (i.e.\ we can substitute in $M$
  while preserving the type).
\end{lemma}

\begin{proof}
  The proof is carried out by induction on the structure of $M$ (or rather
  the structure of the type derivation $\Gamma \vdash M : \tau$).

  \begin{itemize}
  \item $M = y$
    \begin{itemize}
    \item If $y = x$, then $\subst{M}{x}{N} = N$ and $\alpha = \tau$. We
      immediately have $\Gamma \vdash \subst{M}{x}{N} : \tau$ from the
      assumption that $\Gamma \vdash N : \alpha$.
    \item If $y \neq x$, then $\subst{M}{x}{N} = x$ and we get
      $\Gamma \vdash \subst{M}{x}{N} : \tau$ from the assumption that
      $\Gamma, x : \alpha \vdash M : \tau$ and the fact that
      $x \notin \FV(M)$.
    \end{itemize}
  
  \item All the other cases end up being trivial. We follow the definition
    of substitution (Definition~\ref{def:capture-resolving-substitution})
    which just applies substitution to all of the subterms. For every such
    subterm, we make appeal to the induction hypothesis and construct the
    new typing derivation.
  \end{itemize}
\end{proof}

\begin{property}\label{prop:subject-reduction}
  \demph{Subject reduction}
  
  If $\Gamma \vdash M : \tau$ and $M \to N$, then $\Gamma \vdash N : \tau$.
\end{property}

\begin{proof}
  We prove this by induction on the reduction rule used in $M \to N$.
  
  \begin{itemize}
  \item \boxed{M \to_\beta N}

    It must be the case that $M = \ap{(\lam{x}{M'})}{M''}$ and
    $N = \subst{M'}{x}{M''}$. Since, $\Gamma \vdash M : \tau$, we must have the
    following typing derivation:
    
    \begin{prooftree}
      \AxiomC{$\Gamma, x : \alpha \vdash M' : \tau$}
      \RightLabel{[abs]}
      \UnaryInfC{$\Gamma \vdash \lam{x}{M'} : \alpha \to \tau$}
      \AxiomC{$\Gamma \vdash M'' : \alpha$}
      \RightLabel{[app]}
      \BinaryInfC{$\Gamma \vdash \ap{(\lam{x}{M'})}{M''} : \tau$}
    \end{prooftree}
    
    We apply Lemma~\ref{lem:substitution-types} to
    $\Gamma, x : \alpha \vdash M' : \tau$ and $\Gamma \vdash M'' : \alpha$
    to get a typing derivation for
    $\Gamma \vdash \subst{M'}{x}{M''} : \tau$.
    
  \item \boxed{M \to_\eta N}

    We have $M = \lam{x}{\ap{M'}{x}}$ with $x$ fresh for $M'$, $N = M'$ and
    $\tau = \tau_1 \to \tau_2$. Since $\Gamma \vdash M : \tau$, we have
    the following:

    \begin{prooftree}
      \AxiomC{$\Gamma, x : \tau_1 \vdash M' : \tau_1 \to \tau_2$}
      \AxiomC{$\Gamma, x : \tau_1 \vdash x : \tau_1$}
      \RightLabel{[app]}
      \BinaryInfC{$\Gamma, x : \tau_1 \vdash \ap{M'}{x} : \tau_2$}
      \RightLabel{[abs]}
      \UnaryInfC{$\Gamma \vdash \lam{x}{\ap{M'}{x}} : \tau_1 \to \tau_2$}
    \end{prooftree}
    
    From the above derivation, we can extract
    $\Gamma, x : \tau_1 \vdash M' : \tau_1 \to \tau_2$. However, since $x$
    is fresh for $M'$, we can strengthen this to
    $\Gamma \vdash M' : \tau_1 \to \tau_2$, which is what we wanted to
    prove.
    
  \item \boxed{M \to_{\banana{\eta}} N}
    
    We have $M = \ap{\cibanana}{(\etaE{M'})}$, $N = \ap{M_\eta}{M'}$,
    $\tau = \FF_{E'}(\delta)$ and the following typing derivation for $M$:
    
    \begin{prooftree}
      \AxiomC{$\Gamma \vdash M_\eta : \gamma \to \FF_{E'}(\delta)$}
      \AxiomC{$\Gamma \vdash M' : \gamma$}
      \UnaryInfC{$\Gamma \vdash \etaE{M'} : \FF_{E}(\gamma)$}
      \AxiomC{\ldots}
      \RightLabel{[$\banana{}$]}
      \TrinaryInfC{$\Gamma \vdash \ap{\cibanana}{(\etaE{M'})} : \FF_{E'}(\delta)$}
    \end{prooftree}
    
    From the inferred typing judgments for $M_\eta$ and $M'$, we can build
    the typing derivation for $\ap{M_\eta}{M'}$.

    \begin{prooftree}
      \AxiomC{$\Gamma \vdash M_\eta : \gamma \to \FF_{E'}(\delta)$}
      \AxiomC{$\Gamma \vdash M' : \gamma$}
      \RightLabel{[app]}
      \BinaryInfC{$\Gamma \vdash \ap{M_\eta}{M'} : \FF_{E'}(\delta)$}
    \end{prooftree}
    
  \item \boxed{M \to_{\banana{\op{op}}} N}
    
    We have
    $M = \ap{\cibanana}{(\app{\op{op}_j}{M_\petitp}{(\lam{x}{M_\petitc})})}$,
    $N = \app{M_j}{M_\petitp}{(\lam{x}{\ap{\cibanana}{M_\petitc}})}$ and
    $\tau = \FF_{E'}(\delta)$.
    
    \begin{prooftree}
      \AxiomC{$\Gamma \vdash M_j : \alpha_j \to (\beta_j \to \FF_{E'}(\delta)) \to \FF_{E'}(\delta)$}
      \AxiomC{$\Gamma \vdash M_\petitp : \alpha_j$}
      \AxiomC{$\Gamma, x : \beta_j \vdash M_\petitc : \FF_{E}(\gamma)$}
      \def\extraVskip{0pt}
      \noLine
      \BinaryInfC{$\typedopg{\op{op}_j}{\alpha_j}{\beta_j} \in E$}
      \def\extraVskip{2pt}
      \RightLabel{[$\op{op}$]}
      \UnaryInfC{$\Gamma \vdash \app{\op{op}_j}{M_\petitp}{(\lam{x}{M_\petitc})} : \FF_{E}(\gamma)$}
      \AxiomC{\ldots}
      \RightLabel{[$\banana{}$]}
      \TrinaryInfC{$\Gamma \vdash \ap{\cibanana}{(\app{\op{op}_j}{M_\petitp}{(\lam{x}{M_\petitc})})} : \FF_{E'}(\delta)$}
    \end{prooftree}
    
    From the types of $M_\petitp$, $M_\petitc$ and $M_j$, we can calculate
    the type of our redex,
    $\app{M_j}{M_\petitp}{(\lam{x}{\ap{\cibanana}{M_\petitc}})}$.
    
    \begin{adjustwidth}[]{0cm}{-2cm}
    \begin{prooftree}
      \AxiomC{$\Gamma \vdash M_j : \alpha_j \to (\beta_j \to \FF_{E'}(\delta)) \to \FF_{E'}(\delta)$}
      \AxiomC{$\Gamma \vdash M_\petitp : \alpha_j$}
      \RightLabel{[app]}
      \BinaryInfC{$\Gamma \vdash \ap{M_j}{M_\petitp} : (\beta_j \to \FF_{E'}(\delta)) \to \FF_{E'}(\delta)$}
      \AxiomC{$\Gamma, x : \beta_j \vdash M_\petitc : \FF_{E}(\gamma)$}
      \AxiomC{\ldots}
      \RightLabel{[$\banana{}$]}
      \BinaryInfC{$\Gamma, x : \beta_j \vdash \ap{\cibanana}{M_\petitc} : \FF_{E'}(\delta)$}
      \RightLabel{[abs]}
      \UnaryInfC{$\Gamma \vdash \lam{x}{\ap{\cibanana}{M_\petitc}} : \beta_j \to \FF_{E'}(\delta)$}
      \RightLabel{[app]}
      \BinaryInfC{$\Gamma \vdash \app{M_j}{M_\petitp}{(\lam{x}{\ap{\cibanana}{M_\petitc}})} : \FF_{E'}(\delta)$}
    \end{prooftree}
    \end{adjustwidth}
    

  \item \boxed{M \to_{\banana{\op{op}'}} N}
    
    We have
    $M = \ap{\cibanana}{(\app{\op{op}}{M_\petitp}{(\lam{x}{M_\petitc})})}$,
    $N = \app{\op{op}}{M_\petitp}{(\lam{x}{\ap{\cibanana}{M_\petitc}})}$ and
    $\tau = \FF_{E'}(\delta)$.
    
    \begin{prooftree}
      \AxiomC{$\Gamma \vdash M_\petitp : \alpha$}
      \AxiomC{$\Gamma, x : \beta \vdash M_\petitc : \FF_{E}(\gamma)$}
      \def\extraVskip{0pt}
      \noLine
      \BinaryInfC{$\typedop{\op{op}}{\alpha}{\beta} \in E$}
      \def\extraVskip{2pt}
      \RightLabel{[$\op{op}$]}
      \UnaryInfC{$\Gamma \vdash \app{\op{op}}{M_\petitp}{(\lam{x}{M_\petitc})} : \FF_{E}(\gamma)$}
      \AxiomC{$\typedop{\op{op}}{\alpha}{\beta} \in E'$}
      \AxiomC{\ldots}
      \RightLabel{[$\banana{}$]}
      \TrinaryInfC{$\Gamma \vdash \ap{\cibanana}{(\app{\op{op}}{M_\petitp}{(\lam{x}{M_\petitc})})} : \FF_{E'}(\delta)$}
    \end{prooftree}
    
    From the inferred judgments, we can build a typing derivation for the
    redex.

    \begin{prooftree}
      \AxiomC{$\Gamma \vdash M_\petitp : \alpha$}
      \AxiomC{$\Gamma, x : \beta \vdash M_\petitc : \FF_{E}(\gamma)$}
      \AxiomC{\ldots}
      \RightLabel{[$\banana{}$]}
      \BinaryInfC{$\Gamma, x : \beta \vdash \ap{\cibanana}{M_\petitc} : \FF_{E'}(\delta)$}
      \AxiomC{$\typedop{\op{op}}{\alpha}{\beta} \in E'$}
      \RightLabel{[$\op{op}$]}
      \TrinaryInfC{$\Gamma \vdash \app{\op{op}}{M_\petitp}{(\lam{x}{\ap{\cibanana}{M_\petitc}})} : \FF_{E'}(\delta)$}
    \end{prooftree}
    
  \item \boxed{M \to_\cherry N}
    
    In this case, $M = \ap{\cherry}{(\etaE{M'})}$ and $N = M'$.
    
    \begin{prooftree}
      \AxiomC{$\Gamma \vdash M' : \tau$}
      \RightLabel{[$\eta$]}
      \UnaryInfC{$\Gamma \vdash \etaE{M'} : \FF_\emptyset(\tau)$}
      \RightLabel{[$\cherry$]}
      \UnaryInfC{$\Gamma \vdash \ap{\cherry}{(\etaE{M'})} : \tau$}
    \end{prooftree}
    
    We immediately get $\Gamma \vdash M' : \tau$, which is the sought after
    typing derivation of the redex.
    
  \item \boxed{M \to_{\CC_\eta} N}
    
    $M = \ap{\CC}{(\lam{x}{\etaE{M}})}$, $N = \etaE{(\lam{x}{M})}$ and
    $\tau = \FF_E(\gamma \to \delta)$.
    
    \begin{prooftree}
      \AxiomC{$\Gamma, x : \gamma \vdash M : \delta$}
      \RightLabel{[$\eta$]}
      \UnaryInfC{$\Gamma, x : \gamma \vdash \etaE{M} : \FF_E(\delta)$}
      \RightLabel{[abs]}
      \UnaryInfC{$\Gamma \vdash \lam{x}{\etaE{M}} : \gamma \to \FF_E(\delta)$}
      \RightLabel{[$\CC$]}
      \UnaryInfC{$\Gamma \vdash \ap{\CC}{(\lam{x}{\etaE{M}})} : \FF_E(\gamma \to \delta)$}
    \end{prooftree}
    
    From these judgments, we build a type for the redex.

    \begin{prooftree}
      \AxiomC{$\Gamma, x : \gamma \vdash M : \delta$}
      \RightLabel{[abs]}
      \UnaryInfC{$\Gamma \vdash \lam{x}{M} : \gamma \to \delta$}
      \RightLabel{[$\eta$]}
      \UnaryInfC{$\Gamma \vdash \etaE{(\lam{x}{M})} : \FF_E(\gamma \to \delta)$}
    \end{prooftree}
    
  \item \boxed{M \to_{\CC_{\op{op}}} N}
    
    $M = \ap{\CC}{(\lam{x}{\app{\op{op}}{M_\petitp}{(\lam{y}{M_\petitc})}})}$,
    $N = \app{\op{op}}{M_\petitp}{(\lam{y}{\ap{\CC}{(\lam{x}{M_\petitc})}})}$
    and $\tau = \FF_E(\gamma \to \delta)$.
    
    \begin{prooftree}
      \AxiomC{$\Gamma, x : \gamma \vdash M_\petitp : \alpha$}
      \AxiomC{$\Gamma, x : \gamma, y : \beta \vdash M_\petitc : \FF_{E}(\delta)$}
      \def\extraVskip{0pt}
      \noLine
      \BinaryInfC{$\typedop{\op{op}}{\alpha}{\beta} \in E$}
      \def\extraVskip{2pt}
      \RightLabel{[$\op{op}$]}
      \UnaryInfC{$\Gamma, x : \gamma \vdash \app{\op{op}}{M_\petitp}{(\lam{y}{M_\petitc})} : \FF_E(\delta)$}
      \RightLabel{[abs]}
      \UnaryInfC{$\Gamma \vdash \lam{x}{\app{\op{op}}{M_\petitp}{(\lam{y}{M_\petitc})}} : \gamma \to \FF_E(\delta)$}
      \RightLabel{[$\CC$]}
      \UnaryInfC{$\Gamma \vdash \ap{\CC}{(\lam{x}{\app{\op{op}}{M_\petitp}{(\lam{y}{M_\petitc})}})} : \FF_E(\gamma \to \delta)$}
    \end{prooftree}
    
    With the judgments above, we build the derivation below.
    
    \begin{prooftree}
      \AxiomC{$\Gamma \vdash M_\petitp : \alpha$}
      \AxiomC{$\Gamma, y : \beta, x : \gamma \vdash M_\petitc : \FF_E(\delta)$}
      \RightLabel{[abs]}
      \UnaryInfC{$\Gamma, y : \beta \vdash \lam{x}{M_\petitc} : \gamma \to \FF_E(\delta)$}
      \RightLabel{[$\CC$]}
      \UnaryInfC{$\Gamma, y : \beta \vdash \ap{\CC}{(\lam{x}{M_\petitc})} : \FF_E(\gamma \to \delta)$}
      \AxiomC{$\typedop{\op{op}}{\alpha}{\beta} \in E$}
      \RightLabel{[$\op{op}$]}
      \TrinaryInfC{$\Gamma \vdash \app{\op{op}}{M_\petitp}{(\lam{y}{\ap{\CC}{(\lam{x}{M_\petitc})}})} : \FF_E(\gamma \to \delta)$}
    \end{prooftree}
    
    In the above we get $\Gamma \vdash M_\petitp : \alpha$ from
    $\Gamma, x : \gamma \vdash M_\petitp : \alpha$ and the rule's
    condition that $x \notin \FV(M_\petitp)$.
    
  \item \boxed{C[M'] \to C[N']}
    
    The reduction relation of $\banana{\lambda}$ is defined as the context
    closure of the individual reduction rules. We have covered the rules
    themselves, we now address the context closure. By induction
    hypothesis, we know that the reduction from $M' \to N'$ preserves
    types, i.e.\ for any $\Delta$ and $\alpha$ such that
    $\Delta \vdash M' : \alpha$, we have $\Delta \vdash N' : \alpha$.

    We observe that the typing rules of $\banana{\lambda}$
    (Figure~\ref{fig:types}) are compositional, meaning that the type of a
    term depends only on the types of its subterms, not on their syntactic
    form. We can check this easily by looking at the premises of all of the
    typing rules. For every immediate subterm $T$, there is a premise
    $\Delta \vdash T : \alpha$ where $T$ is a metavariable. We can
    therefore replace $T$ and its typing derivation by some other $T'$ with
    $\Delta \vdash T' : \alpha$.

    Since the typing rules of $\banana{\lambda}$ are compositional, we can
    replace the $\Delta \vdash M' : \alpha$ in $\Gamma \vdash C[M'] : \tau$
    by $\Delta \vdash N' : \alpha$ and get $\Gamma \vdash C[N'] : \tau$.
  \end{itemize}
\end{proof}

We have proven subject reduction for core $\banana{\lambda}$. The syntax,
semantics and types that we have introduced for sums and products are
standard. Their proofs of subject reduction carry over into our setting as
well.


\subsection{Progress}
\label{ssec:progress}

Progress means that typed terms are never stuck. Among the terms of
$\banana{\lambda}$, we will have to identify terms which are acceptable
stopping points for reduction. Progress will mean that if a term is not in
one of these acceptable positions, then there must be a way to continue
reducing. The term we will use for these acceptable results is
\emph{value}.

\begin{definition}\label{def:value}
  A $\banana{\lambda}$ term is a \demph{value} if it can be generated by
  the following grammar:

\begin{align*}
  V ::= &\ \lam{x}{M} \\
   | \, &\ \app{\op{op}}{V}{(\lam{x}{M})} \\
   | \, &\ \etaE{V}
\end{align*}

  where $M$ ranges over $\banana{\lambda}$ terms.
\end{definition}

The above definition reflects the intuition that $\banana{\lambda}$
consists of functions and computations, where functions are built using
$\lambda$ and computations using $\op{op}$ and $\eta$. The other syntactic
constructions (application, $\banana{}$, $\cherry$ and $\CC$) all have
rules which are supposed to eventually replace them with other terms.

As with subject reduction, before we proceed to the main property, we will
start with a small lemma.

\begin{lemma}\label{lem:value-classification}
  \demph{Value classification}

  Let $V$ be a closed well-typed value (i.e.\ $\emptyset \vdash V :
  \tau$). Then the following hold:

  \begin{itemize}
  \item if $\tau = \alpha \to \beta$, then $V = \lam{x}{M}$
  \item if $\tau = \FF_E(\alpha)$, then either
    $V = \app{\op{op}}{V_\petitp}{(\lam{x}{M_\petitc})}$ or $V = \etaE{V'}$
  \end{itemize}
\end{lemma}

\begin{proof}\hspace{1cm}

  \begin{itemize}
  \item Assume $\tau = \alpha \to \beta$. If
    $V = \app{\op{op}}{V_\petitp}{(\lam{x}{M_\petitc})}$ or
    $V = \etaE{V'}$, then $\tau$ must be a computation type
    $\FF_E(\gamma)$, which is a contradiction. The only remaining
    possibility is therefore $V = \lam{x}{M}$.
  \item Assume $\tau = \FF_E(\alpha)$. If $V = \lam{x}{M}$, then $\tau$
    must be a function type $\beta \to \gamma$, which is a
    contradiction. The only remaining possibilities are therefore
    $V = \app{\op{op}}{V_\petitp}{(\lam{x}{M_\petitc})}$ or
    $V = \etaE{V'}$.
  \end{itemize}
\end{proof}

\begin{property}\label{prop:progress}
  \demph{Progress}

  Every closed well-typed term $M$ from $\banana{\lambda}$ without $\CC$
  and constants\footnote{Constants are assumed to be reduced away by some
    external rule. In our case, this will be the application of an ACG
    lexicon (\ref{sec:acg}).} is either a value or is reducible to some
  other term.
\end{property}

\begin{proof}
  We will proceed by induction on $M$.

  \begin{itemize}
  \item $M = \lam{x}{M'}$

    Then $M$ is already a value.

  \item $M = x$

    Impossible, since $M$ must be a closed term.

  \item $M = \ap{M_1}{M_2}$

    By induction hypothesis, $M_1$ and $M_2$ are either values or reducible
    terms. If either one is reducible, then our term is reducible as well
    and we are done. If neither is reducible, then they are both
    values. Since $M$ is a closed well-typed term (i.e.\
    $\emptyset \vdash M : \tau$), then
    $\empty \vdash M_1 : \alpha \to \tau$ for some $\alpha$. Thanks to
    Lemma~\ref{lem:value-classification}, we have that
    $M_1 = \lam{x}{M_1'}$. This means that $M = \ap{(\lam{x}{M_1'})}{M_2}$
    and $M$ is therefore reducible with $\beta$.

  \item $M = \app{\op{op}}{M_\petitp}{(\lam{x}{M_\petitc})}$

    By induction hypothesis, $M_\petitp$ is either reducible or a value. If
    $M_\petitp$ is reducible, then so is $M$. If it is a value, then so is
    $M$ as well.

  \item $M = \etaE{N}$

    The same argument as for $\op{op}$. By induction hypothesis $N$ is
    reducible or a value and therefore the same holds for $M$.

  \item $M = \ap{\cibanana}{N}$

    By induction hypothesis, $N$ is either a value or it is itself
    reducible. If it is reducible, then so is $M$. If it is not, then it
    must be a (closed) value. The type of $N$ is a computation type
    $\FF_E(\alpha)$ and so by Lemma~\ref{lem:value-classification}, it must
    either be $\app{\op{op}}{V_\petitp}{(\lam{x}{M_\petitc})}$ or
    $\etaE{V}$. If $N = \app{\op{op}}{V_\petitp}{(\lam{x}{M_\petitc})}$,
    then \\ $\ap{\cibanana}{(\app{\op{op}}{V_\petitp}{(\lam{x}{M_\petitc})})}$
    is reducible by $\banana{\op{op}}$ or $\banana{\op{op}'}$ (depending on
    whether or not $\op{op} \in \{\op{op}_i\}_{i \in I}$). Otherwise, if
    $N = \etaE{V}$, then $\ap{\cibanana}{(\etaE{V})}$ is reducible by
    $\banana{\eta}$.

  \item $M = \ap{\cherry}{N}$

    By induction hypothesis, $N$ is either reducible or a value. As before,
    we only have to focus on the case when $N$ is a value. From
    Lemma~\ref{lem:value-classification}, we know that
    $N = \app{\op{op}}{V_\petitp}{(\lam{x}{M_\petitc})}$ or $N =
    \etaE{V}$. However, we can rule out the former since we know that
    $\emptyset \vdash N : \FF_\emptyset(\alpha)$, meaning that $\op{op}$ is
    not in the empty effect signature $\emptyset$. We therefore end up with
    $\ap{\cherry}{(\etaE{V})}$, which is reducible by $\cherry$.
  \end{itemize}
\end{proof}

We have shown progress for $\banana{\lambda}$ without $\CC$. It is easy to
see that we cannot do better, as the $\CC$ operator can violate progress
and get us stuck quite easily.

\begin{observation}
  There exists a closed well-typed term $M$ from $\banana{\lambda}$ without
  constants that is neither a value nor reducible to some other term.
\end{observation}

\begin{proof}
  The most trivial example is $\ap{\CC}{(\lam{x}{x})}$. The computation
  that is performed by the body of the function $\lam{x}{x}$ is entirely
  determined by the parameter $x$. It is therefore not possible to pull out
  this structure outside of the function. Therefore, applying the $\CC$
  operator to this function is undefined and evaluation gets stuck.
\end{proof}


\section{Algebraic Properties}
\label{sec:algebraic-properties}

In this section, we will clarify what we mean when we say that the
$\FF_E(\alpha)$ computation types form a functor/applicative functor/monad
and we will prove that the constructions in $\banana{\lambda}$ conform to
the laws of these algebraic structures.

The object on which we will build these mathematical structures will be the
meanings of $\banana{\lambda}$ terms. We will therefore start by building
an interpretation for $\banana{\lambda}$, a denotational semantics. Then we
will be in measure to define the algebraic structures mentioned above and
verify that their laws are satisfied.


\subsection{Denotational Semantics}
\label{ssec:denotational-semantics}

We start by identifying the domains of interpretation. For each type, we
designate a set such that all terms having that type will be interpreted in
that set. Before we do so, we introduce some notation on sets.

\begin{notation}
  Let $A$ and $B$ be sets. Then:

  \begin{itemize}
  \item $A^B$ is the set of functions from $B$ to $A$
  \item $A \times B$ is the cartesian product of $A$ and $B$
  \item $A \sqcup B$ is the disjoint union of $A$ and $B$\footnote{Note
      that this disjoint union operator $\sqcup$ is different from the
      $\uplus$ one from Chapter~\ref{chap:definitions}. $A \sqcup B$ is
      defined as
      $\{ (x, 0) \mid x \in A \} \cup \{ (x, 1) \mid x \in B \}$.}
  \item $A_\bot$ is the disjoint union of $A$ and $\{\bot\}$
  \end{itemize}
\end{notation}

\begin{definition}\label{def:type-domains}
  Given a set $A_\nu$ for every atomic type $\nu$, the
  \demph{interpretation of a type} $\tau$ is a set $\sem{\tau}$ defined
  inductively by:
  
  \begin{align*}
    \sem{\nu} &= (A_\nu)_\bot \\
    \sem{\alpha \to \beta} &= (\sem{\alpha} \to \sem{\beta})_\bot \\
    \sem{\FF_E(\gamma)} &=
      (\sem{\gamma} \sqcup \bigsqcup_{\typedop{op}{\alpha}{\beta} \in E} \sem{\alpha} \times \sem{\FF_E(\gamma)}^{\sem{\beta}})_\bot
  \end{align*}
  
  Note that $\sem{\FF_E(\gamma)}$ is recursively defined not only by
  induction on the type itself but also by its use of $\sem{\FF_E(\gamma)}$
  on the right hand side. Formally, we take $\sem{\FF_E(\gamma)}$ to be the
  least fixed point of the monotone functional
  $F(X) = (\sem{\gamma} \sqcup \bigsqcup_{\typedop{op}{\alpha}{\beta} \in
    E} \sem{\alpha} \times X^{\sem{\beta}})_\bot$, whose existence is
  guaranteed by the Knaster-Tarski theorem
  \cite{knaster1928theoreme,tarski1955lattice}.
\end{definition}

\begin{notation}
  We will use $\lambda$ notation to write down elements of
  $\sem{\alpha \to \beta}$:
  
  \begin{itemize}
  \item $\lam{x}{F(x)} \in \sem{\alpha \to \beta}$ when
    $F(x) \in \sem{\beta}$ for every $x \in \sem{\alpha}$
  \item $\bot \in \sem{\alpha \to \beta}$
  \end{itemize}

  We will use the following syntax to write down elements of
  $\sem{\FF_E(\gamma)}$:
  
  \begin{itemize}
  \item $\eta(x) \in \sem{\FF_E(\gamma)}$ with $x \in \sem{\gamma}$
  \item $\op{op}(p,c) \in \sem{\FF_E(\gamma)}$ with
    $\typedop{op}{\alpha}{\beta} \in E$, $p \in \sem{\alpha}$ and
    $c \in \sem{\FF_E(\gamma)}^{\sem{\beta}}$
  \item $\bot \in \sem{\FF_E(\gamma)}$
  \end{itemize}
\end{notation}

The definition of $\sem{\tau}$ follows the definition of a value
(Definition~\ref{def:value}): function types denote functions and
computation types either denote atomic algebraic expressions ($\eta$) or
applications of algebraic operations ($\op{op}$). In the denotational
semantics, we also take care of the fact that terms can get stuck and fail
to yield the expected value. We represent this by adding the element $\bot$
to the interpretation of every type.

\begin{definition}
  We define the \demph{interpretation of a typing context} $\Gamma$
  as the set $\sem{\Gamma}$ of functions that map every
  $x : \alpha \in \Gamma$ to an element of $\sem{\alpha}$.
  
  We will call these functions \demph{valuations}. We will use the notation
  $\subst{e}{x}{f}$ to stand for the \demph{extension} of $e$ with
  $x \mapsto f$. The domain of the extension is $\dom(e) \cup x$. The
  extension maps $x$ to $f$ and every other variable in its domain to
  $e(x)$.
\end{definition}

\begin{definition}\label{def:denotations}
  Assume given $\II(c) \in \sem{\alpha}$ for every constant
  $c : \alpha \in \Sigma$. For a well-typed term $M$ with
  $\Gamma \vdash M : \tau$, we define the \demph{interpretation of term}
  $M$ as a function $\sem{M}$ from $\sem{\Gamma}$ to $\sem{\tau}$. The
  definition proceeds by induction on $M$:\footnote{In the definition, we
    make use of $\sem{\cibanana}(e)$ and $\sem{\CC}$. This notation is
    introduced right after this definition.}
  
  \begin{align*}
    \sem{\lam{x}{M}}(e) &= \lam{X}{(\sem{M}(\subst{e}{x}{X}))} \\
    \sem{x}(e) &= e(x) \\
    \sem{\ap{M}{N}}(e) &= \begin{cases}
      \sem{M}(e)(\sem{N}(e)),& \text{if $\sem{M}(e)$ is a function} \\
      \bot, & \text{if $\sem{M}(e)$ is $\bot$}
    \end{cases} \\
    \sem{c}(e) &= \II(c) \\
    \sem{\app{\op{op}}{M_\petitp}{(\lam{x}{M_\petitc})}}(e) &=
      \op{op}(\sem{M_\petitp}(e), \lam{X}{(\sem{M_\petitc}({\subst{e}{x}{X}}))}) \\
    \sem{\etaE{M}}(e) &= \eta(\sem{M}(e)) \\
    \sem{\ap{\cibanana}{N}}(e) &= \sem{\cibanana}(e)(\sem{N}(e)) \\
    \sem{\ap{\cherry}{M}}(e) &= \begin{cases}
      x, & \text{if $\sem{M}(e) = \eta(x)$} \\
      \bot, & \text{otherwise} \\
    \end{cases} \\
    \sem{\ap{\CC}{M}}(e) &= \sem{\CC}(\sem{M}(e))
  \end{align*}
\end{definition}

\begin{definition}\label{def:banana-denotation}
  The \demph{interpretation of a handler} $\cibanana$ within a valuation
  $e$ (also written as $\sem{\cibanana}(e)$) is the function $h$ defined
  inductively by:

  \begin{align*}
    h(\eta(x)) &= \begin{cases}
      \sem{M_\eta}(e)(x), & \text{if $\sem{M_\eta}(e)$ is a function} \\
      \bot, & \text{otherwise} \\
    \end{cases} \\
    h(\op{op}_j(p, c)) &= \begin{cases} 
      \sem{M_j}(e)(p)(\lam{x}{h(c(x))}), & \text{if $j \in I$, and $\sem{M_j}(e)$ and $\sem{M_j}(e)(p)$ are both functions} \\
      \op{op}_j(p, \lam{x}{h(c(x))}), & \text{if $j \notin I$} \\
      \bot, & \text{otherwise} \\
    \end{cases} \\
    h(\bot) &= \bot
  \end{align*}
  
  The equations defining $h$ use $h$ on the right-hand side. Nevertheless,
  $h$ is well-defined since we can rely on induction. There is a
  \demph{well-founded ordering on the elements of $\sem{\FF_E(\gamma)}$},
  where $\forall x.\ \op{op}(p, c) > c(x)$.

  The monotonic functional
  $F(X) = (\sem{\gamma} \sqcup \bigsqcup_{\typedop{op}{\alpha}{\beta} \in
    E} \sem{\alpha} \times X^{\sem{\beta}})_\bot$ used in defining
  $\sem{\FF_E(\gamma)}$ (Definition~\ref{def:type-domains}) is also
  Scott-continuous (i.e.\ it is both monotonic and it preserves
  suprema). By Kleene fixed-point theorem~\cite{kleene1952introduction}, we
  have that the least fixed point of $F$ is the supremum of the series
  $\emptyset \subseteq F(\emptyset) \subseteq F(F(\emptyset)) \subseteq
  \ldots$.

  Let the \demph{rank} of $x$ be the smallest $n$ such that
  $x \in F^n(\emptyset)$. The ordering $<_\petitr$, defined as
  $x <_\petitr y$ whenever $\rank(x) < \rank(y)$, is a well-founded
  ordering. It is also the inductive ordering that we were looking
  for. Whenever $\rank(\op{op}(p, c)) = n$, then $c$ is a function whose
  codomain is $F^{n-1}(\emptyset)$ and therefore
  $\forall x.\ \op{op}(p, c) >_\petitr c(x)$.
\end{definition}

\begin{definition}
  The \demph{interpretation of the $\CC$ operator} is a function $g$
  defined inductively by:
  
  \begin{align*}
    g(f) &= \begin{cases}
      \eta(h), & \text{if $f$ is a function and $\exists h.\ \forall x.\ f(x) = \eta(h(x))$} \\
      \op{op}(p, \lam{y}{g(\lam{x}{c(x)(y)})}), & \text{if $f$ is a function and $\exists \op{op},p,c.\ \forall x.\ f(x) = \op{op}(p, c(x))$} \\
      \bot, & \text{otherwise} \\
    \end{cases}
  \end{align*}
  
  As with Definition~\ref{def:banana-denotation}, we have to show that this
  is actually a valid definition since we are using $g$ on the right-hand
  side of an equation defining $g$. This time around, the arguments to $g$
  are functions whose codomain is the interpretation of some computation
  type $\FF_E(\beta)$. We can extend a well-founded ordering on the set
  $\sem{\FF_E(\beta)}$ to a \demph{well-founded ordering on
    $\sem{\alpha \to \FF_E(\beta)}$} by stating that $f < g$ whenever $f$
  and $g$ are both functions (not $\bot$) and
  $\forall x \in \sem{\alpha}.\ f(x) < g(x)$.
  
  We have to show that the recursive call to $g$ in the definition above is
  performed on an argument which is smaller than the original function. Let
  $f' = \lam{x}{c(x)(y)}$ be the function to which we recursively apply
  $g$. We have that $f(x) = \op{op}(p, c(x))$ and $f'(x) = c(x)(y)$. We
  know that $\forall y.\ \op{op}(p, c(x)) > c(x)(y)$, since that is the
  property of the well-founded ordering on the elements of
  $\sem{\FF_E(\gamma)}$ established above. Therefore, we have that
  $\forall x \in \sem{\alpha}.\ f(x) > f'(x)$ and so $f > f'$.
\end{definition}

This was the entire definition of our denotational semantics.\footnote{We
  could also extend this interpretation to sums and products. The types
  would be interpreted by
  $\sem{\alpha \times \beta} = (\sem{\alpha} \times \sem{\beta})_\bot$ and
  $\sem{\alpha + \beta} = (\sem{\alpha} \sqcup \sem{\beta})_\bot$. The term
  level definitions would be the standard definitions one would expect for
  pairs and variants (modulo the treatment of $\bot$).} We will now compare
it to the reduction semantics introduced in~\ref{sec:reductions}.

\begin{property}\label{prop:denotation-soundness}
  \demph{Soundness of reduction w.r.t.\ denotations}

  Whenever $M \to N$ in $\banana{\lambda}$, then $\sem{M} = \sem{N}$.
\end{property}

\begin{proof}
  The property relies on two facts: that our denotational semantics is
  compositional, which means that the context closure of reduction rules
  preserves denotations, and that every individual reduction preserves
  denotations. To prove so for the $\beta$ rule is a matter of proving a
  lemma stating that
  $\sem{M}(\subst{e}{x}{\sem{N}(e)}) = \sem{\subst{M}{x}{N}}(e)$, which
  follows from the compositionality of the denotational semantics. For all
  the other rules, it suffices to use the definition of interpretation
  (Definition~\ref{def:denotations}) to calculate the denotation of both
  the left-hand side and the right-hand side and verify that they are the
  same object.
\end{proof}

We see that equalities from the reduction semantics are carried over to the
denotational semantics. The converse, however, is not the case.

\begin{observation}
  \demph{Incompleteness of reduction w.r.t.\ denotations}
  
  There exist terms $M$ and $N$ in $\banana{\lambda}$ such that
  $\sem{M} = \sem{N}$ but $M$ and $N$ are not convertible.
\end{observation}

\begin{proof}
  Consider a stuck term such as $M = \ap{\CC}{(\lam{x}{x})}$ and another
  term $N = \ap{\banana{}}{M} =
  \ap{\banana{}}{(\ap{\CC}{(\lam{x}{x})})}$. Neither one of these two terms
  is reducible and neither one is a value. They are stuck and the
  denotational semantics assigns the value $\bot$ to both of them,
  therefore $\sem{M} = \sem{N}$. However, as a consequence of confluence
  (coming up in~\ref{sec:confluence}), a pair of different normal terms is
  never convertible, and therefore $M$ and $N$ are not convertible.
\end{proof}

And this concludes the definition of the denotational semantics of
$\banana{\lambda}$. Throughout most of the manuscript, we will be using the
reduction semantics introduced in~\ref{sec:reductions}, even though it is
incomplete, since it allows us to simplify terms in a mechanical and
transparent step-by-step manner. However, the denotational semantics will
be useful to us in the rest of this section since it will let us access
extra equalities needed to prove some general laws.


\subsection{Category}
\label{ssec:category}

We aim to show that the computation types in $\banana{\lambda}$ form a
functor, applicative functor and a monad. All these terms are defined
w.r.t.\ some category and so we will start by introducing the category
underlying $\banana{\lambda}$.

\begin{definition}
 A \demph{category} consists of:
\begin{itemize}
\item a set of \demph{objects}
\item for every two objects $A$ and $B$, a set of \demph{arrows} from $A$
  to $B$ (an arrow $f$ from $A$ to $B$ is written as $f : A \to B$)
\item for any two arrows $f : B \to C$ and $g : A \to B$, there exists the
  composition of the two arrows $f \compop g : A \to C$
\item for any object $A$, there exists a special arrow $\id_A : A \to A$
\item the following equations hold for any $f : C \to D$, $g : B \to C$ and
  $h : A \to B$:
  \begin{align}
    \label{law:cat-associativity}(f \compop g) \compop h &= f \compop (g \compop h) & \text{(Associativity)} \\
    \label{law:cat-left-identity}\id_D \compop f &= f & \text{(Left identity)} \\
    \label{law:cat-right-identity}f \compop \id_C &= f & \text{(Right identity)}
  \end{align}
\end{itemize}
\end{definition}

We will be working with a particular category, which we will call
$\banana{\lambda}$. The $\banana{\lambda}$ category consists of:
\begin{description}
\item[objects:] the types of the $\banana{\lambda}$ calculus
\item[arrows:] for any two types $\alpha$ and $\beta$, the arrows from
  $\alpha$ to $\beta$ are the functions from $\sem{\alpha}$ to
  $\sem{\beta}$
\item[composition:] composition of arrows is defined as composition of functions
\item[identities:] for every type $\alpha$, we define $\id_\alpha$ as the
  identity function with domain $\sem{\alpha}$
\end{description}

Since the arrows in our category are functions, the three laws of a
category (associativity (\ref{law:cat-associativity}), left identity
(\ref{law:cat-left-identity}) and right identity
(\ref{law:cat-right-identity})) fall out of the same properties for
functions.


\subsection{The Three Laws}
\label{ssec:three-laws}

Monads form a subset of applicative functors which in turn is a subset of
functors. Instead of incrementally building up from a functor all the way
to a monad, it will end up being more practical to first prove the monad
laws and then illustrate how they let us verify the functor and applicative
functor laws. Therefore, we first define our monadic bind operator and
prove the three monad laws.

\begin{definition}\label{def:bind}
  Let $X$ be from $\sem{\FF_E(\alpha)}$ and $f$ be a function from
  $\sem{\alpha}$ to $\sem{\FF_E(\beta)}$. We define $X \hsbind f$
  inductively on the structure of $X$:

  \begin{align*}
    \op{op}(p, c) \hsbind f &= \op{op}(p, \lam{x}{c(x) \hsbind f}) \\
    \eta(x) \hsbind f &= f(x) \\
    \bot \hsbind f &= \bot
  \end{align*}

  Note that $X \hsbind f$ is equivalent to
  $\sem{x \hsbind y}([x \mapsto X, y \mapsto f])$.
\end{definition}

\begin{law}\label{law:associativity}
  (Associativity of $\hsbind$)
  
  Let $X$ be from $\sem{\FF_E(\alpha)}$, $f$ be a function from
  $\sem{\alpha}$ to $\sem{\FF_E(\beta)}$ and $g$ be a function from
  $\sem{\beta}$ to $\sem{\FF_E(\gamma)}$. Then the following equation
  holds:
  
  $$
  (X \hsbind f) \hsbind g \;=\; X \hsbind (\lam{x}{f(x) \hsbind g})
  $$
\end{law}

\begin{proof}
  Proof by induction on the well-founded structure of $X$:
  
  \begin{itemize}
  \item $X = \bot$
    
    \vspace{-0.5cm}
    \begin{align*}
      (\bot \hsbind f) \hsbind g
      &= \bot \hsbind g \\
      &= \bot \\
      &= \bot \hsbind (\lam{x}{f(x) \hsbind g})
    \end{align*}
    
  \item $X = \eta(x)$

    \vspace{-0.5cm}
    \begin{align*}
      (\eta(x) \hsbind f) \hsbind g
      &= f(x) \hsbind g \\
      &= (\lam{x}{f(x) \hsbind g})(x) \\
      &= \eta(x) \hsbind (\lam{x}{f(x) \hsbind g})
    \end{align*}
    
  \item $X = \op{op}(p, c)$
    
    \vspace{-0.5cm}
    \begin{align*}
      (\op{op}(p, c) \hsbind f) \hsbind g
      &= \op{op}(p, \lam{y}{c(y) \hsbind f}) \hsbind g \\
      &= \op{op}(p, \lam{y}{(c(y) \hsbind f) \hsbind g}) \\
      &= \op{op}(p, \lam{y}{c(y) \hsbind (\lam{x}{(f(x) \hsbind g}))}) \\
      &= \op{op}(p, c) \hsbind (\lam{x}{f(x) \hsbind g})
    \end{align*}
  \end{itemize}
\end{proof}

\begin{law}\label{law:left-identity}
  (Left identity for $\hsbind$)
  
  Let $\eta(x)$ be from $\sem{\FF_E(\alpha)}$ and $f$ be a function from
  $\sem{\alpha}$ to $\sem{\FF_E(\beta)}$. Then the following holds:
  
  $$
  \eta(x) \hsbind f = f(x)
  $$
\end{law}

\begin{proof}
  Follows immediately from the definition of $\hsbind$
  (Definition~\ref{def:bind}).
\end{proof}

\begin{law}\label{law:right-identity}
  (Right identity for $\hsbind$)
  
  Let $X$ be from $\sem{\FF_E(\alpha)}$. Then the following holds:

  $$
  X \hsbind (\lam{x}{\eta(x)}) = X
  $$
\end{law}

\begin{proof}
  By induction on the structure of $X$:
  
  \begin{itemize}
  \item $X = \bot$
    
    \vspace{-0.5cm}
    \begin{align*}
      \bot \hsbind (\lam{x}{\eta(x)})
      &= \bot
    \end{align*}
   
  \item $X = \eta(x)$
    
    \vspace{-0.5cm}
    \begin{align*}
      \eta(x) \hsbind (\lam{x}{\eta(x)})
      &= (\lam{x}{\eta(x)})(x) \\
      &= \eta(x)
    \end{align*}
    
  \item $X = \op{op}(p, c)$

    \vspace{-0.5cm}
    \begin{align*}
      \op{op}(p, c) \hsbind (\lam{x}{\eta(x)})
      &= \op{op}(p, \lam{y}{c(y) \hsbind (\lam{x}{\eta(x)})}) \\
      &= \op{op}(p, \lam{y}{c(y)}) \\
      &= \op{op}(p, c)
    \end{align*}
  \end{itemize}
\end{proof}


\subsection{Functor}
\label{ssec:functor}

We will start by showing what is a functor and in what way do our
computation types form one.

\begin{definition}
A \demph{functor} is a homomorphic mapping from one category to
another. A functor $F$ from a category a $\CC$ to a category $\DD$
consists of:
\begin{itemize}
\item for every object $A$ in $\CC$, an object $F(A)$ in $\DD$
\item for every arrow $f : A \to B$ in $\CC$, an arrow $F(f) : F(A) \to
  F(B)$ in $\DD$
\item the following equations hold:
  \begin{align}
    \label{law:functor-composition}F(f \compop g) &= F(f) \compop F(g) & \text{(Composition)} \\
    \label{law:functor-identity}F(\id_A) &= \id_{F(A)} & \text{(Identity)}
  \end{align}
\end{itemize}
\end{definition}

\begin{definition}
  An \demph{endofunctor} is a functor from some category $\CC$ to the same
  category.
\end{definition}

For every effect signature $E$, we will show that $\FF_E$ is an endofunctor
on the $\banana{\lambda}$ category.
\begin{description}
\item[objects:] for every type $\alpha$, we have a type $\FF_E(\alpha)$
\item[arrows:] for every function $f : \sem{\alpha} \to \sem{\beta}$, \\ we
  have a function
  $\FF_E(f) = \lam{X}{X \hsbind (\lam{x}{\eta(f(x))})} :
  \sem{\FF_E(\alpha)} \to \sem{\FF_E(\beta)}$
\end{description}

We also need to prove that $\FF_E$ satisfies the necesary laws.

Composition~\eqref{law:functor-composition}:

\setcounter{TemporaryCounter}{\value{equation}}
\setcounter{equation}{0}
\NoChapterPrefix
\begin{align}
  \FF_E(f) \compop \FF_E(g)
  &= \lam{X}{\FF_E(f)(\FF_E(g)(X))} \\
  &= \lam{X}{\FF_E(f)((\lam{Z}{Z \hsbind (\lam{x}{\eta(g(x))})})(X))} \\
  &= \lam{X}{\FF_E(f)(X \hsbind (\lam{x}{\eta(g(x))}))} \\
  &= \lam{X}{(\lam{Z}{Z \hsbind (\lam{y}{\eta(f(y))})})(X \hsbind (\lam{x}{\eta(g(x))}))} \\
  &= \lam{X}{(X \hsbind (\lam{x}{\eta(g(x))})) \hsbind (\lam{y}{\eta(f(y))})} \\
  &= \lam{X}{X \hsbind (\lam{z}{(\lam{x}{\eta(g(x))})(z) \hsbind (\lam{y}{\eta(f(y))})})} \\
  &= \lam{X}{X \hsbind (\lam{z}{\eta(g(z)) \hsbind (\lam{y}{\eta(f(y))})})} \\
  &= \lam{X}{X \hsbind (\lam{z}{(\lam{y}{\eta(f(y))})(g(z))})} \\
  &= \lam{X}{X \hsbind (\lam{z}{\eta(f(g(z)))})} \\
  &= \lam{X}{X \hsbind (\lam{z}{\eta((f \compop g)(z))})} \\
  &= \FF_E(f \compop g)
\end{align}
\setcounter{equation}{\value{TemporaryCounter}}
\ChapterPrefix

Throughout most of the proof, we rely only on the definitions of function
composition and the $\FF_E(f)$ function lifter. On Line~6 though, we make
use of the associativity law of $\hsbind$~\eqref{law:associativity}.

Identities~\eqref{law:functor-identity}:

\setcounter{TemporaryCounter}{\value{equation}}
\setcounter{equation}{0}
\NoChapterPrefix
\begin{align}
  \FF_E(\id_\alpha)
  &= \lam{X}{X \hsbind (\lam{x}{\eta(\id_\alpha(x))})} \\
  &= \lam{X}{X \hsbind (\lam{x}{\eta(x)})} \\
  &= \lam{X}{X} \\
  &= \id_{\FF_E(\alpha)}
\end{align}
\setcounter{equation}{\value{TemporaryCounter}}
\ChapterPrefix

Here, we make use of the right identity law of
$\hsbind$~\eqref{law:right-identity} on Line~3.

And so we have a functor $\FF_E$. Its laws let us think of computations of
type $\FF_E(\alpha)$ as containing values of type $\alpha$ over which we
can map functions the same way one would map a function over a list,
satisfying the same basic laws.

We also note that $\banana{\lambda}$ already has syntax for this kind of
mapping: $\FF_E(f)(X) = \sem{x \apr y}([x \mapsto f, y \mapsto X])$ where
$\apr$ is the operator introduced in~\ref{ssec:composing-functions}.


\subsection{Applicative Functor}
\label{ssec:applicative-functor}

The two remaining structures, applicative functors and monads, both have
two popular and slightly different presentations. One is in terms of
natural transformations and is very common in category theory, whereas the
other is given in terms of combinators and is preferred in functional
programming. Since we will not be using category theory in the rest of this
thesis and we will work with lots of combinators, we will focus on the
presentation favored in functional programming.

\pagebreak[4]

\begin{definition}
  An \demph{applicative functor}~\cite{mcbride2008applicative} is a functor
  $F$ alongside with two combinators, $\pure : \alpha \to F(\alpha)$ and
  $\circledast : F(\alpha \to \beta) \to F(\alpha) \to
  F(\beta)$\footnote{$\circledast$ is an infix operator that associates to
    the left, same as application (which it is based on).}  polymorphic in
  $\alpha$ and $\beta$.\footnote{This means that the combinators should not
    have different definitions for different instances of the type
    variables $\alpha$ and $\beta$.} Furthermore, the following laws must
  hold:\footnote{In the Composition Law~\eqref{law:app-composition},
  $(\compop)$ is the function composition combinator.}

  \begin{align}
    \label{law:app-identity}\pure(\id) \circledast u &= u & \text{(Identity)} \\
    \label{law:app-composition}\pure(\compop) \circledast u \circledast v \circledast w &= u \circledast (v \circledast w) & \text{(Composition)} \\
    \label{law:app-homomorphism}\pure(f) \circledast \pure(x) &= \pure(f(x)) & \text{(Homomorphism)} \\
    \label{law:app-interchange}u \circledast \pure(x) &= \pure(\lam{f}{f(x)}) \circledast u & \text{(Interchange)}
  \end{align}
\end{definition}

The intuition is that the functor $F$ adds some notion of computational
effect. $F(\alpha)$ is the type of computations having that effect and
producing a value of type $\alpha$. The $\pure$ combinator injects a value
into the domain of computations without doing anything on the effect
level. The $\circledast$ operator is then meant as a sequencing of two
computations: one yielding a function and the other yielding its argument.

The identity law makes sure that $\pure$ is indeed pure and does not add
any meaningful effect: the effect of $\pure$ must in no way tamper with the
effect of $u$. The composition law tells us that the $\circledast$ operator
must be associative: at the level of effects, there is some monoidal
structure, where order matters but bracketing does not. The homomorphism
law makes sure that $\circledast$ does actually perform function
application. Finally, the interchange law guarantees that $\circledast$ is
not one-sided: the effect of $u$ is respected, no matter if it occurs to
the left or to the right of $\circledast$.

To show that $\FF_E$ is an applicative functor, we will need to define
$\pure$ and $\circledast$ and prove the four applicative functor laws.

\begin{align*}
  \pure(x) &= \eta(x) \\
  F \circledast X &= F \hsbind (\lam{f}{X \hsbind (\lam{x}{\eta(f(x))})})
\end{align*}

Now, we prove the laws.

Identity~\eqref{law:app-identity}:

\setcounter{TemporaryCounter}{\value{equation}}
\setcounter{equation}{0}
\NoChapterPrefix
\begin{align}
  \pure(\id) \circledast u
  &= \eta(\id) \circledast u \\
  &= \eta(\id) \hsbind (\lam{f}{u \hsbind (\lam{x}{\eta(f(x))})}) \\
  &= (\lam{f}{u \hsbind (\lam{x}{\eta(f(x))})})(\id) \\
  &= u \hsbind (\lam{x}{\eta(\id(x))}) \\
  &= u \hsbind (\lam{x}{\eta(x)}) \\
  &= u
\end{align}
\setcounter{equation}{\value{TemporaryCounter}}
\ChapterPrefix

We make use of the left and right identities of $\hsbind$ on Lines~3 and 6,
respectively.

\pagebreak[4]

Composition~\eqref{law:app-composition}:

\setcounter{TemporaryCounter}{\value{equation}}
\setcounter{equation}{0}
\NoChapterPrefix
\begin{align}
  \pure(\compop) \circledast u \circledast v \circledast w
  &= \pure(\lam{f g x}{f(g(x))}) \circledast u \circledast v \circledast w \\
  &= \eta(\lam{f g x}{f(g(x))}) \circledast u \circledast v \circledast w \\
  &= (\eta(\lam{f g x}{f(g(x))}) \hsbind (\lam{f}{u \hsbind (\lam{x}{\eta(f(x))})})) \circledast v \circledast w \\
  &= ((\lam{f}{u \hsbind (\lam{u'}{\eta(f(u'))})})(\lam{f g x}{f(g(x))})) \circledast v \circledast w \\
  &= (u \hsbind (\lam{u'}{\eta((\lam{f g x}{f(g(x))})(u'))})) \circledast v \circledast w \\
  &= (u \hsbind (\lam{u'}{\eta(\lam{g x}{u'(g(x))})})) \circledast v \circledast w \\
  &= ((u \hsbind (\lam{u'}{\eta(\lam{g x}{u'(g(x))})})) \hsbind (\lam{f}{v \hsbind (\lam{v'}{\eta(f(v'))})})) \circledast w \\
  &= (u \hsbind (\lam{u'}{\eta(\lam{g x}{u'(g(x))}) \hsbind (\lam{f}{v \hsbind (\lam{v'}{\eta(f(v'))})})})) \circledast w \\
  &= (u \hsbind (\lam{u'}{(\lam{f}{v \hsbind (\lam{v'}{\eta(f(v'))})})(\lam{g x}{u'(g(x))})})) \circledast w \\
  &= (u \hsbind (\lam{u'}{v \hsbind (\lam{v'}{\eta((\lam{g x}{u'(g(x))})(v'))})})) \circledast w \\
  &= (u \hsbind (\lam{u'}{v \hsbind (\lam{v'}{\eta(\lam{x}{u'(v'(x))})})})) \circledast w \\
  &= (u \hsbind (\lam{u'}{v \hsbind (\lam{v'}{\eta(\lam{x}{u'(v'(x))})})})) \hsbind (\lam{f}{w \hsbind (\lam{w'}{\eta(f(w'))})}) \\
  &= u \hsbind (\lam{u'}{v \hsbind (\lam{v'}{\eta(\lam{x}{u'(v'(x))})}) \hsbind (\lam{f}{w \hsbind (\lam{w'}{\eta(f(w'))})})}) \\
  &= u \hsbind (\lam{u'}{v \hsbind (\lam{v'}{\eta(\lam{x}{u'(v'(x))}) \hsbind (\lam{f}{w \hsbind (\lam{w'}{\eta(f(w'))})})})}) \\
  &= u \hsbind (\lam{u'}{v \hsbind (\lam{v'}{(\lam{f}{w \hsbind (\lam{w'}{\eta(f(w'))})})(\lam{x}{u'(v'(x))})})}) \\
  &= u \hsbind (\lam{u'}{v \hsbind (\lam{v'}{w \hsbind (\lam{w'}{\eta((\lam{x}{u'(v'(x))})(w'))})})}) \\
  &= u \hsbind (\lam{u'}{v \hsbind (\lam{v'}{w \hsbind (\lam{w'}{\eta(u'(v'(w')))})})}) \\
  &= u \hsbind (\lam{u'}{v \hsbind (\lam{v'}{w \hsbind (\lam{w'}{(\lam{x}{\eta(u'(x))})(v'(w'))})})}) \\
  &= u \hsbind (\lam{u'}{v \hsbind (\lam{v'}{w \hsbind (\lam{w'}{\eta(v'(w')) \hsbind (\lam{x}{\eta(u'(x))})})})}) \\
  &= u \hsbind (\lam{u'}{v \hsbind (\lam{v'}{(w \hsbind (\lam{w'}{\eta(v'(w'))})) \hsbind (\lam{x}{\eta(u'(x))})})}) \\
  &= u \hsbind (\lam{u'}{(v \hsbind (\lam{v'}{w \hsbind (\lam{w'}{\eta(v'(w'))})})) \hsbind (\lam{x}{\eta(u'(x))})}) \\
  &= u \hsbind (\lam{u'}{(v \circledast w) \hsbind (\lam{x}{\eta(u'(x))})}) \\
  &= u \circledast (v \circledast w)
\end{align}
\setcounter{equation}{\value{TemporaryCounter}}
\ChapterPrefix

This law relies heavily on the associativity of
$\hsbind$~\eqref{law:associativity}. At the beginning, the expression is
associated to the left:
$((\pure(\compop) \circledast u) \circledast v) \circledast w$. And at the
end, it associated to the right: $u \circledast (v \circledast w)$. In the
first part of the proof, we use left identity (Lines~4, 9 and 15) and
associativity (Lines~8, 13 and 14). We reach a normal form on Line~17 and start
expanding the term again. We expand using left identity on Line~19 and then
we reassociate using associativity on Lines~20 and 21. The rest of the
equalities is due to the definitions of $\pure$ and $\circledast$ and
$\beta$ reduction.

Homomorphism~\eqref{law:app-homomorphism}:

\setcounter{TemporaryCounter}{\value{equation}}
\setcounter{equation}{0}
\NoChapterPrefix
\begin{align}
\pure(f) \circledast \pure(x)
&= \eta(f) \circledast \eta(x) \\
&= \eta(f) \hsbind (\lam{f'}{\eta(x) \hsbind (\lam{x'}{\eta(f'(x'))})}) \\
&= (\lam{f'}{\eta(x) \hsbind (\lam{x'}{\eta(f'(x'))})})(f) \\
&= \eta(x) \hsbind (\lam{x'}{\eta(f(x'))}) \\
&= (\lam{x'}{\eta(f(x'))})(x) \\
&= \eta(f(x)) \\
&= \pure(f(x))
\end{align}
\setcounter{equation}{\value{TemporaryCounter}}
\ChapterPrefix

This proof is very direct. We just have to normalize the term using the
left identity of $\hsbind$~\eqref{law:left-identity}.

\pagebreak[3]

Interchange~\eqref{law:app-interchange}:

\setcounter{TemporaryCounter}{\value{equation}}
\setcounter{equation}{0}
\NoChapterPrefix
\begin{align}
\pure(\lam{f}{f(x)}) \circledast u
&= \eta(\lam{f}{f(x)}) \circledast u \\
&= \eta(\lam{f}{f(x)}) \hsbind (\lam{f'}{u \hsbind (\lam{u'}{\eta(f'(u'))})}) \\
&= (\lam{f'}{u \hsbind (\lam{u'}{\eta(f'(u'))})})(\lam{f}{f(x)}) \\
&= u \hsbind (\lam{u'}{\eta((\lam{f}{f(x)})(u'))}) \\
&= u \hsbind (\lam{u'}{\eta(u'(x))}) \\
&= u \hsbind (\lam{u'}{(\lam{x'}{\eta(u'(x'))})(x)}) \\
&= u \hsbind (\lam{u'}{\eta(x) \hsbind (\lam{x'}{\eta(u'(x'))})}) \\
&= u \circledast \eta(x) \\
&= u \circledast \pure(x)
\end{align}
\setcounter{equation}{\value{TemporaryCounter}}
\ChapterPrefix

We start with the law's right-hand side and normalize it on Line~5. Then we
do some expansions (left identity on Line~7) to get the desired form.

And there we are, we now have an applicative functor. Applicative functors
are very similar to monads, which we will cover in the next part. The
trade-off between the two is in terms of expressivity vs
composability. When we have two applicative functors, we can compose them
and recover $\pure$ and $\circledast$ such that they both satisfy the
necessary laws. However, the same is as not easy with monads and one has to
go a level higher and compose monad transformers. On the other hand, when
chaining two computations within a monad, we can use an operator which lets
the effects of the second computation depend on the result of the first,
whereas when chaining two computations within an applicative functor, the
effects of both computations must be independent.

Oleg Kiselyov explores the use of applicative functors for natural language
semantics in his recent Applicative Abstract Categorial
Grammars~\cite{kiselyov2015applicative,kiselyov2015swing}. The
composability of applicative functors facilitates the combination, which is
the objective of our method as well. However, partly in order to compensate
for the limited expressivity of applicative functors, several
interpretation passes are required until the logical form of a sentence is
constructed.

The two combinators that make up an applicative functor are accessible in
$\banana{\lambda}$. The $\pure$ operator is the function $\eta$ and
$\circledast$ is the combinator $\aplr$ introduced
in~\ref{ssec:composing-functions}.


\subsection{Monad}
\label{ssec:monad}

\begin{definition}
  A \demph{monad} is a functor $F$ and two combinators,
  $\eta : \alpha \to F(\alpha)$ and
  $\hsbind : F(\alpha) \to (\alpha \to F(\beta)) \to F(\beta)$, polymorphic
  in $\alpha$ and $\beta$. These objects must also satisfy the following
  laws:

  \begin{align}
    (X \hsbind f) \hsbind g &= X \hsbind (\lam{x}{f(x) \hsbind g})
    & \label{law:monad-assoc} \text{(Associativity)} \\
    \eta(x) \hsbind f &= f(x)
    & \label{law:monad-left-id} \text{(Left identity)} \\
    X \hsbind \eta &= X
    & \label{law:monad-right-id} \text{(Right identity)}
  \end{align}
\end{definition}

To understand why the laws look the way they do, we will consider functions
of type $\alpha \to F(\beta)$. These are the kinds of functions one might
use to model call-by-value~\cite{moggi1991notions,moggi1990abstract}: we
take a value $\alpha$ and then yield some computation $F(\beta)$. Now
assume we would use this type of functions to model procedures of input
type $\alpha$ and output type $\beta$ and we would want these procedures to
form a category. For every type $\alpha$, we would an identity procedure
with input type and output type $\alpha$, therefore a function of type
$\alpha \to F(\alpha)$. The polymorphic $\eta$ combinator will be this
identity procedure. We would also like to be able to compose a procedure
from $\alpha$ to $\beta$ with a procedure from $\beta$ to $\gamma$, i.e.\
compose functions of types $\alpha \to F(\beta)$ and $\beta \to
F(\gamma)$. When composing $f : \alpha \to F(\beta)$ with
$g : \beta \to F(\gamma)$, we run into the problem of having some
$f(x) : F(\beta)$ and $g : \beta \to F(\gamma)$ that we cannot
compose. This is where $\hsbind$ comes in and composes these two values for
us. Let $f \kleisli g = \lam{x}{f(x) \hsbind g}$ be the resulting
composition operator. In order for this structure to be a category, it
needs to satisfy the following:

\begin{align*}
  (f \kleisli g) \kleisli h &= f \kleisli (g \kleisli h) \\
  \eta \kleisli f &= f \\
  f \kleisli \eta &= f
\end{align*}

By taking $f$ to be the constant function that returns $X$, we end up with
the laws of the monad. Conversely, with the principle of extensionality, we
can derive these laws from the monad laws. Therefore
$\left<F, \eta, \hsbind\right>$ forms a monad whenever the derived
$\kleisli$ and $\eta$ form a category. This kind of category is called a
\emph{Kleisli category} and the particular presentation of a monad that we
have given here is known as a \emph{Kleisli triple}.

To prove that $\FF_E$ is a monad will be trivial: we have already done so!
The $\eta$ combinator is of course our $\eta$ and $\hsbind$ is our
$\hsbind$. The three laws that we need to verify are three laws that we
have introduced in~\ref{ssec:three-laws} and have been using throughout
this section.

Monads have been introduced to natural language semantics by Chung-chieh
Shan in 2002~\cite{shan2002monads}. Since then, they have seen occasional
use, mostly to handle dynamics without burdening the semantics with
context/continuation passing~\cite{unger2011dynamic,champollion2015back},
but also other phenomena such as conventional
implicature/opacity~\cite{giorgolo2011multidimensional,giorgolo2012monads,giorgolo2014monads}. The
challenge of combining different phenomena which rely on different monads
has been tackled from two angles: using distributive laws for
monads~\cite{giorgolo2015natural} and using monad
transformers~\cite{charlow2014semantics,barker2015monads}.

In the $\banana{\lambda}$ calculus, the monadic operations $\eta$ and
$\hsbind$ are available as the $\eta$ constructor and the $\hsbind$
combinator introduced in~\ref{ssec:composing-functions}.

\subsection{Free Monad}
\label{ssec:free-monad}

A free monad is a construction of a monad such that the resulting monad
satisfies the monad laws but does not satisfy anything more than that. This
is similar to the idea of a free monoid generated by some set $A$. If we
have two expressions from a monoid, we know that the operation is
associative and so the grouping does not matter. The only thing that
matters is which elements are multiplied and in which order. We can
therefore think of the elements of this monoid as lists of values from
$A$. The free monad is a similar construction, but instead of building a
monoid out of a set, it builds a monad out of a functor.

Let $S$ be some functor, we define the monad
$\left<F, \eta, \hsbind\right>$ where:

\begin{itemize}
\item $F(\gamma) = \gamma + S(F(\gamma))$\footnote{The plus is a coproduct,
    which in the case of our category of sets corresponds to disjoint
    union. We will work with coproducts using a similar syntax to the one
    that we have given to sums in~\ref{sec:sums-and-products}.}
\item $\eta(x) = \inl(x)$
\item $X \hsbind f = \casemon{X}{x}{f(x)}{s}{\inr(S(\lam{X'}{X' \hsbind f})(s))}$
\end{itemize}

The functor $S$ correspond to our effect signature. For
$\typedop{op}{\alpha}{\beta}$, we can define a functor
$S_{\op{op}}(\gamma) = \alpha \times \gamma^\beta$. This represents
providing a value of type $\alpha$ and waiting for an answer of type
$\beta$ before continuing with $\gamma$. By taking the free monad of this
functor, we end up with a monad where after each operation, we continue
with a new computation of the same kind until we yield a $\gamma$,
$F(\gamma) = \gamma + \alpha \times F(\gamma)^\beta$.

If we want to offer more operations with different input and output types,
we can take the coproduct of several functors. For an effect signature $E$,
$S_E(\gamma) = \sum_{\op{op} \in E} S_{\op{op}}(\gamma)$.  The resulting
free monad then looks like
$F(\gamma) = \gamma + \sum_{\typedop{op}{\alpha}{\beta} \in E} \alpha
\times F(\gamma)^\beta$. If we replace sums with disjoint unions and types
with their interpretations, you almost get the kind of set in which we
interpret our computation types
in~\ref{ssec:denotational-semantics}. Importantly, the $\eta$ and $\hsbind$
are the same as the ones of the free monad given above. The only difference
between the free monad and the monad used in our interpretation are the
occurrences of $\bot$. The monad that we used is actually a combination of
the $F(X) = X_\bot$ partiality monad and a free monad.

Free monads are another solution to the combination of different effects
within a single calculus, hitherto unexplored in its application to natural
language semantics. A very early appearance of this technique dates back to
1994~\cite{cartwright1994extensible}. It has recently gained prominence
with the work on extensible effects and effect
handlers~\cite{kiselyov2013extensible,bauer2012programming,kammar2013handlers,brady2013programming,plotkin2013handling,pretnar2010logic}.


\section{Confluence}
\label{sec:confluence}

The object of our study during this section will be the proof of the
\emph{confluence property} of $\banana{\lambda}$. Informally, it means that
a single term cannot reduce to two or more different results. Together with
the termination (which we address in Section~\ref{sec:termination}),
confluence can give us the property that every term yields exactly one
result and does so in a finite amount of steps (a property known as
\emph{strong normalization}). Confluence also gives us a strong tool to
prove inequality of terms. If two terms reduce to different normal forms,
confluence guarantees us that they are not convertible.

\begin{definition}
  A reduction relution $\to$ on a set $A$ is said to be \demph{confluent}
  whenever for each $a,b,c \in A$ such that $a \to b$ and $a \to c$ there
  is a $d \in A$ such that $b \tto d$ and $c \tto d$.
\end{definition}

Proofs of this property are often mechanical and follow the same
pattern. Our strategy will be to reuse a general result which applies one
such proof for a general class of rewriting systems. Our rewriting system
is a system of reductions on terms and the reductions have side conditions
concerning the binding of free variables. A good fit for this kind of
system are the Combinatory Reduction Systems (CRSs) of
Klop~\cite{klop1993combinatory}.

The main result about CRSs that we will make use of is the following
(Corollary~13.6 in~\cite{klop1993combinatory}).

\begin{theorem}\label{thm:confluence-crs}
  \demph{Confluence of orthogonal CRSs}

  Every orthogonal CRS is confluent.
\end{theorem}

We will model $\calc$ as a CRS. However, $\eta$-reduction will deny us
orthogonality. We will therefore first prove confluence of $\calc$ without
$\eta$-reduction and then we will manually show that confluence is
preserved on adding $\eta$-reduction back.

\begin{notation}
  The \demph{intensional $\banana{\lambda}$ calculus $\intcalc$} is the
  $\banana{\lambda}$ calculus without the $\eta$-reduction rule.
\end{notation}

The rest of this section will go like this:

\begin{itemize}
\item CRS: a formalism for higher-order rewriting (\ref{ssec:crs})
\item $\calc$ is a CRS (\ref{ssec:banana-as-crs})
\item Klop et al [93]: Every orthogonal CRS is confluent
  (\ref{ssec:orthogonal-crs})
  \begin{itemize}
  \item $\intcalc$ is an orthogonal CRS $\Rightarrow$ $\intcalc$ is confluent
    (Lemma~\ref{lem:confluence-int})
  \item $\eta$ is an orthogonal CRS $\Rightarrow$ $\eta$ is confluent
    (Lemma~\ref{lem:confluence-eta})
  \end{itemize}
\item $\intcalc + \eta$ is confluent (\ref{ssec:confluence-eta},
  Theorem~\ref{thm:confluence})
  \begin{itemize}
  \item because $\intcalc$ and $\eta$ commute (Lemma~\ref{lem:eta-commutes})
  \end{itemize}
\end{itemize}


\subsection{Combinatory Reduction Systems}
\label{ssec:crs}

A Combinatory Reduction System is defined by an alphabet and a set of
rewriting rules. We will first cover the alphabet.

\begin{definition}
  A \demph{CRS alphabet} consists of:
  \begin{itemize}
  \item a set $\Var$ of \emph{variables} (written lower-case as $x$, $y$,
    $z$,\ldots)
  \item a set $\MVar$ of \emph{metavariables} (written upper-case as $M$,
    $N$, \ldots), each with is own arity
  \item a set of \emph{function symbols}, each with its own arity
  \end{itemize}
\end{definition}

Let us sketch the difference between the variables in $\Var$ and the
metavariables in $\MVar$. The variables in $\Var$ are the variables of the
object-level terms, in our case it will be the variables of
$\banana{\lambda}$. The variables in $\MVar$ are the metavariables that
will occur in our reduction rules and which we will have to instantiate in
order to derive specific application of those rules. In other words, the
variables in $\Var$ are there to express the binding structure within the
terms being reduced and the metavariables in $\MVar$ are there to stand in
for specific terms when applying a reduction rule.

\begin{definition}
  The \demph{metaterms} of a CRS are given inductively:
  \begin{itemize}
  \item variables are metaterms
  \item if $t$ is a metaterm and $x$ a variable, then $[x]t$ is a metaterm
    called \demph{abstraction}
  \item if $F$ is an $n$-ary function symbol and $t_1$,\ldots,$t_n$ are
    metaterms, then $F(t_1,\ldots,t_n)$ is a metaterm
  \item if $M$ is an $n$-ary metavariable and $t_1$,\ldots,$t_n$ are
    metaterms, then $M(t_1,\ldots,t_n)$ is a metaterm
  \end{itemize}
\end{definition}

\begin{definition}
  The \demph{terms} of a CRS are its metaterms which do not contain any
  metavariables.
\end{definition}

To finish the formal introduction of CRSs, we give the definition of a CRS
reduction rule.

\begin{definition}
  A \demph{CRS reduction rule} is a pair of metaterms $s \to t$ such that:
  \begin{itemize}
  \item $s$ and $t$ are both closed, i.e.\ all variables are bound using
    the $[\_]\_$ abstraction binder
  \item $s$ is of the form $F(t_1,\ldots,t_n)$
  \item all the metavariables that occur in $t$ also occur in $s$
  \item any metavariable $M$ that occurs in $s$ only occurs in the form
    $M(x_1,\ldots,x_k)$, where $x_i$ are pairwise distinct variables
  \end{itemize}
\end{definition}

\begin{definition}
  A \demph{Combinatory Reduction System (CRS)} is a pair of a CRS alphabet
  and a set of CRS reduction rules.
\end{definition}

We will only sketch the way that a CRS gives rise to a reduction relation
and we will direct curious readers to Sections~11 and 12 of
$\cite{klop1993combinatory}$.

When we instantiate the metavariables in a CRS rule, we use a
\emph{valuation} that assigns to every $n$-ary metavariable a term with
holes labelled from 1 to $n$. The instantiation of $M(t_1,\ldots,t_n)$ then
replaces the metavariable $M$ using the valuation and then fills the holes
labelled $1,\ldots,n$ with the terms $t_1,\ldots,t_n$ respectively.

The crucial detail is that in a particular context, a metavariable can only
be instantiated with terms $M$ that do not contain any free variables bound
in that context. This means that for the instantiation of $M$ to contain a
variable bound in its context, $M$ must explicitly take that variable as an
argument. All other variables not explicitly declared can therefore be
safely assumed to not occur freely within $M$.

Consider the following examples of $\beta$ and $\eta$ reduction.

\begin{align*}
  \ap{(\lam{x}{M(x)})}{N} & \to M(N) \\
  \lam{x}{\ap{N}{x}} & \to N
\end{align*}

More formally written as:

\begin{align*}
  @(\lambda([x]M(x)),N) & \to M(N) \\
  \lambda([x]@(N,x)) & \to N
\end{align*}

where $\lambda$ is a unary function symbol and $@$ is a binary function
symbol. In both of the versions, $M$ is a unary metavariable and $N$ is a
nullary metavariable. In the rule for $\beta$-reduction, we can observe how
the idea of instantiating metavariables by terms with holes lets us express
the same idea for which we had to introduce the meta-level operation of
substitution. In the rule for $\eta$-reduction, we see that $N$ appears in
a context where $x$ is bound but it does not have $x$ as one of its
arguments. Therefore, it will be impossible to instantiate $N$ in such a
way that it contains a free occurrence of $x$. In both of those rules, we
were able to get rid of meta-level operations (substitution) and conditions
($x \notin FV(N)$) and have them both implemented by the formalism itself.


\subsection{\texorpdfstring{$\banana{\lambda}$}{Our Calculus} as a CRS}
\label{ssec:banana-as-crs}

We will now see how to rephrase the reduction rules of $\banana{\lambda}$ in
order to fit in to the CRS framework. We have already seen how to translate
the $\beta$ and $\eta$ rules in the previous subsection. The next
rules to address are the rules defining the semantics of the $\banana{}$
handlers.

We will repeat the rules for handlers to make the issue at hand clear.

\begin{tabular}{lr}
  $\ap{\cibanana}{(\ap{\eta}{N})} \to$ & rule $\banana{\eta}$ \\
  $\ap{M_\eta}{N}$ & \\
  \\
  $\ap{\cibanana}{(\ap{\ap{\op{op}_j}{N_{\mathrm{p}}}}{(\lam{x}{N_{\mathrm{c}}})})} \to$ & rule $\banana{\op{op}}$ \\
  $\ap{M_j}{\ap{N_{\mathrm{p}}}{(\lam{x}{\ap{\cibanana}{N_{\mathrm{c}}}})}}$
  & where $j \in I$ \\
  & and $x \notin \FV((M_i)_{i \in I}, M_\eta)$ \\
  \\
  $\ap{\cibanana}{(\ap{\ap{\op{op}_j}{N_{\mathrm{p}}}}{(\lam{x}{N_{\mathrm{c}}})})} \to$ & rule $\banana{\op{op}'}$ \\
  $\ap{\op{op}_j}{\ap{N_{\mathrm{p}}}{(\lam{x}{\ap{\cibanana}{N_{\mathrm{c}}}})}}$
  & where $j \notin I$ \\
  & and $x \notin \FV((M_i)_{i \in I}, M_\eta)$
\end{tabular}

The syntax of CRSs does not allow us to use the $(\onto{\op{op}_i}{M_i})_{i
  \in I}$ notation nor capture the $j \in I$ or $j \notin I$ conditions.
The symbols $\op{op}_i$ are problematic as well, since technically, they
are not concrete $\banana{\lambda}$ syntax but metavariables standing in for
operation symbols.

We do away with all of the above problems by expanding these meta-notations
and adding a separate rule for every possible instantiation of the
schema. This means that for each sequence of distinct operation symbols
$\op{op}_1$,\ldots,$\op{op}_n$, we end up with:
\begin{itemize}
\item a special rewriting rule
  $\ap{\banana{\onto{\op{op}_1}{M_1},\ldots,\onto{\op{op}_n}{M_n},\ 
    \onto{\eta}{M_\eta}}}{(\etaE{N})}
  \to \ap{M_\eta}{N}$
\item for every $1 \le i \le n$, a special rewriting rule \\
  $\ap{\banana{\onto{\op{op}_1}{M_1},\ldots,\onto{\op{op}_n}{M_n},\ 
    \onto{\eta}{M_\eta}}}{(\app{\op{op}_i}{N_{\mathrm{p}}}{(\lam{x}{N_{\mathrm{c}}(x)})})}
  \\ \to
  \app{M_i}{N_{\mathrm{p}}}{(\lam{x}{\ap{\banana{\onto{\op{op}_1}{M_1},\ldots,\onto{\op{op}_n}{M_n},\ \onto{\eta}{M_\eta}}}{N_{\mathrm{c}}(x)}})}$
\item for every $\op{op}' \in \EE \setminus \{\op{op}_i \| 1 \le i \le n\}$, a special
  rewriting rule \\
  $\ap{\banana{\onto{\op{op}_1}{M_1},\ldots,\onto{\op{op}_n}{M_n},\ 
    \onto{\eta}{M_\eta}}}{(\app{\op{op}'}{N_{\mathrm{p}}}{(\lam{x}{N_{\mathrm{c}}(x)})})}
  \\ \to
  \app{\op{op}'}{N_{\mathrm{p}}}{(\lam{x}{\ap{\banana{\onto{\op{op}_1}{M_1},\ldots,\onto{\op{op}_n}{M_n},\ \onto{\eta}{M_\eta}}}{N_{\mathrm{c}}(x)}})}$
\end{itemize}

The rule for the cherry $\cherry$ extraction operator is already in CRS
form, so all we have to do is address the rules for the $\CC$ operator. We
present them side-by-side in their original form and in CRS-style.

Original:
\begin{align*}
  \ap{\CC}{(\lam{x}{\ap{\eta}{M}})} & \to \ap{\eta}{(\lam{x}{M})} \\
  \ap{\CC}{(\lam{x}{\ap{\ap{\op{op}}{M_{\mathrm{p}}}}{(\lam{y}{M_{\mathrm{c}}})}})}
  & \to
  \ap{\ap{\op{op}}{M_{\mathrm{p}}}}{(\lam{y}{\ap{\CC}{(\lam{x}{M_{\mathrm{c}}})}})} \\
  & \mathrm{where\ } x \notin \FV(M_{\mathrm{p}})
\end{align*}

CRS-style:
\begin{align*}
  \ap{\CC}{(\lam{x}{\ap{\eta}{(M(x))}})} & \to \ap{\eta}{(\lam{x}{M(x)})} \\
  \ap{\CC}{(\lam{x}{\ap{\ap{\op{op}}{M_{\mathrm{p}}}}{(\lam{y}{M_{\mathrm{c}}(x,y)})}})}
  & \to \ap{\ap{\op{op}}{M_{\mathrm{p}}}}{(\lam{y}{\ap{\CC}{(\lam{x}{M_{\mathrm{c}}(x,y)})}})}
\end{align*}

We can see that the only difference is to replace ``simple'' metavariables
$M$, $M_{\mathrm{p}}$ and $M_{\mathrm{c}}$ with their higher-order
versions: the unary $M$, nullary $M_{\mathrm{p}}$ and binary
$M_{\mathrm{c}}$. We see that every CRS metavariable is applied to the
variables in its scope, except for $M_{\mathrm{p}}$, which thus loses
access to the variable $x$. This way, the condition that $x$ must not
appear free in $M_{\mathrm{p}}$ is now encoded directly in the reduction
rule itself.

In~\ref{ssec:crs}, we have said that a CRS is formed by a set of reduction
rules and by an alphabet. We have already seen all of the rules of our CRS
($\beta$ and $\eta$ were given at the end of~\ref{ssec:crs} and the
$\cherry$ rule is the same as the original one in~\ref{sec:reductions}). In
order to have a complete definition, all that remains is to identify the
alphabet.

The set of variables $Var$ is exactly the set of variables $\XX$ used in
the definition of $\banana{\lambda}$. The set of metavariables $MVar$
consists of the unary $M$, nullary $N$, nullary $N_{\mathrm{p}}$, unary
$N_{\mathrm{c}}$, nullary $M_{\mathrm{p}}$, binary $M_{\mathrm{c}}$,
nullary $M_i$ and nullary $M_\eta$. The set of function symbols is composed
of the following:

\begin{itemize}
\item the binary symbol $@$ for function application
\item the unary symbol $\lambda$ for function abstraction
\item a nullary symbol for every constant in the signature $\Sigma$
\item the unary symbol $\eta$ for the injection operator
\item a binary symbol $\op{op}$ for every $\op{op} \in \EE$
\item a $(n+2)$-ary symbol
  $(\ap{\banana{\onto{\op{op}_1}{\_},\ \ldots,\ \onto{\op{op}_n}{\_},\ \onto{\eta}{\_}}}{\_})$
  for every sequence $\op{op}_1,\ldots,\op{op}_n$ of distinct symbols from
  $\EE$ of length $n$
\item the unary symbol $\cherry$ for the extraction operator
\item the unary symbol $\CC$ for the $\CC$ operator
\end{itemize}

In giving the CRS-style reduction rules above, we have used the ``native''
syntax of $\banana{\lambda}$ instead of writing out everything in terms of
function symbols. For clarity, we give the rules governing the relationship
of the two. We write:

\begin{itemize}
\item $@(t,u)$ as $\ap{t}{u}$
\item $\lambda([x]t)$ as $\lam{x}{t}$
\item $\eta(t)$ as $\etaE{t}$
\item $\op{op}(t_{\mathrm{p}},[x]t_{\mathrm{c}})$ as
  $\app{\op{op}}{t_{\mathrm{p}}}{(\lam{x}{t_{\mathrm{c}}})}$\footnote{Note
    that with this translation,
    $\app{\op{op}}{t_{\mathrm{p}}}{(\lam{x}{t_{\mathrm{c}}})}$ does not
    contain $\lam{x}{t_{\mathrm{c}}}$ as a subterm. This is the same as in
    $\banana{\lambda}$, where the notion of evaluation context
    (see~\ref{sec:reductions}) does not identify $\lam{x}{t_{\mathrm{c}}}$,
    but rather $t_{\mathrm{c}}$, as a subterm of
    $\app{\op{op}}{t_{\mathrm{p}}}{(\lam{x}{t_{\mathrm{c}}})}$. This
    becomes important in our discussion of confluence since it makes it
    impossible to make the $\lambda$ disappear by something like
    $\eta$-reduction.}
\item
  $(\ap{\banana{\onto{\op{op}_1}{\_},\ \ldots,\ \onto{\op{op}_n}{\_},\ \onto{\eta}{\_}}}{\_})(t_1,\ldots,t_n,t_\eta,u)$
  as $\ap{\banana{\onto{\op{op}_1}{t_1},\ \ldots,\ \onto{\op{op}_n}{t_n},\ \onto{\eta}{t_\eta}}}{u}$
\item $\cherry(t)$ as $\ap{\cherry}{t}$
\item $\CC(t)$ as $\ap{\CC}{t}$
\end{itemize}

We have connected the terms of $\banana{\lambda}$ with CRS terms and we
have also expressed all of our reduction rules in terms of CRS reduction
rules. As in $\banana{\lambda}$, CRS then proceeds to take a context
closure of this redex-contractum relation. Our translation from
$\banana{\lambda}$ to a CRS also preserves subterms\footnote{More
  precisely, if $a$ is a subterm of $b$ in $\banana{\lambda}$ then the CRS
  version of $a$ is a subterm of the CRS version of $b$. In the other
  direction, whenever $a$ is a variable or a function-headed term which is
  a subterm of $b$ in the CRS version of $\banana{\lambda}$, then the
  corresponding $a$ in $\banana{\lambda}$ is a subterm of the corresponding
  $b$.}  and so we end up constructing the same reduction relation.


\subsection{Orthogonal CRSs}
\label{ssec:orthogonal-crs}

In order to use Theorem~\ref{thm:confluence-crs}, we need to show that our
CRS is orthogonal, so let us start us by looking at what ``orthogonal''
means in the context of CRSs.

\begin{definition}
  A CRS is \demph{orthogonal} if it is non-overlapping and left-linear.
\end{definition}

We will need to satisfy two criteria: no overlaps and left linearity. We
will start with the latter.

\begin{definition}
  A CRS is \demph{left-linear} if the left-hand sides of all its reduction
  rules are linear. A CRS metaterm is \demph{linear} if no metavariable
  occurs twice within it.
\end{definition}

By going through the rules we have given in~\ref{ssec:banana-as-crs}, we
can see at a glance that no rule uses the same metavariable twice in its
left-hand side and so our CRS is indeed left-linear.

\begin{definition}
  A CRS is \demph{non-overlapping} if:
  \begin{itemize}
    \item Let $r = s \to t$ be some reduction rule of the CRS and let
      $M_1$,\ldots,$M_n$ be all the metavariables occurring in the
      left-hand side $s$. Whenever we can instantiate the metavariables in
      $s$ such that the resulting term contains a redex for some other rule
      $r'$, then said redex must be contained in the instantiation of one
      of the metavariables $M_i$.
    \item Similarly, whenever we can instantiate the metavariables in $s$
      such that the resulting term \emph{properly contains} a redex
      \emph{for the same rule $r$}, then that redex as well must be
      contained in the instantiation of one of the metavariables $M_i$.
  \end{itemize}
\end{definition}

In simpler words, no left-hand side of any rule can contain bits which look
like the top of the left-hand side of some other rule. Let us try and
verify this property in $\banana{\lambda}$:
\begin{itemize}
\item The $\banana{}$ rules have no overlaps with any of the other
  rules. Their left-hand sides are constructed only of the $\banana{}$
  symbols and the $\op{op}$ and $\eta$ constructors. Since there is no
  reduction rule headed by $\op{op}$ and $\eta$, they have no overlap with
  any of the other rules. Furthermore, the three $\banana{}$ rules are
  mutually exclusive, so there is no overlap between themselves.
\item The $\cherry$ rule does not overlap with any of the other neither,
  since the left-hand side contains only $\cherry$ and $\eta$, and there is
  no reduction rule headed by $\eta$.
\item The $\CC$ rules are both mutually exclusives, so there is no overlap
  between the two. However, their left-hand sides are built not only out of
  $\CC$, $\op{op}$ and $\eta$, but also of $\lambda$, for which there is
  the $\eta$ reduction rule. Fortunately, in this case, the $\CC$ rules
  only apply when the $\lambda$-abstraction's body is an $\eta$ expression
  or an $\op{op}$ expression, whereas the $\eta$ rule applies only when the
  body is an application expression.\footnote{This is not so much a
    fortunate conincidence but rather a deliberate choice in the design of
    the calculus. For example, it is one of the reasons why, in $\calc$,
    $\etaE{x}$ is not decomposed as an application of the built-in function
    $\eta$ to $x$, but is treated as a special form.} Therefore, there is
  no overlap.
\end{itemize}

We have established that all the reduction rules in our system are pairwise
non-overlapping \emph{except} for $\beta$ and $\eta$. However,
these two have a notorious overlap.

We can instantiate the metavariables in the left-hand side of the $\beta$
rule to get a term which contains an $\eta$-redex which shares the
$\lambda$-abstraction with the $\beta$-redex.

$$
\ap{(\lam{x}{\ap{y}{x}})}{z}
$$

We can also instantiate the metavariables in the left-hand side of the
$\eta$ rule to create a $\beta$-redex which shares the application with
the $\eta$-redex.

$$
\lam{x}{\ap{(\lam{z}{z})}{x}}
$$

Because of these overlaps, the $\calc$ CRS is therefore \emph{not}
orthogonal. However, we can still make good use of
Theorem~\ref{thm:confluence-crs}.

\begin{lemma}\label{lem:confluence-int}
  \demph{Confluence of $\intcalc$}

  The $\banana{\lambda}$ reduction system without the $\eta$ rule is
  confluent.
\end{lemma}

\begin{proof}
  If we exclude the $\eta$ rule, we have a CRS which is left-linear and
  also non-overlapping.\footnote{We know that $\beta$ does not overlap
    any of the other rules. Neither does it overlap itself since its
    left-hand side does not have an application subexpression.} Therefore,
  it is orthogonal and thanks to Theorem~\ref{thm:confluence-crs}, also
  confluent.
\end{proof}

\begin{lemma}\label{lem:confluence-eta}
  \demph{Confluence of $\eta$-reduction}

  The reduction system on $\banana{\lambda}$ terms containing only the
  $\eta$ reduction rule is confluent.\footnote{This also holds for
    $\banana{\lambda}$ with sums and products since their rules are
    left-linear and do not overlap with the $\banana{\lambda}$ rules.}
\end{lemma}

\begin{proof}
  We have seen that $\eta$ is a valid left-linear CRS rule. It also
  does not overlap itself since its left-hand side does not contain any
  $\lambda$ subexpression. The CRS consisting of just the $\eta$ rule
  is therefore orthogonal and confluent.
\end{proof}


\subsection{Putting \texorpdfstring{$\eta$}{eta} Back in
  \texorpdfstring{$\banana{\lambda}$}{Our Calculus}}
\label{ssec:confluence-eta}

We have shown that both $\intcalc$ and $\eta$ are confluent. The reduction
relation of the complete $\banana{\lambda}$ calculus is the union of these
two reduction relations. Using the Lemma of Hindley-Rosen (1.0.8.(2)
in~\cite{klop1992term}), we can show that this union is confluent by
showing that the two reduction relations commute together.

\begin{definition}
  Let $\to_1$ and $\to_2$ be two reduction relations on the same set of
  terms $A$. $\to_1$ and $\to_2$ \demph{commute} if for every $a,b,c \in A$
  such that $a \tto_1 b$ and $a \tto_2 c$, there exists a $d \in A$ such
  that $b \tto_2 d$ and $c \tto_1 d$.
\end{definition}

\begin{lemma}\label{lem:hindley-rosen}
  \demph{Lemma of Hindley-Rosen}~\cite{klop1992term}
  
  Let $\to_1$ and $\to_2$ be two confluent reduction relations on the same
  set of terms. If $\to_1$ and $\to_2$ commute, then the reduction relation
  $\to_1 \cup \to_2$ is confluent.
\end{lemma}

We will not be proving the commutativity directly from the
definition. Instead, we will use a lemma due to Hindley (1.0.8.(3)
in~\cite{klop1992term}).

\begin{lemma}\label{lem:commutativity}
  Let $\to_1$ and $\to_2$ be two reduction relations on the same set of
  terms $A$. Suppose that whenever there are $a,b,c \in A$ such that
  $a \to_1 b$ and $a \to_2 c$, there is also some $d \in A$ such that
  $b \tto_2 d$ and $c \to_1^= d$ (meaning $c \to_1 d$ or $c = d$). In that
  case, $\to_1$ commutes with $\to_2$.~\cite{klop1992term}
\end{lemma}

We can use this to prove that $\banana{\lambda}_{-\eta}$ commutes with the
$\eta$ reduction rule.

\begin{lemma}\label{lem:eta-commutes}
  \demph{Commutativity of $\eta$ and $\banana{\lambda}_{-\eta}$}

  The reduction relations induced by $\eta$ and by the rest of the
  $\banana{\lambda}$ rules commute.
\end{lemma}

\begin{proof}
  We will prove this lemma by an appeal to
  Lemma~\ref{lem:commutativity}. Let $\to_\eta$ be the reduction relation
  induced by the rule $\eta$ and $\to_{\intcalc}$ the reduction
  relation induced by all the other reduction rules in
  $\banana{\lambda}$. We need to prove that for all terms $a$, $b$ and $c$
  where $a \to_{\intcalc} b$ and $a \to_\eta c$, we have a term $d$ such that
  $b \tto_\eta d$ and $c \to_{\intcalc}^= d$.

  This will turn out to be a routine proof by induction on the structure of
  the term $a$. The base cases are trivial since terms without any proper
  subterms happen to have no redexes in $\banana{\lambda}$ and therefore
  trivially satisfy the criterion. In the inductive step, we will proceed
  by analyzing the relative positions of the redexes which led to the
  reductions $a \to_{\intcalc} b$ and $a \to_\eta c$.
  \begin{itemize}
  \item If both reductions occurred within a common subterm of $a$, i.e.\
    $a = C[a']$, $b = C[b']$ and $c = C[c']$ while at the same time
    $a' \to_{\intcalc} b'$ and $a' \to_\eta c'$, we can use the induction
    hypothesis for $a'$. This gives us a $d'$ such that $b' \tto_\eta d'$
    and $c' \to_{\intcalc}^= d'$ and therefore we also have $d = C[d']$ with
    $b \tto_\eta d$ and $c \to_{\intcalc}^= d$.
  \item If both reductions occurred within non-overlapping subterms of $a$,
    i.e.\ $a = C[a_1, a_2]$, $b = C[b', a_2]$ and $c = C[a_1, c']$ with
    $a \to_{\intcalc} b$ and $a \to_\eta c$: We can take $d = C[b', c']$ since
    we have $b \tto_\eta d$ in one step and $c \to_{\intcalc}^= d$ in one step
    too.
  \item If the redex in $a \to_{\intcalc} b$ is the entire term $a$, but the
    redex in $a \to_\eta c$ is a proper subterm of $a$: We will solve this
    by case analysis on the form of $a$:
    \begin{itemize}
    \item If $a$ is an application: Since $a$ is an application and also a
      $\intcalc$-redex, it must match the left-hand side of the $\beta$
      rule, $\ap{(\lam{x}{M(x)})}{N}$, and $b$ must be $M(N)$.
      \begin{itemize}
      \item We will first deal with the case when the $\eta$-redex which
        lead to $c$ originated in $M(x)$. In that case
        $M(x) \to_\eta M'(x)$ and $c = \ap{(\lam{x}{M'(x)})}{N}$. Our
        sought-after $d$ is then $M'(N)$, since $c \to_{\intcalc}^= d$ via
        $\beta$ in one step and $b = M(N) \tto_\eta d = M'(N)$.
      \item Now we get to one of the two interesting cases which
        necessitated this whole lemma: the overlap between $\beta$ and
        $\eta$, with $\beta$ on the top. If the $\eta$-redex did not
        originate in $M(x)$, then the $\eta$-redex must be
        $\lam{x}{M(x)}$. Therefore, $M = \ap{T}{x}$ and
        $a = \ap{(\lam{x}{\ap{T}{x}})}{N}$. Performing the $\eta$-reduction
        yields $c = \ap{T}{N}$. In this case, both $b$ and $c$ are equal to
        $\ap{T}{N}$ and so we can choose $\ap{T}{N}$ as our $d$.
      \end{itemize}
    \item If $a$ is any other kind of term: Let $l \to r$ be the rule used
      in $a \to_{\intcalc} b$. Not counting $\beta$, which only acts on
      applications and which we dealt with just above, the rules of
      $\intcalc$ do not overlap with the $\eta$ rule. This means the
      $\eta$-redex which led to $c$ must lie entirely inside a part of $l$
      which corresponds to a metavariable. Let $M$ be that metavariable,
      then we will decompose $l$ into $L(M)$ and $r$ into $R(M)$. We have
      $a = L(a')$ for some $a'$, $b = R(a')$ and
      $c = L(a'')$\footnote{Since our rules are left-linear, $M$ is
        guaranteed to appear in $L(M)$ at most once. Therefore, if
        $a' \to_\eta a''$ in one step, then also $L(a') \to_\eta L(a'')$ in
        one step as well.}. Our $d$ will be $R(a'')$ and we have
      $b = R(a') \tto_\eta d = R(a'')$ in several steps\footnote{$a'$ can
        occur multiple times in $R(a')$ when the rule $l \to r$ is
        duplicating (which is actually the case for the $\banana{\op{op}}$
        rules). However, we are able to go from $R(a')$ to $R(a'')$ in
        multiple steps. NB: This is why we use
        Lemma~\ref{lem:commutativity} instead of trying to prove
        commutativity directly.} and
      $c = L(a'') \to_{\intcalc}^= d = R(a'')$ in one step of $l \to r$.
    \end{itemize}
  \item If the redex in $a \to_\eta c$ is the entire term $a$, but the
    redex in $a \to_{\intcalc} b$ is a proper subterm of $a$: In this case,
    $a$ must be an abstraction that matches the left-hand side of the
    $\eta$ rule, i.e.\ $a = \lam{x}{\ap{N}{x}}$. Also, we have $c = N$.
    \begin{itemize}
    \item As before, we will first deal with the case when the
      $\intcalc$-redex is contained completely within $N$. Then
      $N \to_{\intcalc} N'$ and $b = \lam{x}{\ap{N'}{x}}$. The common
      reduct $d$ is $N'$ since $b \tto_\eta d$ in one step and
      $c = N \to_{\intcalc}^= d = N'$ as established before.
    \item Now this is where we deal with the second overlap between $\beta$
      and $\eta$ in our reduction system, the one with $\eta$ on top. The
      $\intcalc$-redex in $a$ must be $\ap{N}{x}$ and the reduction rule in
      question must therefore be $\beta$. Therefore, $N = \lam{y}{T(y)}$
      and $a = \lam{x}{\ap{(\lam{y}{T(y)})}{x}}$. Performing the
      $\beta$-reduction gives us $b = \lam{x}{T(x)}$ which is, however,
      equal to $c = N = \lam{y}{T(y)}$. So we can choose $d = b$ and we are
      done.
    \end{itemize}
  \item If $a$ is the redex for both reductions $a \to_{\intcalc} b$ and
    $a \to_\eta c$, then $a$ must match the left-hand side of a $\intcalc$
    rule and the $\eta$ rule. However, this is impossible since the
    left-hand side of the $\eta$ rule is headed by abstraction, which is
    the case for none of the rules of $\intcalc$.
  \end{itemize}
\end{proof}

Equipped with this lemma, we can go on to prove our main result,
Theorem~\ref{thm:confluence}, the confluence of $\banana{\lambda}$.

\begin{theorem}\label{thm:confluence}
  \demph{Confluence of $\banana{\lambda}$}
  
  The reduction relation $\to$ on the set of $\banana{\lambda}$ terms,
  defined by the reduction rules in~\ref{sec:reductions}, is confluent.
\end{theorem}

\begin{proof}
  From Lemma~\ref{lem:confluence-int}, we know that the
  $\banana{\lambda}_{-\eta}$ system is confluent and from
  Lemma~\ref{lem:confluence-eta}, we know that the $\eta$ reduction rule is
  confluent as well. Lemma~\ref{lem:eta-commutes} tells us that these two
  reduction systems commute and therefore, by
  Lemma~\ref{lem:hindley-rosen}, their union, which is the
  $\banana{\lambda}$ reduction system, commutes as well.
\end{proof}


\section{Termination}
\label{sec:termination}

\begin{definition}
  A reduction relation is \demph{terminating} if there is no infinite chain
  $M_1 \to M_2 \to \ldots$.
\end{definition}

In this section, we will prove termination with a similar strategy as the
one we employed for confluence. $\banana{\lambda}$ is an extension of the
$\lambda$-calculus with computation types and some operations on
computations. Our computations can be thought of as algebraic expressions,
i.e.\ they have a tree-like inductive structure. The reason that all
computations in $\banana{\lambda}$ terminate is that the operations defined on
computations rely on well-founded recursion. However, it is quite tricky to
go from this intuition to a formal proof of termination. Fortunately, we
can rely on existing results.

Blanqui, Jouannaud and Okada have introduced Inductive Data Type Systems
(IDTSs)~\cite{blanqui2002inductive,blanqui2000termination}. Like the CRSs
of the previous section, IDTSs are a class of rewriting systems for which
we can prove certain interesting general results. In this section, we will
start by examining the definition of an IDTS and fitting $\banana{\lambda}$
into that definition. The theory of IDTSs comes with a sufficient condition
for termination known as the General Schema. We will show that our IDTS
validates this schema and is therefore terminating.

The encoding of $\calc$ into an IDTS will omit the reduction rules
governing the $\CC$ construction. Our termination (and strong
normalization) result will therefore be limited to the $\calc$ calculus
without $\CC$. We address the difficulties with the $\CC$ operator in
Subsection~\ref{ssec:c-termination}.

\begin{notation}
  The \demph{$\CC$-free $\banana{\lambda}$ calculus $\poorcalc$} is the
  $\banana{\lambda}$ calculus without the $\CC_\eta$ and $\CC_{\op{op}}$
  reduction rules.
\end{notation}

The plan will look like this:

\begin{itemize}
\item IDTS = $\lambda$-calculus with inductive types (\ref{ssec:idts})
\item The $\idts$ IDTS (\ref{ssec:banana-idts})
  \begin{itemize}
  \item if $\idts$ terminates, then $\poorcalc$ terminates
    (Lemma~\ref{lem:banana-tau-termination})
  \end{itemize}
\item Blanqui~\cite{blanqui2002inductive}: General Schema $\Rightarrow$ termination
  (\ref{ssec:termination-for-idts})
  \begin{itemize}
  \item Theorem~\ref{thm:idts-termination}: $\idts$ terminates (via Blanqui~\cite{blanqui2002inductive})
  \item Theorem~\ref{thm:termination}: $\poorcalc$ terminates (via
    Lemma~\ref{lem:banana-tau-termination})
  \item and therefore $\poorcalc$ is strongly normalizing
    (Theorem\ref{thm:strong-normalization})
  \end{itemize}
\item The $\CC$ operator (\ref{ssec:c-termination})
\end{itemize}


\subsection{Inductive Data Type Systems}
\label{ssec:idts}

We will reformulate here the original definition of Inductive Data Type
Systems from~\cite{blanqui2002inductive}.

\begin{definition}
  An \demph{Inductive Data Type System (IDTS)} is a pair of an IDTS
  signature and a set of IDTS rewrite rules.
\end{definition}

Just like a CRS, an IDTS is a signature/alphabet coupled with some rewrite
rules. Let us first look at the signature and the rules for building terms
on top of the signature; the rewrite rules will follow.

Since IDTS is based on the simply-typed $\lambda$-calculus, we start with
the definition of simple types.

\begin{definition}
  The set of \demph{types $T(\II)$} contains:
  \begin{itemize}
  \item all the types from $\II$
  \item a type $\alpha \to \beta$ for every $\alpha$ and $\beta$ in $T(\II)$
  \end{itemize}
\end{definition}

\begin{definition}
  An \demph{IDTS signature} consists of:
  \begin{itemize}
  \item $\II$, a set of basic types that we refer to as \demph{inductive types}
  \item $\XX$, a family $(X_\tau)_{\tau \in T(\II)}$ of sets of \demph{variables}
  \item $\FF$, a family ${(F_{\alpha_1,\ldots,\alpha_n,\beta})}_{\alpha_1,\ldots,\alpha_n,\beta \in T(\II)}$ of sets of \demph{function symbols}
  \item $\CC$, a family ${(C_{\alpha_1,\ldots,\alpha_n,\beta})}_{\alpha_1,\ldots,\alpha_n,\beta \in T(\II)}$ of sets of \demph{constructors}
    such that $C_{\alpha_1,\ldots,\alpha_n,\beta} \subseteq F_{\alpha_1,\ldots,\alpha_n,\beta}$
  \end{itemize}
\end{definition}

Unlike CRS, there is no distinction between object-level variables and the
metavariables that appear in reduction rules and also no distinction
between terms and metaterms. All these roles are filled by the variables in
$\XX$ and by $\lambda$-terms.

\begin{definition}
  The \demph{typed terms} of an IDTS are given inductively:
  \begin{itemize}
  \item variables from $X_\tau$ are terms of type $\tau$
  \item if $t$ is a term of type $\beta$ and $x$ a variable from
    $X_\alpha$, then $\lam{x}{t}$ is a term of type $\alpha \to \beta$
  \item if $t$ is a term of type $\alpha \to \beta$ and $u$ is a term of
    type $\alpha$, then $\ap{t}{u}$ is a term of type $\beta$
  \item if $f$ is a function symbol from
    $F_{\alpha_1,\ldots,\alpha_n,\beta}$ and $t_1$,\ldots,$t_n$ are terms
    of types $\alpha_1,\ldots,\alpha_n$ respectively, then
    $f(t_1,\ldots,t_n)$ is a term of type $\beta$
  \end{itemize}
\end{definition}

The terms of an IDTS are $\lambda$-terms extended with function
symbols. These function symbols stand for the constructors and operators on
the inductive types that we will want to enrich the $\lambda$-calculus
with. The meaning behind the $\lambda$-calculus primitives of
$\lambda$-abstraction and application is defined using the $\beta$ and
$\eta$ reductions. As for the function symbols, their meaning is defined by
the given IDTS rewrite rules.

\begin{definition}
  An \demph{IDTS rewrite rule} is a pair of terms $l \to r$ such that:
  \begin{itemize}
  \item $l$ is of the form $f(t_1,\ldots,t_n)$\footnote{We say that $l$ is
      \emph{headed} by the function symbol $f$.}
  \item $FV(r) \subseteq FV(l)$\footnote{I.e.\ the free variables of $r$
      are included in the free variables of $l$, where the notion of a free
      variable is the same one as in the simply-typed $\lambda$-calculus.}
  \item $l$ and $r$ have the same type
  \end{itemize}
\end{definition}

\begin{definition}
  A term $u$ \demph{$R$-rewrites} to a term $u'$ with the rule
  $l \to r \in R$, written as $u \to_R u'$, if there exists a substitution
  $\theta$ such that $u = l\theta$ and
  $u' = r\theta$.\footnote{$\alpha$-equivalent terms are considered
    equal. However, terms are \emph{not} matched modulo $\beta$ or $\eta$
    equivalence.} Furthermore, this relation is closed on
  contexts/subterms: if $u$, a subterm of $v$, rewrites to $u'$, then $v$
  rewrites to $v'$, where $v'$ is $v$ in which the occurrence of $u$ was
  replaced with $u'$.
\end{definition}

The reduction relation induced by an IDTS with the rewrite rules $R$ is
$\to_{\beta,\eta} \cup \to_R$. In the next subsection, we will see how we
can represent our $\poorcalc$ calculus as an IDTS.


\subsection{\texorpdfstring{$\poorcalc$}{Our Calculus} as an IDTS}
\label{ssec:banana-idts}

We will try to plug $\banana{\lambda}$ into the IDTS framework in order to
benefit from its general termination results. The biggest obstacle will be
that IDTSs assign a fixed type to every symbol. In $\banana{\lambda}$,
symbols are polymorphic: the $\eta$ constructor can produce expressions
like $\etaE{\star} : \FF_E(1)$ or
$\etaE{(\lam{x}{x})} : \FF_E(\alpha \to \alpha)$ and that for any choice of
$E$. We would therefore like to replace function symbols such as $\eta$
with specialized symbols $\eta_{\FF_E(\alpha)}$. For a given type $\alpha$
and effect signature $E$, the symbol $\eta_{\FF_E(\alpha)}$ would have the
type $\alpha \to \FF_E(\alpha)$, i.e.\ it would belong to
$F_{\alpha,\FF_E(\alpha)}$.

We will call this calculus with specialized symbols
$\banana{\lambda}_\tau$. There will not be a bijection between $\poorcalc$
and $\banana{\lambda}_\tau$ since a single term in $\poorcalc$ will
generally correspond to a multitude of specialized versions in
$\banana{\lambda}_\tau$ (think of $\lam{x}{x}$ in $\banana{\lambda}$ versus
$\lam{x_\iota}{x_\iota}$, $\lam{x_o}{x_o}$\ldots\ in
$\banana{\lambda}_\tau$). Therefore, the results we prove for
$\banana{\lambda}_\tau$ will not automatically transfer to
$\poorcalc$. In the rest of this subsection, we will elaborate the
definition of $\banana{\lambda}_\tau$ and show why termination carries over
from $\banana{\lambda}_\tau$ to $\poorcalc$.


\subsubsection{Defining $\banana{\lambda}_\tau$}
\label{sssec:banana-tau}

$\banana{\lambda}_\tau$ will be defined as an IDTS. This means we need to
first identify the signature. The inductive types $\II$ of
$\banana{\lambda}_\tau$ will be the set of the atomic types and the
computation types of $\calc$. This means that the set of types of this
IDTS, $T(\II)$, will be equal to the set of types of $\calc$.
\footnote{Note that throughout this section, we will make a distinction
  between two notions of base types: atomic types and inductive
  types. Atomic types are the base types of $\calc$. Inductive types are
  the base types of IDTSs. In our particular IDTS, inductive types include
  the base types of $\calc$, i.e.\ the atomic types, and the computation
  types of $\calc$. This means that from the point of view of the IDTS,
  computation types are just another base type.}

Next, we will introduce function symbols for all the syntactic
constructions of $\poorcalc$:

\begin{itemize}
\item $c \in F_{\alpha}$ for every constant $c : \alpha \in \Sigma$
\item $\eta_{\alpha, E} \in F_{\alpha, \FF_E(\alpha)}$ (i.e.\ for every
  type $\alpha$ and every effect signature $E$, there will be a function
  symbol $\eta_{\alpha,E}$ of type $\alpha \to \FF_E(\alpha)$)
\item $\op{op}_{\gamma, E} \in F_{\alpha, \beta \to \FF_E(\gamma),
  \FF_E(\gamma)}$ for any operation symbol $\op{op}$ from $\EE$ and any $E$
  such that $\typedop{op}{\alpha}{\beta} \in E$
\item $\cherry_\alpha \in F_{\FF_\emptyset(\alpha), \alpha}$
\item $\banana{}_{\op{op}_1, \ldots, \op{op}_n, \gamma, \delta, E, E'} \in F_{\alpha_1 \to (\beta_1 \to \FF_{E'}(\delta)), \ldots,
  \alpha_n \to (\beta_n \to \FF_{E'}(\delta)), \gamma \to \FF_{E'}(\delta),
  \FF_E(\gamma), \FF_{E'}(\delta)}$ where:
  \begin{itemize}
  \item $\op{op}_1 : \alpha_1 \rightarrowtail \beta_1 \in E$, \ldots,
    $\op{op}_n : \alpha_n \rightarrowtail \beta_n \in E$
  \item $E \setminus \{\op{op}_1, \ldots, \op{op}_n\} \subseteq E'$
  \end{itemize}
\item
  $\CC_{\alpha,\beta,E} \in F_{\alpha \to \FF_E(\beta),\FF_E(\alpha \to
    \beta)}$\footnote{Even though the $\idts$ IDTS will not include the
    reduction rules for $\CC$, we still include the function symbols in
    order to get the same term language as in $\calc$.}
\end{itemize}

The list above is based on the typing rules of $\banana{\lambda}$ found on
Figure~\ref{fig:types}. We convert the typing rules of $\banana{\lambda}$
into the typed function symbols of $\banana{\lambda}_\tau$ with the
following process:

\begin{itemize}
\item We take a typing rule of $\banana{\lambda}$, other than [var], [app]
  and [abs] (since variables, applications and $\lambda$-abstractions are
  already present in the language of IDTS terms).
\item We identify all the type-level metavariables. That is, metavariables
  $\alpha, \beta, \gamma \ldots$ ranging over types, metavariables
  $E, E', \ldots$ ranging over effect signatures and metavariables
  $\op{op}$ ranging over operation symbols.
\item We strip these metavariables down to a minimal non-redundant set
  (e.g.\ in the $[\op{op}]$ rule, we have that
  $\typedop{op}{\alpha}{\beta} \in E$, which means $E$ and $\op{op}$
  determine $\alpha$ and $\beta$ and therefore $\alpha$ and $\beta$ are
  redundant)
\item We introduce a family of symbols: for every possible instantiation of
  the metavariables mentioned above, we will have a different symbol. The
  arity of the symbol will correspond to the number of typing judgments
  that serve as hypotheses to the typing rule. The types of the arguments
  and of the result will be derived from the types of the judgments of the
  hypotheses and the conclusion respectively. If a variable of type
  $\alpha$ is bound in a premise of type
  $\beta$, then that will correspond to the IDTS function type $\alpha \to
  \beta$.
  \begin{itemize}
  \item Example: In the $[\eta]$ rule, we have two metavariables: $\alpha$
    standing in for a type and $E$ standing in for an effect signature. The
    rule has one typing judgment hypothesis. For every type $\alpha$ and
    every effect signature $E$, we will therefore have a unary symbol
    $\eta_{\alpha, E}$ of type $\alpha \to \FF_E(\alpha)$ (i.e.\ belonging
    to $F_{\alpha, \FF_E(\alpha)}$).
  \end{itemize}
\end{itemize}

A specifically-typed symbol in $\banana{\lambda}_\tau$ then corresponds to
an instantiation of the type metavariables in a $\banana{\lambda}$ typing
rule. We can follow this correspondence further and see that
$\banana{\lambda}_\tau$ IDTS terms, written using the above function
symbols, correspond to typing derivations in $\banana{\lambda}$.

Our signature now has types and function symbols. To complete the
signature, we need to designate which function symbols are constructors. In
our IDTS, the only constructors are the $\eta$ and the $\op{op}$
symbols. Finally, to complete the IDTS signature, we need to specify the
sets of variables and so we will take some arbitrary sets with $x_\tau,
y_\tau, M_\tau, N_\tau, \ldots \in X_\tau$.

\begin{figure}
  \centering
  \begin{tabular}{lr}
  let $\banana{}^{(\op{op}_i)_{i \in
    I}}_{\typehint{\FF_E(\gamma),\FF_{E'}(\delta)}}(N_{\typehint{\FF_E(\gamma)}})$ & \\
  \multicolumn{2}{l}{$ = \banana{}^{(\op{op}_i)_{i \in I}}_{\typehint{(\alpha_i \to (\beta_i \to \FF_{E'}(\delta)) \to
  \FF_{E'}(\delta))_{i \in I}, \gamma \to \FF_{E'}(\delta), \FF_E(\gamma),
  \FF_{E'}(\delta)}} ((M^i_{\typehint{\alpha_i \to (\beta_i \to
    \FF_{E'}(\delta)) \to \FF_{E'}(\delta)}})_{i \in I},
    M^\eta_{\typehint{\gamma \to \FF_{E'}(\delta)}}, N_{\typehint{\FF_E(\gamma)}})$} \\
  \\
  $\banana{}^{(\op{op}_i)_{i \in I}} (\eta (N)) \to$ & rules $\banana{\eta}^{(\op{op}_i)_{i \in I}}_{\FF_E(\gamma),\FF_{E'}(\delta)}$ \\
  $\ap{M^\eta}{N}$ & \\
  \multicolumn{2}{l}{$\banana{}^{(\op{op}_i)_{i \in I}}_{\typehint{\FF_E(\gamma), \FF_{E'}(\delta)}} (\eta_{\typehint{\gamma, \FF_E(\gamma)}} (N_{\typehint{\gamma}})) \to$} \\
  \multicolumn{2}{l}{$\ap{M^\eta_{\typehint{\gamma \to \FF_{E'}(\delta)}}}{N_{\typehint{\gamma}}}$} \\
  \\
  $\banana{}^{(\op{op}_i)_{i \in I}} (\op{op}_j (N^{\mathrm{p}}, N^{\mathrm{c}})) \to$ & rules $\banana{\op{op}}^{(\op{op}_i)_{i \in I},j}_{\FF_E(\gamma),\FF_{E'}(\delta)}$ \\
  $\app{M^j}{N^{\mathrm{p}}}{(\lam{x}{\banana{}^{(\op{op}_i)_{i \in I}}(\ap{N^{\mathrm{c}}}{x})})}$ & where $j \in I$ \\
  \multicolumn{2}{l}{$\banana{}^{(\op{op}_i)_{i \in I}}_{\typehint{\FF_E(\gamma), \FF_{E'}(\delta)}} (\op{op}_{j \typehint{\alpha, \beta \to \FF_E(\gamma), \FF_E(\gamma)}} (N^{\mathrm{p}}_{\typehint{\alpha}}, N^{\mathrm{c}}_{\typehint{\beta \to \FF_E(\gamma)}})) \to$} \\
  \multicolumn{2}{l}{$\app{M^j_{\typehint{\alpha \to (\beta \to
    \FF_{E'}(\delta)) \to \FF_{E'}(\delta)}}}{N^{\mathrm{p}}_{\typehint{\alpha}}}{(\lam{x_{\typehint{\beta}}}{\banana{}^{(\op{op}_i)_{i \in I}}_{\typehint{\FF_E(\gamma), \FF_{E'}(\delta)}} (\ap{N^{\mathrm{c}}_{\typehint{\beta \to \FF_E(\gamma)}}}{x_{\typehint{\beta}}})})}$} \\
  \\
  $\banana{}^{(\op{op}_i)_{i \in I}} (\op{op}_j (N^{\mathrm{p}}, N^{\mathrm{c}})) \to$ & rules $\banana{\op{op}'}^{(\op{op}_i)_{i \in I},j}_{\FF_E(\gamma),\FF_{E'}(\delta)}$ \\
  $\op{op}_j (N^{\mathrm{p}}, \lam{x}{\banana{}^{(\op{op}_i)_{i \in I}} (\ap{N^{\mathrm{c}}}{x})})$ & where $j \notin I$ \\
  \multicolumn{2}{l}{$\banana{}^{(\op{op}_i)_{i \in I}}_{\typehint{\FF_E(\gamma), \FF_{E'}(\delta)}} (\op{op}_{j \typehint{\alpha, \beta \to \FF_E(\gamma), \FF_E(\gamma)}} (N^{\mathrm{p}}_{\typehint{\alpha}}, N^{\mathrm{c}}_{\typehint{\beta \to \FF_E(\gamma)}})) \to$} \\
  \multicolumn{2}{l}{$\op{op}_{j \typehint{\alpha, \beta \to \FF_{E'}(\delta), \FF_{E'}(\delta)}} (N^{\mathrm{p}}_{\typehint{\alpha}}, \lam{x_{\typehint{\beta}}}{\banana{}^{(\op{op}_i)_{i \in I}}_{\typehint{\FF_E(\gamma), \FF_{E'}(\delta)}} (\ap{N^{\mathrm{c}}_{\typehint{\beta \to \FF_E(\gamma)}}}{x_{\typehint{\beta}}})})$} \\
  \\
  $\cherry (\eta (N)) \to$ & rules $\cherry_\alpha$ \\
  $N$ & \\
  \multicolumn{2}{l}{$\cherry_{\typehint{\FF_\emptyset(\alpha), \alpha}} (\eta_{\typehint{\alpha, \FF_\emptyset(\alpha)}} (N_{\typehint\alpha})) \to$} \\
  \multicolumn{2}{l}{$N_{\typehint\alpha}$}
  \end{tabular}
  
  \caption{\label{fig:tau-types} The IDTS rewrite rules for
    $\banana{\lambda}_\tau$, both with and without type annotations
    $\banana{\lambda}$.}
  
\end{figure}

We have finalized the signature, now we need to give the rewrite rules. The
rules for $\banana{\lambda}_\tau$ are given in
Figure~\ref{fig:tau-types}. An important property of an IDTS rewrite rule
is that both its left-hand and right-hand side are well-typed and that they
have the same type. In order to facilitate the reader's verification that
this is indeed the case, we have used a different labelling scheme for
function symbols. When we write $f_{\alpha_1,\ldots,\alpha_n,\beta}$, we
are referring to the instance of symbol $f$ which has the type
$\alpha_1 \to \ldots \to \alpha_n \to \beta$ (i.e.\ belongs to
$F_{\alpha_1,\ldots,\alpha_n,\beta}$). This way, instead of using a symbol
name like $\eta_{\alpha,E}$, forcing you to look up its type
$\alpha \to \FF_E(\alpha)$, we will refer to this symbol directly as
$\eta_{\alpha,\FF_E(\alpha)}$.

In Figure~\ref{fig:tau-types}, you will also find the rewrite rules with
all the subscripts removed. This allows you to get a high-level look at the
terms without any of the type annotation noise.

When describing the rewrite rules for handlers, we introduce a shortcut
$\banana{}^{(\op{op}_i)_{i \in
    I}}_{\typehint{\FF_E(\gamma),\FF_{E'}(\delta)}}(N_{\typehint{\FF_E(\gamma)}})$,
which stands for $\banana{}$ partially applied to the clauses $M^i$ and
$M^\eta$. We then reuse this shortcut in all of the $\banana{}$ rules.


\subsubsection{Connecting $\banana{\lambda}_\tau$ to $\calc$}
\label{sssec:connecting-bananas}

We have given a complete formal definition of $\banana{\lambda}_\tau$. This
will let us find a proof of termination for $\banana{\lambda}_\tau$ using
the theory of IDTSs. However, in order to carry over this result to our
original calculus, we will need to formalize the relationship between the
two.

\begin{definition}
  \demph{Term} is a function from $\banana{\lambda}_\tau$ terms to
  $\poorcalc$ terms which removes any type annotations (the subscripts on
  function symbols, variables and metavariables) and translates
  $\banana{\lambda}_\tau$ syntax to $\banana{\lambda}$ syntax using the
  following equations:
  
  \begin{align*}
    & \Term(x) &&= x \\
    & \Term(c) &&= c \\
    & \Term(\lam{x}{M}) &&= \lam{x}{\Term(M)} \\
    & \Term(\ap{M}{N}) &&= \ap{(\Term(M))}{(\Term(N))} \\
    & \Term(\eta(M)) &&= \etaE{(\Term(M))} \\
    & \Term(\op{op}(M^{\mathrm{p}}, M^{\mathrm{c}})) &&=
        \begin{cases}
          \app{\op{op}}{(\Term(M^{\mathrm{p}}))}{(\Term(M^{\mathrm{c}}))}
            \\ \text{\qquad if $\Term(M^{\mathrm{c}})$ is a $\lambda$-abstraction} \\
          \app{\op{op}}{(\Term(M^{\mathrm{p}}))}{(\lam{x}{\ap{(\Term(M^{\mathrm{c}}))}{x}})}
            \\ \text{\qquad otherwise; $x$ is also assumed fresh for $\Term(M^{\mathrm{c}})$} 
        \end{cases} \\
    & \Term(\cherry(M)) &&= \ap{\cherry}{(\Term(M))} \\
    & \Term(\banana{}_{\op{op}_1, \ldots, \op{op}_n}(M_1, \ldots, M_n, M_\eta, N)) &&= \ap{\banana{\onto{\op{op}_1}{\Term(M_1)},\ \onto{\op{op}_n}{\Term(M_n)},\ \onto{\eta}{\Term(M_\eta)}}}{(\Term(N))} \\
    & \Term(\CC(M)) &&= \ap{\CC}{(\Term(M))}
  \end{align*}
\end{definition}

\begin{definition}
  \demph{Types} is a function from $\banana{\lambda}$ terms to sets of
  $\banana{\lambda}_\tau$ terms, defined by the equation below.
  
  $$
  \Types(M) = \{ m \mid \Term(m) = M \}
  $$
\end{definition}

\begin{lemma}
  \label{lem:infinite-chains}
  Let $M$ and $N$ be $\banana{\lambda}$ terms. Then,

  $$
  M \to_{\poorcalc} N \quad \Rightarrow \quad
  \forall m \in \Types(M).\ \exists n \in \Types(N).\ m \to n
  $$
  
  In the above, upper-case letters stand for $\banana{\lambda}$ terms,
  while lower-case letters stand for $\banana{\lambda}_\tau$ terms.
\end{lemma}

\begin{proof}
  This property is essentially a stronger kind of subject reduction for
  $\poorcalc$. In proofs of subject reduction, we examine every reduction
  rule and we show how a typing derivation of the redex can be transformed
  into a typing derivation of the contractum. We can think of
  $\banana{\lambda}_\tau$ terms as $\banana{\lambda}$ typing
  derivations. The reduction rules in Figure~\ref{fig:tau-types} are the
  rules which tell us how to take a typing of the redex and transform it
  into a typing of the contractum.
  
  In order to prove this property, we will need to check the following:
  \begin{itemize}
  \item The redexes and contracta in Figure~\ref{fig:tau-types} are
    well-formed (i.e.\ well-typed). For that reason, we have included the
    type of every variable and function symbol as a subscript.
  \item Applying $\Term$ to the left-hand and right-hand sides of the
    $\banana{\lambda}_\tau$ rules yields the left-hand and right-hand sides
    of all the $\poorcalc$ rules.\footnote{Except for $\beta$ and $\eta$,
      but these two rules are already included in the IDTS formalism.} In
    Figure~\ref{fig:tau-types}, we have included the $\idts$ terms with
    their type annotations removed so that it is easier to see that they
    match the rewriting rules of $\poorcalc$ presented in
    Figure~\ref{fig:reductions}. There is a difference in the recursive
    $\banana{}$ rules due to the fact that IDTSs use only
    $\alpha$-equivalence matching and therefore we cannot match an
    arbitrary expression containing the variable $x$ using the pattern
    $N_\petitc(x)$ like we did in our CRS formulation
    (Subsection~\ref{ssec:banana-as-crs}). However, this difference is
    smoothed over by the way the $\Term$ function handles the $\op{op}$
    function symbols.
  \item Finally, we have to check whether the rewriting rules in
    Figure~\ref{fig:tau-types} actually apply to \emph{all} the
    $m \in \Types(M)$. In other words, we need to check whether the type
    annotation scheme used for the redexes is the most general and covers
    all possible typings of the redex. In our case, we have followed the
    typing rule constraints and given the most general type annotations.
  \end{itemize}
  
  Given a reduction in $\poorcalc$ from $M$ to $N$, we can find the untyped
  reduction rule used in Figure~\ref{fig:tau-types}. We know that if
  $m \in \Types(M)$, then $m$ then matches the redex of the corresponding
  typed rule. We also know that the contractum of the typed rule belongs to
  $\Types(N)$ and therefore, the property holds. Furthermore, if we were to
  formalize the correspondence between $\banana{\lambda}$ typing
  derivations and $\banana{\lambda}_\tau$ terms, we would get another proof
  of subject reduction for $\poorcalc$.
\end{proof}

\begin{lemma}\label{lem:banana-tau-termination}
  If the reduction relation of $\banana{\lambda}_\tau$ is terminating, then
  so is the $\poorcalc$ reduction relation on well-typed $\calc$ terms.
\end{lemma}

\begin{proof}
  Consider the contrapositive: if there exists an infinite $\poorcalc$
  chain of well-typed $\banana{\lambda}$ terms, we can use
  Lemma~\ref{lem:infinite-chains} to translate it, link by link, to an
  infinite $\banana{\lambda}_\tau$ chain. However, infinite
  $\banana{\lambda}_\tau$ reduction chains do not exist since
  $\banana{\lambda}_\tau$ is terminating.
\end{proof}


\subsection{Termination for IDTSs}
\label{ssec:termination-for-idts}

So far, we have introduced an IDTS and have shown that if this IDTS is
terminating, then so is $\poorcalc$. We will now look at a general result
for IDTSs that we will make use of.

\begin{theorem}
  \label{thm:idts-normalization} \demph{Strong
    normalization}~\cite{blanqui2002inductive}

  Under the assumptions 1 and 2, the combination of

  \begin{enumerate}
  \item the simply-typed $\lambda$-calculus with $\beta\eta$-reductions and
  \item higher-order rewrite rules following the General Schema
  \end{enumerate}

  is terminating.
\end{theorem}

The theorem was lifted verbatim\footnote{In fact, the actual Theorem
  in~\cite{blanqui2002inductive} states that the system is \emph{strongly
    normalizing}. However, by strongly normalizing they mean that every
  term is computable, i.e.\ that there is no infinite reduction chain.}
from~\cite{blanqui2002inductive} and parts of it deserve explaining:

\begin{itemize}
\item What are assumptions 1 and 2?
\item What is the General Schema?
\end{itemize}

We will deal with these in order.

\subsubsection{Checking Off the Assumptions}

First, we deal with the assumptions 1 and 2.

\begin{definition}
  The \demph{Assumptions 1 and 2} are defined as:
  \begin{enumerate}
  \item 
    \begin{enumerate}
      \item \label{ass:inductive-well-founded} $>_\II$ is well-founded
      \item \label{ass:positive-types} all inductive types are
        strictly positive
    \end{enumerate}
  \item \label{ass:function-symbols}
    \begin{enumerate}
      \item \label{ass:functions-well-founded} $>_\FF$ is well-founded
      \item \label{ass:stat} $stat_f = stat_g$ whenever $f =_\FF g$
    \end{enumerate}
  \end{enumerate}
\end{definition}

For these to make sense to us, we will need to identify some more structure
on top of our IDTS: namely the $>_\II$ and $>_\FF$ relations.

The $>_\II$ relation, which is a binary relation on inductive types, is
determined by our choice of constructors.

\begin{definition}
  The inductive type $\alpha$ \demph{depends on} the inductive type $\beta$
  if there is a constructor $c \in C_{\tau_1,\ldots,\tau_n,\alpha}$ such
  that $\beta$ occurs in one of $\tau_1, \ldots, \tau_n$.
  
  We will use $\ge_\II$ to mean the reflexive-transitive closure of this
  relation. Furthermore, we will use $=_\II$ and $>_\II$ to mean the
  associated equivalence and strict ordering respectively.
\end{definition}


\begin{observation}
  If $\tau_1 \le_\II \tau_2$, then $\tau_1$ is a subterm of $\tau_2$.
\end{observation}
\begin{proof}
  We will prove this by induction on the structure of the inductive type
  $\tau_2$. If $\tau_2$ is an atomic type, then $\tau_2$ has no
  constructors, so it does not depend on any other type. If we look at the
  reflexive-transitive closure of that, $\ge_\II$, then the only type
  $\alpha$ such that $\tau_2 \ge_\II \alpha$ is, by reflexivity, $\tau_2$
  itself, which is a subterm of $\tau_2$.
  
  If $\tau_2$ is the computation type $\FF_E(\gamma)$, then we will have
  several constructors. We have $\eta_{\gamma,E}$ with a single argument of
  type $\gamma$. We thus know that $\FF_E(\gamma)$ depends on
  $\gamma$.\footnote{This is a slight abuse of notation. The dependency
    relation is a relation on inductive types. However, $\gamma$ might be a
    complex (function) type. By saying that $\FF_E(\gamma)$ depends on
    $\gamma$, we mean to say that for every inductive type $\gamma'$
    occurring in $\gamma$, we have that $\FF_E(\gamma)$ depends on
    $\gamma'$.} For every $\typedop{op}{\alpha}{\beta} \in E$, we have a
  constructor $\op{op}_{\gamma,E}$ with arguments of types $\alpha$ and
  $\beta \to \FF_E(\gamma)$. This tells us that $\FF_E(\gamma)$ also
  depends on $\alpha$, $\beta$ and $\FF_E(\gamma)$. $\FF_E(\gamma)$ does
  not have any more constructors, so those are all the types it depends on.
  The $\ge_\II$ relation, which is the subject of this observation, is the
  reflexive-transitive closure of the dependency relation between inductive
  types. This means that $\tau_2 \ge_\II \tau_1$ if either
  $\tau_2 = \tau_1$ or $\tau_2$ depends on some $\tau_2' \neq \tau_2$ such
  that $\tau_2' \ge_\II \tau_1$.
  \begin{itemize}
  \item If $\tau_2 = \tau_1$, then trivially $\tau_1$ is a subterm of
    $\tau_2$ and we are done.
  \item If $\tau_2$ depends on some $\tau_2' \neq \tau_2$, then $\tau_2'$
    must be either $\gamma$ or one of the $\alpha$ or $\beta$ from $E$
    since $\tau_2 = \FF_E(\gamma)$. In all these cases, we can apply the
    induction hypothesis for $\tau_2'$. We know that
    $\tau_2' \ge_\II \tau_1$ and by the induction hypothesis, we now know
    that $\tau_1$ is a subterm of $\tau_2'$. Since $\tau_2'$ is a subterm
    of $\tau_2$, we have that $\tau_1$ is a subterm of $\tau_2$.
  \end{itemize}
\end{proof}

\begin{corollary}\label{cor:inductive-type-eq}
  If $\tau_1 =_\II \tau_2$, then $\tau_1 = \tau_2$.
\end{corollary}

\begin{corollary}\label{cor:inductive-type-lt}
  If $\tau_1 <_\II \tau_2$, then $\tau_1$ is a proper subterm of $\tau_2$.
\end{corollary}

We can now check assumption~\ref{ass:inductive-well-founded}. Since the
proper subterm relation is well-founded (i.e.\ has no infinite descending
chains) and $>_\II$ is a subset of the proper subterm relation, then
$>_\II$ must be well-founded as well.

We can also check assumption~\ref{ass:positive-types} once we explain what a
strictly positive type is.

\begin{definition}
  An inductive type $\alpha$ is \demph{strictly positive} if every
  constructor for $\alpha$ (i.e.\ every
  $c \in C_{\tau_1,\ldots,\tau_n,\alpha}$ for some
  $\tau_1, \ldots, \tau_n$) is strictly positive.
\end{definition}

\begin{definition}\label{def:positive-constructor}
  A constructor $c \in C_{\tau_1,\ldots,\tau_n,\beta}$ is \demph{strictly
    positive} if inductive types $\alpha =_\II \beta$ occur only at
  strictly positive positions in $\tau_1, \ldots, \tau_n$.\footnote{In the
    IDTS paper~\cite{blanqui2002inductive}, the definition is more general
    and allows inductive types $\alpha$ different from $\beta$ to occur
    even in non-strictly positive positions (even though they are
    $\II$-equivalent to $\beta$). Our definition is stricter, but
    sufficient for our case and simpler to define.}
\end{definition}

\begin{definition}
  The inductive types occurring in \demph{strictly-positive positions}
  $(\SPos)$ within a type are defined by the following:

  \begin{align*}
    \SPos(\alpha \to \beta) &= \Pos(\beta) \\
    \SPos(\nu) &= \{ \nu \} \text{\quad with $\nu$ an inductive type} \\
  \end{align*}
\end{definition}

In our IDTS, $\alpha =_\II \beta$ is true only when $\alpha = \beta$. The
only time a type occurs in the type of one of its constructor's arguments
is in the case of the $\op{op}$ constructors. Given
$\typedop{op}{\alpha}{\beta} \in E$, $\op{op}_{\gamma,E}$ is a constructor
of $\FF_E(\gamma)$; the type of its second argument is
$\beta \to \FF_E(\gamma)$. This occurrence is strictly positive and so we
validate assumption~\ref{ass:positive-types}.

To check assumption~\ref{ass:function-symbols}, we will need to introduce
the $>_\FF$ and $=_\FF$ relations. As $>_\II$ was determined by the
structure of the constructors, $>_\FF$ will be determined by the structure
of the rewriting rules $\RR$ of our IDTS.

\begin{definition}
  A function symbol $g$ \demph{depends on} a function symbol $f$ if there
  is a rule defining $g$ (i.e.\ whose left-hand side is headed by $g$) and
  in the right-hand side of which $f$ occurs.

  We will use $\ge_\FF$ as the name for the reflexive-transitive closure of
  this relation. We will also write $=_\FF$ and $>_\FF$ for the associated
  equivalence and strict ordering respectively.
\end{definition}

We go through the rewriting rules of our IDTS:

\begin{itemize}
\item The $\banana{\eta}$ rule uses no function symbols in the right-hand
  side.
\item The $\banana{\op{op}}$ rule uses the same $\banana{}$ function symbol
  in the right-hand side as in the left-hand side.
\item The $\banana{\op{op}'}$ rule uses the $\banana{}$ function symbol and
  an $\op{op}$ function symbol.
\item The $\cherry$ rule uses no function symbols in the right-hand side.
\end{itemize}

Therefore, we have that every $\banana{}$ function symbol depends on itself
and on some $\op{op}$ function symbols. There is no other dependency in our
IDTS.\@ This means we can check off
assumption~\ref{ass:functions-well-founded} since $>_\FF$ is well-founded
(it contains only the pairs $\banana{} >_\FF \op{op}$).

Assumption~\ref{ass:stat} is trivial in our case since, within our IDTS,
$f =_\FF g$ only when $f = g$. This assumption only comes into play in the
general theory of IDTSs when one exploits mutual recursion with functions
of multiple arguments. The $stat_f$ values mentioned in the
assumption~\ref{ass:stat} describe the way in which a function's arguments
should be ordered to guarantee that recursive calls are always made to
smaller arguments. In the case of mutual recursion, both functions must
agree on the order according to which they will decrease their
arguments. Since we do not deal with mutual recursion in
$\banana{\lambda}$, we will not go into any more detail into
this.


\subsubsection{General Schema}
\label{sssec:general-schema}

There is one last obstacle in our way towards proving termination of
$\banana{\lambda}_\tau$. We will need to verify that the rewrite rules that
we gave in Figure~\ref{fig:tau-types} satisfy the General Schema.

\begin{definition}
  A rewrite rule $f(l_1, \ldots, l_n) \to r$ follows the \demph{General
    Schema (GS)} if $r \in \CC\CC_f(l_1, \ldots, l_n)$ and, for every
  $x \in FV(r)$, $x \in \Acc(l_1, \ldots, l_n)$.
\end{definition}

We have two more notions left to explain: the set of \emph{accessible
  subterms} $\Acc(v)$ of some term $v$ and the \emph{computable closure}
$\CC\CC_f(l_1, \ldots, l_n)$ of some function-headed term
$f(l_1, \ldots, l_n)$.

The strong normalization theorem of~\cite{blanqui2002inductive}
(Theorem~\ref{thm:idts-normalization}) is proven using Tait's method of
computability predicates~\cite{tait1967intensional}. The method consists of
defining a predicate on terms called \emph{computability}, showing that
every computable term is strongly normalizing (i.e.\ terminating) and
finally showing that every (well-typed) term is computable. The crucial
problem is proving that whenever $u_1, \ldots, u_n$ are computable, then so
is $f(u_1, \ldots, u_n)$. We can prove this by showing that all the
immediate reducts of $f(u_1, \ldots, u_n)$ are computable. The problem then
eventually reduces to having to prove, for every rewriting rule
$f(u_1, \ldots, u_n) \to r$, that $r$ is computable whenever
$u_1, \ldots, u_n$ are computable.

This is when the notions of accessible subterms and computable closure come
into play. The accessible subterms of $v$ are those subterms $u$ such that
$u$ is computable whenever $v$ is computable. By forcing the free variables
of the right-hand sides of the rules to be accessible in the arguments of
the left-hand sides, the General Schema restricts our access to only those
parts of the arguments which are guaranteed to be computable. The
computable closure then takes these accessible parts and defines all the
terms (i.e.\ potential right-hand sides) that can be built out of them
while still preserving computability.

Having discussed the motivation behind the two notions, we now turn to
their definitions.

\begin{definition}
  The set \demph{$\Acc(v)$} of \demph{accessible subterms} of $v$ is
  defined inductively by:

  \begin{enumerate}
  \item $v \in \Acc(v)$
  \item if $\lam{x}{u} \in \Acc(v)$, then $u \in \Acc(v)$
  \item if $C(u_1, \ldots, u_n) \in \Acc(v)$, where $C$ ranges over
    constructors, then each $u_i \in \Acc(v)$
  \item \ldots\footnote{The definition in~\cite{blanqui2002inductive}
      includes more cases, but we can restrict ourselves to these.}
  \end{enumerate}
\end{definition}

The rewriting rules of our IDTS $\idts$ validate this condition, since in
all of our rules $f(l_1, \ldots, l_n) \to r$, the patterns
$l_1, \ldots, l_n$ are built only out of variables and
constructors. Therefore, all of the subterms of $l_1, \ldots, l_n$,
including all the free variables, are accessible due to clauses (1) and (3)
of the definition of accessibility.

\begin{definition}
  The IDTS term $r$ is in the \demph{computable closure
    $\CC\CC_f(l_1, \ldots, l_n)$} if for every function-headed subterm
  $g(u_1, \ldots, u_m)$, one of the following is true:\footnote{Again, the
    complete definition can be found
    in~\cite[p.12]{blanqui2002inductive}. The original definition is
    slightly more permissive and general, but also more complex to define.}

  \begin{itemize}
  \item $g <_\FF f$
  \item $g =_\FF f$ and the arguments $u_1, \ldots, u_m$ are smaller than
    the arguments $l_1, \ldots, l_n$
  \end{itemize}
\end{definition}

In our IDTS, there are only two rules that use function symbols in the
right-hand side: $\banana{\op{op}}$ and $\banana{\op{op}'}$.

\begin{align*}
  \banana{}^{(\op{op}_i)_{i \in I}}(M_i \ldots, M_\eta, \op{op}_j(N_{\mathrm{p}}, N_{\mathrm{c}})) \quad &\to_{\banana{\op{op}}} \quad
  \app{M_j}{N_{\mathrm{p}}}{(\lam{x}{\banana{}^{(\op{op}_i)_{i \in I}}(M_i \ldots, M_\eta, \ap{N_{\mathrm{c}}}{x})})} \\
  \banana{}^{(\op{op}_i)_{i \in I}}(M_i \ldots, M_\eta, \op{op}_j(N_{\mathrm{p}}, N_{\mathrm{c}})) \quad &\to_{\banana{\op{op}'}} \quad
  \op{op}_j(N_{\mathrm{p}}, \lam{x}{\banana{}^{(\op{op}_i)_{i \in I}}(M_i, \ldots, M_\eta, \ap{N_{\mathrm{c}}}{x})})
\end{align*}

The only function-headed terms that appear in the right-hand sides of these
rules are:

\begin{itemize}
\item
  $\op{op}_j(N_{\mathrm{p}}, \lam{x}{\banana{}^{(\op{op}_i)_{i \in I}}(M_i,
    \ldots, M_\eta, \ap{N_{\mathrm{c}}}{x})})$
\item
  $\banana{}^{(\op{op}_i)_{i \in I}}(M_i \ldots, M_\eta,
  \ap{N_{\mathrm{c}}}{x})$
\end{itemize}

The first preserves computability since, by definition,
$\op{op}_j <_\FF \banana{}^{(\op{op}_i)_{i \in I}}$. The second is more
interesting because we have a recursive use of the $\banana{}$ symbol. We
will therefore need to show that the arguments
$(M_i, \ldots, M_\eta, \ap{N_{\mathrm{c}}}{x})$ are smaller than the
arguments
$(M_i \ldots, M_\eta, \op{op}_j(N_{\mathrm{p}}, N_{\mathrm{c}}))$. Since
the first $n + 1$ arguments to $\banana{}$ do not vary, we will be
comparing the last argument. This means we need to show that
$\op{op}_j(N_{\mathrm{p}}, N_{\mathrm{c}}) > \ap{N_{\mathrm{c}}}{x}$. The
notion of ``greater than'' that is in play is defined
in~\cite{blanqui2002inductive}.

\begin{notation}
  Let $\Pos(u)$ be all the \demph{positions within the term $u$} (i.e.\ all
  paths from the root of the syntactic tree of $u$ down to some
  subterm). For $p \in \Pos(u)$, we use the expression $u|_p$ to designate
  the \demph{subterm of $u$ at position $p$} and we write $\epsilon$ for
  the \demph{root position}/empty path where $u|_\epsilon = u$.
\end{notation}

\begin{definition}\label{def:argument-ordering}
  Let $\alpha$ be a strictly positive inductive type and $u$ and $v$ be two
  terms of type $\alpha$. We say that $u$ is \demph{greater than} $v$,
  $u > v$, if there is some $p \in \Pos(u)$ such that $p \neq \epsilon$,
  $v = \appp{u|_p}{w_1}{\ldots}{w_n}$ and, for all $q < p$, $u|_q$ is
  constructor-headed.
\end{definition}

What the definition of $u > v$ tells us is that in order to find a term
smaller than $u$, we have to drill down several (at least one) layers of
constructors, pull out some $u|_p$ and then apply it to arbitrary arguments
$w_1, \ldots, w_n$ to get a smaller term
$v = \appp{u|_p}{w_1}{\ldots}{w_n}$. We can therefore immediately conclude
that $\op{op}_j(N_{\mathrm{p}}, N_{\mathrm{c}}) > \ap{N_{\mathrm{c}}}{x}$:
we go through the $\op{op}_j$ constructor to get $u|_p = N_{\mathrm{c}}$
and then we apply it to the argument $w_1 = x$ to get the smaller term
$\ap{N_{\mathrm{c}}}{x}$.

We have now checked that the General Schema is satisfied by all of the
rules in $\idts$ and are ready to apply
Theorem~\ref{thm:idts-normalization}.

\begin{theorem}\label{thm:idts-termination}
  \demph{Termination of $\idts$}

  The reduction relation induced by the IDTS $\idts$ is
  terminating.\footnote{This result can be extended to $\calc$ with sums
    and products. The pair construction $\left<-,-\right>$ and the
    injections $\inl$ and $\inr$ will be the constructors for
    $\alpha \times \beta$ and $\alpha + \beta$ respectively, with
    $\alpha <_\II \alpha \times \beta$, $\beta <_\II \alpha \times \beta$,
    $\alpha <_\II \alpha + \beta$ and $\beta <_\II \alpha + \beta$. All of
    the rules defining case analysis and projections $\pi_1$ and $\pi_2$
    satisfy the General Schema.}
\end{theorem}

\begin{proof}
  In the prequel, we have shown that the rules of the IDTS $\idts$ satisfy
  the General Schema. From Theorem~\ref{thm:idts-normalization}
  of~\cite{blanqui2002inductive}, we can conclude that the reduction
  relation of $\idts$ is terminating.
\end{proof}

\begin{theorem}\label{thm:termination}
  \demph{Termination of $\poorcalc$}
  
  The reduction relation of $\banana{\lambda}$ without the $\CC_{\op{op}}$
  and $\CC_\eta$ rules is terminating.
\end{theorem}

\begin{proof}
  By Theorem~\ref{thm:idts-termination} and
  Lemma~\ref{lem:banana-tau-termination}.
\end{proof}

\begin{theorem}\label{thm:strong-normalization}
  \demph{Strong normalization of $\poorcalc$}
  
  There are no infinite reduction chains in $\poorcalc$ and all maximal
  reduction chains originating in a $\poorcalc$ term $M$ terminate in the
  same term, the normal form of $M$.
\end{theorem}

\begin{proof}
  The lack of infinite reduction chains is due to the termination of
  $\poorcalc$ (Theorem~\ref{thm:termination}) and the fact that all maximal
  reduction chains lead to the same term is entailed by the confluence of
  $\calc$ (Theorem~\ref{thm:confluence}).\footnote{Confluence of
    $\poorcalc$ follows from the same proof as the confluence of
    $\calc$. We can encode $\calc_{-\CC-\eta}$ as an orthogonal CRS and
    then add $\eta$-reduction the same way we did in
    Subsection~\ref{ssec:confluence-eta}.}
\end{proof}


\subsection{The Case of the $\CC$ Operator and Termination}
\label{ssec:c-termination}

In this subsection, we address the $\CC$ operator and its conspicuous
absence from the proof of termination and strong normalization. One of the
reasons why proving the termination of $\calc$ with the $\CC$ reduction
rules is tricky is the fact that the rules rely on true higher-order
pattern matching.

\begin{align*}
  \ap{\CC}{(\lam{x}{\ap{\eta}{(\ap{M}{x})}})} & \to_{\CC_\eta} \ap{\eta}{M} \\
  \ap{\CC}{(\lam{x}{\ap{\ap{\op{op}}{M_{\mathrm{p}}}}{(\lam{y}{\app{M_{\mathrm{c}}}{x}{y}})}})}
  & \to_{\CC_{\op{op}}} \ap{\ap{\op{op}}{M_{\mathrm{p}}}}{(\lam{y}{\ap{\CC}{(\lam{x}{\app{M_{\mathrm{c}}}{x}{y}})}})}
\end{align*}

The left-hand side of the $\CC_\eta$ rule is supposed to match terms like
$\ap{\CC}{(\lam{x}{\etaE{x}})}$. However, to be able to match the pattern
$\ap{\CC}{(\lam{x}{\ap{\eta}{(\ap{M}{x})}})}$ to
$\ap{\CC}{(\lam{x}{\etaE{x}})}$, we have to substitute $\lam{x}{x}$ for $M$
and then $\beta$-reduce the pattern. In other words, matching needs to be
performed modulo $\beta$-equivalence. Otherwise, the reduction rule above
does not behave as the one we set down in
Chapter~\ref{chap:definitions}. However, as we have said in
Subsection~\ref{ssec:idts}, IDTS only performs matching modulo
$\alpha$-equivalence. In the IDTS paper~\cite{blanqui2002inductive}, the
authors conjecture that the termination of rules satisfying the General
Schema should also extend to the case of higher-order pattern-matching.

Frédéric Blanqui, the first author of the IDTS
paper~\cite{blanqui2002inductive}, extended the IDTS formalism to CRS-style
higher-order rewriting in~\cite{blanqui2000termination}. This means that
$\calc$ (with its reduction system and type system) can be faithfully
represented as an IDTS. However, in his extension, Blanqui does not
consider the case of certain non-basic strictly-positive inductive types,
such as recursors on Brouwer's ordinals or $\banana{}$ handlers on our
computations: the resulting theorem about termination relies on an ordering
for arguments which is different from the one of
Definition~\ref{def:argument-ordering}. This means that under this version
of the formalism, the General Schema is not able to guarantee termination
for neither the $\banana{}$ handlers, nor the $\CC$ operator. In the
conclusion of~\cite{blanqui2000termination}, Blanqui conjectures that the
kind of interpretation that is necessary to prove the termination of rules
such as the recursor for Brouwer's ordinals should eventually be adaptable
to higher-order pattern matching.

We followed the work on IDTSs to Makoto Hamana's paper on higher-order
semantic labelling~\cite{hamana2007higher}. Hamana takes the CRS-style
IDTSs of Blanqui~\cite{blanqui2000termination} and gives a strategy for
proving termination which is complete. The strategy relies on choosing a
monotonic interpretation of the language in some poset such that the poset
is a model for the reduction relation (i.e.\ if $M$ reduces to $N$, then
the interpretation of $M$ is greater than or equal to the interpretation of
$N$). The function symbols are then annotated with labels from a
well-founded set, which are computed by a monotonic function of the
symbol's arguments' interpretations. This produces an IDTS with labelled
function symbols which usually trivially satisfies the General Schema
because no symbol is recursive anymore (where before there was a recursive
rule for $f$, there would now be a family of rules defining $f_a$ in terms
of $f_b$, where $a > b$ and $f_a >_\FF f_b$).

The denotational semantics give us a promising interpretation w.r.t.\
termination and comparing the size of arguments on recursive calls. Recall
that in Subsection~\ref{ssec:denotational-semantics}, we have given a
well-founded ordering on computation denotations by
$\forall x.\ \op{op}(p, c) > c(x)$ and a well-founded ordering on function
denotations by $f > g \equiv \forall x.\, f(x) > g(x)$. If we consider all
of our recursive rules and compute the denotations of the arguments of the
recursive function calls, we find that they are actually strictly
decreasing w.r.t.\ to these well-founded orderings.

However, translating this informal reasoning into a proof of termination
for the reduction system is challenging. If we try to use these ordered
denotations as the partially ordered model in higher-order semantic
labelling, we face the problem of the interpretation not being
monotonic. In our calculus, we can define functions which are semantically
non-monotonic.\footnote{For example, we can encode natural numbers as the
  type $\FF_{\{\typedop{succ}{1}{1}\}}(1)$. Then,
  $\banana{\onto{\op{succ}}{(\lam{\_ k}{\etaE{\star}})},\
    \onto{\eta}{(\lam{\_}{\ap{\op{succ}!}{\star}})}}$ is the function that
  maps 0 to 1 and all the other naturals to 0. Since, in the case of the
  type $\FF_{\{\typedop{succ}{1}{1}\}(1)}$, our structural ordering on
  computations corresponds to the ordering on natural numbers, this
  function is not monotonic.} This means that the interpretation of
function application is not monotonic, since if the function term denotes a
non-monotonic function, then the interpretation of the application is not
monotonic w.r.t.\ the argument.

% In the examples from his paper on higher-order semantic
% labelling~\cite{hamana2007higher}, Hamana often uses the term model as the
% interpretation, where the terms are interpreted as themselves or as terms
% in some other calculus, ordered on the terms is taken to be the reduction
% relation. Such an interpretation is easily monotonic and a model of the
% reduction relation. Furthermore, since our denotations are also a model of
% the $\calc$ reduction relation (see
% Property~\ref{prop:denotation-soundness}), the function which would label
% recursive function symbols with the denotation of the decreasing arguments
% would also be monotonic. Finally, having the denotations of the decreasing
% arguments as labels on the function symbols would allow us to derive the
% termination of the system via the General Schema. However, in order for
% this interpretation to be valid, we would need to know that the reduction
% relation is a partial order. While the reduction relation is necessarily
% reflexive and transitive, we do not know whether it is antisymmetric. As a
% matter of fact, antisymmetry is one of the consequences of the termination
% that we are trying to prove and so we end up with a circular proof.

Blanqui and Roux make use of Hamana's higher-order semantic labelling when
proving termination based on sized types~\cite{blanqui2009relation}. The
approach relies on annotating the types of constructors and function
symbols with expressions that relate the size of the input to the size of
the output. While the technique looks promising, the result obtained is
limited to rewriting on first-order patterns (constructor patterns and, in
an extension, algebraic patterns). Their work on sized-types based
termination was continued in Roux's dissertation~\cite{roux2011size} and
Blanqui's journal article~\cite{blanqui2015size}. However, Roux restricts
himself to algebraic patterns and Blanqui to matching modulo
$\alpha$-equivalence, and so neither is sufficient to cover the reduction
rules defining $\CC$.

In our search for a proof of termination for $\CC$, we have also looked at
recursive path orderings. The computability path ordering
(CPO)~\cite{blanqui2015computability}, which is an extension of the
higher-order recursive path ordering (HORPO)~\cite{jouannaud1999higher},
gives an inference system, a logic program, that tries to establish whether
a term $v$ lies in the computability closure of another term $u$ and
therefore whether the system with the rule $u \to v$ can be proven
terminating. We tried following the inference system and trying to verify
the reduction rules for the $\CC$ operator but we end up with an unsolvable
goal due to the way that the $\CC_{\op{op}}$ reorders $\lambda$-binders and
renames variables.
