We will present a calculus with special constructions for dealing with
effects and handlers, which we will then apply to the problem of natural
language semantics in the rest of the thesis. Our calculus is an extension
of the simply-typed lambda calculus (STLC). We enrich STLC with a type for
representing effectful computations alongside with operations to create and
process values of this type.

\chapter{Definitions}

We are tempted to start by first giving the formal definitions of all the
essential components of our calculus:

\begin{itemize}
\item the syntax of the terms in our calculus
\item the syntax of the types in our calculus
\item the judgments that relate types to terms
\item a reduction semantics
\end{itemize}

However, before we do so, we will briefly sketch the ideas behind the
calculus so you can start building an intuition about the meaning of the
symbols that we will be introducing below.

\section{Sketching Out the Calculus}
%% \todo{This now serves both as a sketch and a justification of the calculus,
%%   along with the history of the techniques used. This could maybe later
%%   live in its own place, somewhere in the introduction.}

We will be adding a new type constructor, $\mathcal{F}$, into our
language. The type $\mathcal{F}(\alpha)$ will correspond to effectful
\emph{computations} that produce values of type $\alpha$. The idea comes
from the programming language Haskell and its use of monads
\cite{moggi1991notions,wadler1992essence,jones2003haskell}. Our type
constructor $\mathcal{F}$ will also stand in for a monad, one that has been
already encoded in Haskell in several ways
\cite{kiselyov2013extensible,kammar2013handlers}. The motivation behind our
calculus is thus to build a minimal language which gives us directly the
primitive operations for working with this particular monad. This way, we
end up with a language that:
\begin{itemize}
\item is smaller than Haskell (and thus more mananageable to analyse),
\item is closer to the STLC favored by semanticists,
\item and which makes more evident the features that our proposal relies on.
\end{itemize}

The transition from type $\alpha$ to a type $\mathcal{F}(\alpha)$ is meant
as a generalization of a scheme often seen in semantics when novel forms of
meaning are studied (e.g., type raising \cite{montague1973proper},
dynamization \cite{lebedeva2012expression}, intensionalization
\cite{de2013note}). The distinction between the type $\alpha$ and the type
$\mathcal{F}(\alpha)$ will, in different analyses, align with dichotomies
such as the following:

\begin{itemize}
\item static/dynamic meaning
\item extension/intension
\item semantics/pragmatics
\end{itemize}

The question then is, what form should our general $\mathcal{F}$ type
constructor take? We want to have a construction that can combine all the
existing ones. One can do the combining at the level of monads with the use
of monad transformers, a technique pioneered by Moggi and very
well-established in the Haskell programming community
\cite{moggi1991notions}. Simon Charlow has made the case that this
technique can be exploited to great benefit in natural language semantics
as well \cite{charlow2014semantics}.

However, a competing technique has emerged in recent years and it is the
goal of this thesis to introduce it to semanticists and verify its
applicability to the study of natural language. The technique goes by many
names, ``algebraic effects and handlers'' and ``extensible effects'' being
the most commonly used ones. This is in part due to the fact that it lies
at the confluence of several research programs. This fact will allow us to
present the theory from two different perspectives, so you can be equipped
with two different intuitive models.

%% Mention Lewis' ``What should meanings do instead of what should meanings
%% be.'' somewhere?

\subsubsection*{Algebraic Effects and Handlers}

Hyland, Power and Plotkin have studied the problem of deriving denotational
semantics of programming languages that combine different side effects
\cite{hyland2006combining}. In their approach, rather then modeling the
individual effects using monads and combining the monads, every effect is
expressed in terms of \emph{operators} on computations. Computations thus
become algebraic expressions with effects as operations and values as part
of the generator set.

Let us take the example of nondeterminism. In the monadic framework, this
effect is analyzed by shifting the type of denotations from $\alpha$ to the
powerset $\mathcal{P}(\alpha)$. In the algebraic framework, a binary
operator $+$ is introduced and is given meaning through a set of equations.
In this case, these are the equations of a semilattice (stating the
operator's associativity, commutativity and idempotence).

When the time comes to combine two effects, their signatures are summed
together and their theories are combined through either a sum or a tensor
(a sum that adds commutativity laws for operators coming from the two
different effects).

In order to fit exception handlers into their theory, Plotkin and Pretnar
enriched the theory with a general notion of a \emph{handler}
\cite{plotkin2013handling}. A handler's purpose is to replace occurrences
of an operator within a computation by another expression. This notion was
shown to be very useful. Since using a handler on a computation is similar
to interpreting its algebraic expression in a particular algebra, in many
practical applications, the use of handlers has replaced equational
theories altogether
\cite{bauer2012programming,kammar2013handlers,brady2013programming}.

\subsubsection*{Extensible Effects}

In the early 90's, Cartwright and Felleisen were working on the following
problem. Imagine you have a simple programming language along with some
denotational semantics or some other interpretation. In your simple
language, numerical expressions might be interpreted as numbers. In that
case, the literal number 3 would denote the number 3 and the application of
the sum operator to two numerical expressions would denote the sum of their
interpretations. Now consider that you want to add mutable variables to
your language. Numerical expressions no longer denote specific numbers, but
rather functions from states of the variable store to both a number and an
updated variable store (since expressions can now both read from and write
to variables). The number 3 is thus no longer interpreted as the number 3
but as a combination of a constant function yielding the number 3 and an
identity function. The addition operator now has to take care to thread the
state of the memory through the evaluation of both of its arguments. In
short, we are forced to give new interpretations for the entire language.

Cartwright and Felleisen proposed a solution to this problem
\cite{cartwright1994extensible}. In their system, an expression can either
yield a value or produce an effect. If it produces an effect, the effect
percolates through the program all the way to the top, with the context
that the effect projected from stored as a continuation. The effect and the
continuation are then passed to an external ``authority'' that handles the
effect, often by producing some output and passing it back to the
continuation. When a new feature is added to the language, it often
suffices to add a new kind of effect and introduce a new clause into the
central ``authority''. The central authority then ends up being a
collection of small modular interpreters for the various effect
types. Denotation-wise, every expression can thus have a stable denotation
which is either a pure value or an effect request coupled with a
continuation.

Later on, this project was picked up by Oleg Kiselyov, who, following
Plotkin and Pretnar's work on handlers, proposed to break down the
``authority'' into the smaller constituent interpreters and have them be
part of the language themselves \cite{kiselyov2013extensible}.

\subsubsection*{Synthesis}

In our language, we can see values of type $\mathcal{F}(\alpha)$ as
algebraic expressions built on top of some effect signature and the
generator set $\alpha$. Since an
%% \todo{Maybe introduce a more precise term
%%   which would correspond to an algebraic expression in which variables have
%%   been replaced by members of a carrier/generator set.}
{algebraic expression} is either a constant or an operator applied to some
other expressions/computations, we can also use the ``extensible effects''
perspective. Under that perspective, we can think of computations as
something which is either a pure value or an effect coupled with a
continuation (the effect corresponds to the operator and the continuation
to the indexed family of operands).

Our calculus will contain tools for injecting values of type $\alpha$ into
the type of algebraic expressions $\mathcal{F}(\alpha)$, as well as tools
for constructing expressions using operators from the effect
signature. Under the ``extensible effects'' point of view, the latter can
be seen as tools for creating a computation that will raise a request to
perform an effect.

Finally, we will have a special form for defining handlers. In the
``algebraic effects and handlers'' frame of mind, these can be thought of
folds or catamorphisms that traverse the algebraic expression and transform
certain operators within. On the other hand, with ``extensible effects'',
the intuition is more similar to that of an exception handler which
intercepts requests of a certain type and decides how the computation
should continue.

\section{Terms}

Having sketched the idea behind our calculus, we will now turn our
attention to the specifics. We start by defining the syntactic
constructions used to build the terms of our language.

Without further ado, we give the syntax of the expressions of our
language. First off, let $\XX$ be a set of variables, $\Sigma$ a typed
signature and $\EE$ a set of operation symbols.

The expressions of our language are comprised of the following:
\begin{description}
  \item[abstraction] $\lam{x}{M}$, where $x$ is a variable from $\XX$ and
    $M$ is an expression
  \item[application] $\ap{M}{N}$, where $M$ and $N$ are expressions
  \item[variable] $x$, where $x$ is a variable from $\XX$
  \item[constant] $c$, where $c$ is a constant from $\Sigma$
  \item[operation] $\op{op}$, where $\op{op}$ is an operator from $\EE$
  \item[injection] $\eta$
  \item[handler] $\banana{\onto{\op{op}_1} M_1,\ \dots,\ \onto{\op{op}_n} M_n,\ 
                          \onto{\eta} M_\eta}$
    where $\op{op}_i$ are operators from $\EE$ and $M_i$ and $M_\eta$ are
    expressions
  \item[extraction] $\cherry$
  \item[exchange] $\CC$
\end{description}

The first four constructions --- abstraction, application, variables and
constants --- come directly from STLC with constants.

The next four deal with the algebraic expressions used to encode
computations. Let us sketch the behaviors of these four kinds of
expressions under the two readings outlined above.

\subsection*{Algebraic Expressions -- The Denotational View}

The $\eta$ function serves to inject values from the generator set into the
set of algebraic expressions. It is a constructor for the trivial atomic
algebraic expression consisting of just a single constant.

Next, for every symbol $\op{op}$ in $\EE$, we have a corresponding function
$\op{op}$ in our calculus. The function $\op{op}$ is a constructor for
algebraic expressions whose topmost operation is $\op{op}$. The $\op{op}$
constructor takes as argument a function that provides its operands, which
are further algebraic expressions.

The banana brackets $\banana{\onto{\op{op}_1}
  M_1,\ \dots,\ \onto{\op{op}_n} M_n,\ \onto{\eta} M_\eta}$ contain algebras:
interpretations of operators and constants. These components are combined
into a catamorphism that can interpret algebraic expressions (hence the use
of banana brackets \cite{meijer1991functional})\footnote{Since the banana
  brackets can contain an arbitrary number of operator clauses, we use the
  syntax of named parameters/records commonly used in popular programming
  languages such as Ruby, Python or JavaScript.}.

The extraction function $\cherry$, pronounced ``cherry'', takes an atomic
algebraic expression (the kind produced by $\eta$) and projects out the
element of the generator set.

\subsection*{Effectful Computations -- The Operational View}

We will now explain these constructions from the point of view the
computational point of view.

The $\eta$ function ``returns'' a given value. The result of applying it to
a value $x$ is a computation that immediately terminates and produces the
value $x$.

The symbols from $\EE$ become something like system calls. A computation
can interrupt its execution and throw an exception with a request to
perform a system-level operation. For every symbol $\op{op}$ in $\EE$,
there is a constructor $\op{op}$ that produces a computation which issues a
request to perform the operation $\op{op}$. This constructor takes as an
argument a continuation which yields the computation that should be pursued
after the system-level operation $\op{op}$ has been performed.

The banana brackets $\banana{\onto{\op{op}_1}
  M_1,\ \dots,\ \onto{\op{op}_n} M_n,\ \onto{\eta} M_\eta}$ describe
handlers: they contain clauses for different kinds of interrupts (operation
requests) and for successful computations (clause $\eta$). They behave very
much like handlers in languages with resumable exceptions such as Common
Lisp or Dylan.

Finally, the cherry function $\cherry$ can take a computation that is
guaranteed to be free of side effects and run it to capture its result.

\vspace{2em}

The 9th construction in our calculus is the $\CC$ operator. $\CC$ serves as
a link between the function type discussed by STLC (constructions 1--4) and
the computation type introduced in our calculus (constructions 5--8). $\CC$
is a (partial) function that takes a computation that produces a function
and returns a function that yields computations. In a way, $\CC$ makes
abstracting over a variable and performing an operation commute
together\footnote{This is \emph{very} reminiscent of the idea behind Paul
  Blain Levy's call-by-push-value calculus \cite{levy1999call}, which
  treats abstracting over a variable as an effectful operation of popping a
  value from a stack. Using call-by-push-value should prove to be a
  rewarding way to refine our approach.}.

We will see the utility of $\CC$ later on. The idea came to us from a paper
by Philippe de Groote \cite{degroote2015conservativity} which tried to
solve a similar problem. The name comes from the $\mathbf{C}$ combinator,
which reorders the order of abstractions in a $\lambda$-term. $\mathbf{C}$
can be seen as a special case of our $\CC$ when the type of computations
producing values $\alpha$ is $\gamma \to \alpha$ for some $\gamma$.


\section{Types and Typing Rules}

We now give a syntax for the types of our calculus alongside with a typing
judgment. In the grammar below, $\nu$ ranges over atomic types from set
$\mathcal{T}$.

The types of our language consist of:
\begin{description}
\item[function] $\alpha \to \beta$, where $\alpha$ and $\beta$ are types
\item[atomic] $\nu$, where $\nu$ is an atomic type from $\TT$
\item[computation] $\FF_E(\alpha)$, where $\alpha$ is a type and $E$ is an
  effect signature (defined next)
\end{description}

In giving the typing rules, we will rely on the standard notion of a
\emph{context}. For us, specifically, a context is a partial mapping from
the variables in $\XX$ to the types defined above.  We commonly write
$\Gamma, x : \alpha$ for a context that assigns to $x$ the type $\alpha$
and to other variables $y$ the type $\Gamma(y)$. We also write $x : \alpha
\in \Gamma$ to say that the context maps $x$ to $\alpha$. Note, however,
that for $\Delta = \Gamma, x : \alpha, x : \beta$, $x : \beta \in \Delta$
while $x : \alpha \notin \Delta$.

\emph{Effect signatures} are very much like contexts. They are partial
mappings from the set of operation symbols $\EE$ to pairs of types. We will
write the elements of effect signatures the following way:
$\typedop{op}{\alpha}{\beta} \in E$ means that $E$ maps $\op{op}$ to the
pair of types $\alpha$ and $\beta$. When dealing with effect signatures, we
will often make use of the disjoint union operator $\uplus$. The term $E_1
\uplus E_2$ serves as a constraint demanding that the domains of $E_1$ and
$E_2$ be disjoint and the same time it denotes the effect signature that is
the union of $E_1$ and $E_2$.

The last kind of dictionary used by the type system is a standard
\emph{higher-order signature} for the constants. For those, we adopt the
same conventions.

In our typing judgments, contexts will appear to the left of the turnstile
and they will hold information about the statically (lexically) bound
variables, as in STLC. Effect signatures will appear as indices of
computation types and they will hold information about the operations that
are dynamically bound by handlers. Finally, plain (higher-order) signatures
will appear below the turnstile and they will give the types to globally
accessible constants.

The typing judgments are presented in Figure~\ref{fig:types}. $\Gamma$
stands for contexts, $Sigma$ for higher-order signatures. Metavariables
$M$, $N$\ldots stand for expressions, $\alpha$, $\beta$,
$\gamma$\ldots\ stand for types and $E$, $E'$\ldots stand for effect
signatures.

  \def\labelSpacing{4pt}
  \begin{prooftree}
    \AxiomC{$\Gamma, x : \alpha \vdash_\Sigma M : \beta$}
    \RightLabel{[abs]}
    \UnaryInfC{$\Gamma \vdash_\Sigma \lam{x}{M} : \alpha \to \beta$}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$\Gamma \vdash_\Sigma M : \alpha \to \beta$}
    \AxiomC{$\Gamma \vdash_\Sigma N : \alpha$}
    \RightLabel{[app]}
    \BinaryInfC{$\Gamma \vdash_\Sigma M N : \beta$}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$x : \alpha \in \Gamma$}
    \RightLabel{[var]}
    \UnaryInfC{$\Gamma \vdash_\Sigma x : \alpha$}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$c : \alpha \in \Sigma$}
    \RightLabel{[const]}
    \UnaryInfC{$\Gamma \vdash_\Sigma c : \alpha$}
  \end{prooftree}
  \begin{prooftree}
    \RightLabel{[op]}
    \AxiomC{$\Gamma \vdash_\Sigma \op{op} : \alpha \to (\beta \to
      \FF_\rho(\gamma)) \to \FF_{\row{\typedop{op}{\alpha}{\beta}}{\rho}}(\gamma)$}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$\Gamma \vdash_{\Sigma,E} \eta : \alpha \to \mathcal{F}(\alpha)
      $\hskip 4pt [$\eta$]}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$OP_i : \alpha_i \to \beta_i \in E$}
    \noLine
    \def\extraVskip{0pt}
    \UnaryInfC{$\Gamma \vdash_{\Sigma,E} M_i : \alpha_i \to (\beta_i \to
      \mathcal{F}(\delta)) \to \mathcal{F}(\delta)$}
    \noLine
    \UnaryInfC{$\Gamma \vdash_{\Sigma,E} M_\eta : \gamma \to \mathcal{F}(\delta)$}
    \def\extraVskip{2pt}
    \RightLabel{[$\mathcal{H}$]}
    \UnaryInfC{$\Gamma \vdash_{\Sigma,E} [\mathcal{H}\ (OP_i\ M_i)_{i \in I}\ (\eta\ M_\eta)] : \mathcal{F}(\gamma) \to \mathcal{F}(\delta)$}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$\Gamma \vdash_{\Sigma,E} \mathcal{C} : (\alpha \to
      \mathcal{F}(\beta)) \to \mathcal{F}(\alpha \to \beta)$\hskip 4pt [$\mathcal{C}$]}
  \end{prooftree}

The typing rules mirror the syntax of expressions. Again, the first four
rules come from STLC with constants. The rules $op$ and $\eta$ let us
construct computations, expressions of type
$\mathcal{F}(\alpha)$. Computations can then be interpreted using handlers
whose type is given by the $\mathcal{H}$ rule. Finally, the $\mathcal{C}$
rule gives a type to the primitive operation of the same name, which
already gives us an idea of what it will do.

\paragraph{Algebraic Expressions}

The $\eta$ function serves to inject values from the generator set into the
set of algebraic expressions. It is a constructor for the trivial atomic
algebraic expression consisting of just a single constant.

Every symbol $\op{op}$ in $\EE$ corresponds to a family of operators
$\op{OP}$ and gives rise to a constructor $\op{op}$ in our calculus. This
constructor takes two arguments: the first is an index $i$ which identifies
a particular operator $\op{OP}_i$ in the family $\op{OP}$, the second is
the set of operands expected by the operator. The set of operands is
encoded by a function 



\section{Reduction Rules}

We will now present a set of type-preserving rules that we will use to
evaluate/simplify expressions in our language.

\vspace{3mm}

\begin{tabular}{lr}
  $(\lambda x.\ M)\ N \rightarrow$ & [$\beta$] \\
  $M[x/N]$ & \\
  \\
  $[\mathcal{H}\ (OP_i\ M_i)\ldots\ (\eta\ M_\eta)]\ (\eta\ N) \rightarrow$ & [$\mathcal{H}$-$\eta$] \\
  $M_\eta\ N$ & \\
  \\
  $[\mathcal{H}\ (OP_i\ M_i)\ldots\ (\eta\ M_\eta)]\ (OP_i\ N_a\ N_k) \rightarrow$ & [$\mathcal{H}$-$OP$] \\
  $M_i\ N_a\ (\lambda x.\ [\mathcal{H}\ (OP_i\ M_i)\ldots\ (\eta\ M_\eta)]\ (N_k\ x))$ & where $x$ is fresh \\
  \\
  $[\mathcal{H}\ (OP_i\ M_i)\ldots\ (\eta\ M_\eta)]\ (OP\ N_a\ N_k) \rightarrow$ & [$\mathcal{H}$-$OP^*$] \\
  $OP\ N_a\ (\lambda x.\ [\mathcal{H}\ (OP_i\ M_i)\ldots\ (\eta\ M_\eta)]\ (N_k\ x))$ & where $x$ is fresh \\
  & and $OP \notin \{OP_i\}_i$ \\
  \\
  $\mathcal{C}\ (\lambda x.\ \eta\ M) \rightarrow$ & [$\mathcal{C}$-$\eta$] \\
  $\eta\ (\lambda x.\ M)$ & \\
  \\
  $\mathcal{C}\ (\lambda x.\ OP\ M_a\ M_k) \rightarrow$ & [$\mathcal{C}$-$OP$] \\
  $OP\ M_a\ (\lambda y.\ \mathcal{C}\ (\lambda x.\ M_k\ y))$ & where $y$ is fresh \\
  & and $x \notin FV(M_a)$
\end{tabular}

\vspace{3mm}

Besides the standard $\beta$-reduction rule, we have rules that define the
behavior of the handlers formed using $\mathcal{H}$ and of the
$\mathcal{C}$ operator.


\section{Sums and Products}


\section{Common Combinators}

Here we will introduce a collection of generally useful syntactic shortcuts
and combinators for our calculus.

\begin{align*}
  \_ \circ \_ &: (\beta \to \gamma) \to (\alpha \to \beta) \to (\alpha \to \gamma) \\
  f \circ g &= \lambda x.\ f\ (g\ x) \\
  \_^* &: (\alpha \to \mathcal{F}(\beta)) \to (\mathcal{F}(\alpha) \to \mathcal{F}(\beta)) \\
  f^* &= [\mathcal{H}\ (\eta\ f)] \\
  \mathcal{F} &: (\alpha \to \beta) \to (\mathcal{F}(\alpha) \to \mathcal{F}(\beta)) \\
  \mathcal{F} &= \lambda f.\ (\eta \circ f)^* \\
  \_ \hsbind \_ &: \mathcal{F}(\alpha) \to (\alpha \to \mathcal{F}(\beta)) \to \mathcal{F}(\beta) \\
  M \hsbind N &= N^*\ M \\
  \\
  [\mathcal{H}\ (OP_i\ M_i)\ldots] &= [\mathcal{H}\ (OP_i\ M_i)\ldots\ (\eta\ \eta)]
\end{align*}

We will also make use of combinators for function application where one or
more of the arguments are computations.

\begin{align*}
  \_ \apl \_ &: \mathcal{F}(\alpha \to \beta) \to \alpha \to \mathcal{F}(\beta) \\
  F \apl x &= F \hsbind (\lambda x.\ \eta\ (f\ x)) \\
  \_ \apr \_ &: (\alpha \to \beta) \to \mathcal{F}(\alpha) \to \mathcal{F}(\beta) \\
  f \apr X &= X \hsbind (\lambda x.\ \eta\ (f\ x))
\end{align*}
