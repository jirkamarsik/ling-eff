We will present a calculus with special constructions for dealing with
effects and handlers, which we will then apply to the problem of natural
language semantics in the rest of the thesis. Our calculus is an extension
of the simply-typed lambda calculus (STLC). We enrich STLC with a type for
representing effectful computations alongside with operations to create and
process values of this type.

\chapter{Definitions}

We are tempted to start by first giving the formal definitions of all the
essential components of our calculus:

\begin{itemize}
\item the syntax of the terms in our calculus
\item the syntax of the types in our calculus
\item the judgments that relate types to terms
\item a reduction semantics
\end{itemize}

However, before we do so, we will briefly sketch the ideas behind the
calculus so you can start building an intuition about the meaning of the
symbols that we will be introducing below.

\section{Sketching Out the Calculus}
%% \todo{This now serves both as a sketch and a justification of the calculus,
%%   along with the history of the techniques used. This could maybe later
%%   live in its own place, somewhere in the introduction.}

We will be adding a new type constructor, $\mathcal{F}$, into our
language. The type $\mathcal{F}(\alpha)$ will correspond to effectful
\emph{computations} that produce values of type $\alpha$. The idea comes
from the programming language Haskell and its use of monads
\cite{moggi1991notions,wadler1992essence,jones2003haskell}. Our type
constructor $\mathcal{F}$ will also stand in for a monad, one that has been
already encoded in Haskell in several ways
\cite{kiselyov2013extensible,kammar2013handlers}. The motivation behind our
calculus is thus to build a minimal language which gives us directly the
primitive operations for working with this particular monad. This way, we
end up with a language that:
\begin{itemize}
\item is smaller than Haskell (and thus more mananageable to analyse),
\item is closer to the STLC favored by semanticists,
\item and which makes more evident the features that our proposal relies on.
\end{itemize}

The transition from type $\alpha$ to a type $\mathcal{F}(\alpha)$ is meant
as a generalization of a scheme often seen in semantics when novel forms of
meaning are studied (e.g., type raising \cite{montague1973proper},
dynamization \cite{lebedeva2012expression}, intensionalization
\cite{de2013note}). The distinction between the type $\alpha$ and the type
$\mathcal{F}(\alpha)$ will, in different analyses, align with dichotomies
such as the following:

\begin{itemize}
\item static/dynamic meaning
\item extension/intension
\item semantics/pragmatics
\end{itemize}

The question then is, what form should our general $\mathcal{F}$ type
constructor take? We want to have a construction that can combine all the
existing ones. One can do the combining at the level of monads with the use
of monad transformers, a technique pioneered by Moggi and very
well-established in the Haskell programming community
\cite{moggi1991notions}. Simon Charlow has made the case that this
technique can be exploited to great benefit in natural language semantics
as well \cite{charlow2014semantics}.

However, a competing technique has emerged in recent years and it is the
goal of this thesis to introduce it to semanticists and verify its
applicability to the study of natural language. The technique goes by many
names, ``algebraic effects and handlers'' and ``extensible effects'' being
the most commonly used ones. This is in part due to the fact that it lies
at the confluence of several research programs. This fact will allow us to
present the theory from two different perspectives so you can be equipped
with two different intuitive models.

%% Mention Lewis' ``What should meanings do instead of what should meanings
%% be.'' somewhere?

\subsubsection*{Algebraic Effects and Handlers}

Hyland, Power and Plotkin have studied the problem of deriving denotational
semantics of programming languages that combine different side effects
\cite{hyland2006combining}. In their approach, rather then modeling the
individual effects using monads and combining the monads, every effect is
expressed in terms of \emph{operators} on computations. Computations thus
become algebraic expressions with effects as operations and values as part
of the generator set.

Let us take the example of nondeterminism. In the monadic framework, this
effect is analyzed by shifting the type of denotations from $\alpha$ to the
powerset $\mathcal{P}(\alpha)$. In the algebraic framework, a binary
operator $+$ is introduced and is given meaning through a set of equations.
In this case, these are the equations of a semilattice (stating the
operator's associativity, commutativity and idempotence).

When the time comes to combine two effects, their signatures are summed
together and their theories are combined through either a sum or a tensor
(a sum that adds commutativity laws for operators coming from the two
different effects).

In order to fit exception handlers into their theory, Plotkin and Pretnar
enriched the theory with a general notion of a \emph{handler}
\cite{plotkin2013handling}. A handler's purpose is to replace occurrences
of an operator within a computation by another expression. This notion was
shown to be very useful. Since using a handler on a computation is similar
to interpreting its algebraic expression in a particular algebra, in many
practical applications, the use of handlers has replaced equational
theories altogether
\cite{bauer2012programming,kammar2013handlers,brady2013programming}.

\subsubsection*{Extensible Effects}

In the early 90's, Cartwright and Felleisen were working on the following
problem. Imagine you have a simple programming language along with some
denotational semantics or some other interpretation. In your simple
language, numerical expressions might be interpreted as numbers. In that
case, the literal number 3 would denote the number 3 and the application of
the sum operator to two numerical expressions would denote the sum of their
interpretations. Now consider that you want to add mutable variables to
your language. Numerical expressions no longer denote specific numbers, but
rather functions from states of the variable store to both a number and an
updated variable store (since expressions can now both read from and write
to variables). The number 3 is thus no longer interpreted as the number 3
but as a combination of a constant function yielding the number 3 and an
identity function. The addition operator now has to take care to thread the
state of the memory through the evaluation of both of its arguments. In
short, we are forced to give new interpretations for the entire language.

Cartwright and Felleisen proposed a solution to this problem
\cite{cartwright1994extensible}. In their system, an expression can either
yield a value or produce an effect. If it produces an effect, the effect
percolates through the program all the way to the top, with the context
that the effect projected from stored as a continuation. The effect and the
continuation are then passed to an external ``authority'' that handles the
effect, often by producing some output and passing it back to the
continuation. When a new feature is added to the language, it often
suffices to add a new kind of effect and introduce a new clause into the
central ``authority''. The central authority then ends up being a
collection of small modular interpreters for the various effect
types. Denotation-wise, every expression can thus have a stable denotation
which is either a pure value or an effect request coupled with a
continuation.

Later on, this project was picked up by Oleg Kiselyov, who, following
Plotkin and Pretnar's work on handlers, proposed to break down the
``authority'' into the smaller constituent interpreters and have them be
part of the language themselves \cite{kiselyov2013extensible}.

\subsubsection*{Synthesis}

In our language, we can see values of type $\mathcal{F}(\alpha)$ as
algebraic expressions built on top of some effect signature and the
generator set $\alpha$. Since an
%% \todo{Maybe introduce a more precise term
%%   which would correspond to an algebraic expression in which variables have
%%   been replaced by members of a carrier/generator set.}
{algebraic expression} is either a constant or an operator applied to some
other expressions/computations, we can also use the ``extensible effects''
perspective. Under that perspective, we can think of computations as
something which is either a pure value or an effect coupled with a
continuation (the effect corresponds to the operator and the continuation
to the indexed family of operands).

Our calculus will contain tools for injecting values of type $\alpha$ into
the type of algebraic expressions $\mathcal{F}(\alpha)$, as well as tools
for constructing expressions using operators from the effect
signature. Under the ``extensible effects'' point of view, the latter can
be seen as tools for creating a computation that will raise a request to
perform an effect.

Finally, we will have a special form for defining handlers. In the
``algebraic effects and handlers'' frame of mind, these can be thought of
folds or catamorphisms that traverse the algebraic expression and transform
certain operators within. On the other hand, with ``extensible effects'',
the intuition is more similar to that of an exception handler which
intercepts requests of a certain type and decides how the computation
should continue.

\section{Terms}

Having sketched the idea behind our calculus, we will now turn our
attention to the specifics. We start by defining the syntactic
constructions used to build the terms of our language.

Without further ado, we give the syntax of the expressions of our
language. First off, let $\XX$ be a set of variables, $\Sigma$ a typed
signature and $\EE$ a set of operation symbols.

The expressions of our language are comprised of the following:
\begin{description}
  \item[abstraction] $\lam{x}{M}$, where $x$ is a variable from $\XX$ and
    $M$ is an expression
  \item[application] $\ap{M}{N}$, where $M$ and $N$ are expressions
  \item[variable] $x$, where $x$ is a variable from $\XX$
  \item[constant] $c$, where $c$ is a constant from $\Sigma$
  \item[operation] $\op{op}$, where $\op{op}$ is an operator from $\EE$
  \item[injection] $\eta$
  \item[handler] $\banana{\onto{\op{op}_1} M_1,\ \dots,\ \onto{\op{op}_n} M_n,\ 
                          \onto{\eta} M_\eta}$
    where $\op{op}_i$ are operators from $\EE$ and $M_i$ and $M_\eta$ are
    expressions
  \item[extraction] $\cherry$
  \item[exchange] $\CC$
\end{description}

The first four constructions --- abstraction, application, variables and
constants --- come directly from STLC with constants.

The next four deal with the algebraic expressions used to encode
computations. Let us sketch the behaviors of these four kinds of
expressions under the two readings outlined above.

\subsection*{Algebraic Expressions -- The Denotational View}

The $\eta$ function serves to inject values from the generator set into the
set of algebraic expressions. It is a constructor for the trivial atomic
algebraic expression consisting of just a single constant.

Next, for every symbol $\op{op}$ in $\EE$, we have a corresponding function
$\op{op}$ in our calculus. The function $\op{op}$ is a constructor for
algebraic expressions whose topmost operation is $\op{op}$. The $\op{op}$
constructor takes as argument a function that provides its operands, which
are further algebraic expressions.

The banana brackets $\banana{\onto{\op{op}_1}
  M_1,\ \dots,\ \onto{\op{op}_n} M_n,\ \onto{\eta} M_\eta}$ contain algebras:
interpretations of operators and constants. These components are combined
into a catamorphism that can interpret algebraic expressions (hence the use
of banana brackets \cite{meijer1991functional})\footnote{Since the banana
  brackets can contain an arbitrary number of operator clauses, we use the
  syntax of named parameters/records commonly used in popular programming
  languages such as Ruby, Python or JavaScript.}.

The extraction function $\cherry$, pronounced ``cherry'', takes an atomic
algebraic expression (the kind produced by $\eta$) and projects out the
element of the generator set.

\subsection*{Effectful Computations -- The Operational View}

We will now explain these constructions from the computational point of
view.

The $\eta$ function ``returns'' a given value. The result of applying it to
a value $x$ is a computation that immediately terminates and produces the
value $x$.

The symbols from $\EE$ become something like system calls. A computation
can interrupt its execution and throw an exception with a request to
perform a system-level operation. For every symbol $\op{op}$ in $\EE$,
there is a constructor $\op{op}$ that produces a computation which issues a
request to perform the operation $\op{op}$. This constructor takes as an
argument a continuation which yields the computation that should be pursued
after the system-level operation $\op{op}$ has been performed.

The banana brackets $\banana{\onto{\op{op}_1}
  M_1,\ \dots,\ \onto{\op{op}_n} M_n,\ \onto{\eta} M_\eta}$ describe
handlers: they contain clauses for different kinds of interrupts (operation
requests) and for successful computations (clause $\eta$). They behave very
much like handlers in languages with resumable exceptions such as Common
Lisp or Dylan.

Finally, the cherry function $\cherry$ can take a computation that is
guaranteed to be free of side effects and run it to capture its result.

\vspace{2em}

The 9th construction in our calculus is the $\CC$ operator. $\CC$ serves as
a link between the function type discussed by STLC (constructions 1--4) and
the computation type introduced in our calculus (constructions 5--8). $\CC$
is a (partial) function that takes a computation that produces a function
and returns a function that yields computations. In a way, $\CC$ makes
abstracting over a variable and performing an operation commute
together\footnote{This is \emph{very} reminiscent of the idea behind Paul
  Blain Levy's call-by-push-value calculus \cite{levy1999call}, which
  treats abstracting over a variable as an effectful operation of popping a
  value from a stack. Using call-by-push-value should prove to be a
  rewarding way to refine our approach.}.

We will see the utility of $\CC$ later on. The idea came to us from a paper
by Philippe de Groote \cite{degroote2015conservativity} which tried to
solve a similar problem. The name comes from the $\mathbf{C}$ combinator,
which reorders the order of abstractions in a $\lambda$-term. $\mathbf{C}$
can be seen as a special case of our $\CC$ when the type of computations
producing values $\alpha$ is $\gamma \to \alpha$ for some $\gamma$.


\section{Types and Typing Rules}

We now give a syntax for the types of our calculus alongside with a typing
relation. In the grammar below, $\nu$ ranges over atomic types from set
$\mathcal{T}$.

The types of our language consist of:
\begin{description}
\item[function] $\alpha \to \beta$, where $\alpha$ and $\beta$ are types
\item[atom] $\nu$, where $\nu$ is an atomic type from $\TT$
\item[computation] $\FF_E(\alpha)$, where $\alpha$ is a type and $E$ is an
  effect signature (defined next)
\end{description}

The only novelty here is the $\FF_E(\alpha)$
computation\footnote{Throughout this manuscript, we will be using the term
  \emph{computation} to mean values of type $\FF_E(\alpha)$. Programs
  written in our calculus are simply called terms and their normal forms
  are called values. To break it down, in our calculus, terms evaluate to
  values, some of which can be computations (those of an $\FF$ type).}
type. This type will be inhabited by effectful computations that have
permission to perform the effects described in $E$ and yield values of type
$\alpha$. The representation will be that of an algebraic expression with
operators taken from the signature $E$ and constants (elements of the
generator set) taken to be in type $\alpha$.

In giving the typing rules, we will rely on the standard notion of a
\emph{context}. For us, specifically, a context is a partial mapping from
the variables in $\XX$ to the types defined above.  We commonly write
$\Gamma, x : \alpha$ for a context that assigns to $x$ the type $\alpha$
and to other variables $y$ the type $\Gamma(y)$. We also write $x : \alpha
\in \Gamma$ to say that the context maps $x$ to $\alpha$. Note, however,
that for $\Delta = \Gamma, x : \alpha, x : \beta$, $x : \beta \in \Delta$
while $x : \alpha \notin \Delta$.

\emph{Effect signatures} are very much like contexts. They are partial
mappings from the set of operation symbols $\EE$ to pairs of types. We will
write the elements of effect signatures the following way:
$\typedop{op}{\alpha}{\beta} \in E$ means that $E$ maps $\op{op}$ to the
pair of types $\alpha$ and $\beta$\footnote{The two types $\alpha$ and
  $\beta$ are to be seen as the operation's \emph{input} and \emph{output}
  types, respectively.}. When dealing with effect signatures, we will often
make use of the disjoint union operator $\uplus$. The term $E_1 \uplus E_2$
serves as a constraint demanding that the domains of $E_1$ and $E_2$ be
disjoint and at the same time it denotes the effect signature that is the
union of $E_1$ and $E_2$.

The last kind of dictionary used by the type system is a standard
\emph{higher-order signature} for the constants (a map from names of
constants to types). For those, we adopt the same conventions.

In our typing judgments, contexts will appear to the left of the turnstile
and they will hold information about the \emph{statically} (lexically)
bound variables, as in STLC.\@ Effect signatures will appear as indices of
computation types and they will hold information about the operations that
are \emph{dynamically} bound by handlers. Finally, there will be a single
higher-order signature that will globally characterize all the available
constants.

The typing judgments are presented in Figure~\ref{fig:types}. Metavariables
$M$, $N$\ldots\ stand for expressions, $\alpha$, $\beta$,
$\gamma$\ldots\ stand for types, $\Gamma$, $\Delta$\ldots\ stand for
contexts, $\op{op}$, $\op{op}_i$ stand for operation symbols and $E$,
$E'$\ldots\ stand for effect signatures. $\Sigma$ refers to the
higher-order signature giving types to constants.


\begin{figure}
  \def\labelSpacing{4pt}

  \begin{subfigure}{.5\textwidth}
   \begin{prooftree}
    \AxiomC{$\Gamma, x : \alpha \vdash M : \beta$}
    \RightLabel{[abs]}
    \UnaryInfC{$\Gamma \vdash \lam{x}{M} : \alpha \to \beta$}
   \end{prooftree}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
   \begin{prooftree}
    \AxiomC{$\Gamma \vdash M : \alpha \to \beta$}
    \AxiomC{$\Gamma \vdash N : \alpha$}
    \RightLabel{[app]}
    \BinaryInfC{$\Gamma \vdash M N : \beta$}
   \end{prooftree}
  \end{subfigure}

  \vspace{2mm}
 
  \begin{subfigure}{.5\textwidth}
   \begin{prooftree}
    \AxiomC{$x : \alpha \in \Gamma$}
    \RightLabel{[var]}
    \UnaryInfC{$\Gamma \vdash x : \alpha$}
   \end{prooftree}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
   \begin{prooftree}
    \AxiomC{$c : \alpha \in \Sigma$}
    \RightLabel{[const]}
    \UnaryInfC{$\Gamma \vdash c : \alpha$}
   \end{prooftree}
  \end{subfigure}

  \vspace{6mm}

  \begin{subfigure}{.5\textwidth}
   \begin{prooftree}
    \AxiomC{$\Gamma \vdash \eta : \alpha \to \FF_E(\alpha)$
    \hskip 4pt [$\eta$]}
   \end{prooftree}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
   \begin{prooftree}
    \AxiomC{$\Gamma \vdash \cherry : \FF_\emptyset(\alpha) \to \alpha$
    \hskip 4pt [$\cherry$]}
   \end{prooftree}
  \end{subfigure}

  \vspace{3mm}

  \hspace{-1.5cm}
  \begin{subfigure}{.5\textwidth}
   \begin{prooftree}
    \AxiomC{$\typedop{op}{\alpha}{\beta} \in E$}
    \RightLabel{[op]}
    \UnaryInfC{$\Gamma \vdash \op{op} : \alpha \to (\beta \to \FF_E(\gamma)) \to \FF_E(\gamma)$}
   \end{prooftree}
  \end{subfigure}
  \hspace{1cm}
  \begin{subfigure}{.5\textwidth}
   \begin{prooftree}
    \AxiomC{$E = \{\typedopg{\op{op}_i}{\alpha_i}{\beta_i}\}_{i \in I} \uplus E_f$}
    \noLine
    \def\extraVskip{0pt}
    \UnaryInfC{$E' = E'' \uplus E_f$}
    \noLine
    \UnaryInfC{$[\Gamma \vdash M_i : \alpha_i \to (\beta_i \to
      \FF_{E'}(\delta)) \to \FF_{E'}(\delta)]_{i \in I}$}
    \noLine
    \UnaryInfC{$\Gamma \vdash M_\eta : \gamma \to \FF_{E'}(\delta)$}
    \def\extraVskip{2pt}
    \RightLabel{[$\banana{}$]}
    \UnaryInfC{$\Gamma \vdash \banana{(\onto{\op{op}_i}{M_i})_{i \in I},\ \onto{\eta}{M_\eta}} : \FF_{E}(\gamma) \to \FF_{E'}(\delta)$}
   \end{prooftree}
  \end{subfigure}

  \vspace{6mm}

  \begin{subfigure}{\textwidth}
   \begin{prooftree}
    \AxiomC{$\Gamma \vdash \CC : (\alpha \to \FF_E(\beta)) \to
      \FF_E(\alpha \to \beta)$
    \hskip 4pt [$\CC$]}
   \end{prooftree}
  \end{subfigure}

  \caption{\label{fig:types}Typing rules for our calculus.}
\end{figure}


The typing rules mirror the syntax of expressions. Again, the first four
rules come from STLC.\@ The next four deal with introducing pure
computations, enriching them with effectful operations, handling those
operations away and finally eliminating pure computations. The $\CC$ rule
lets us start to see what we meant by saying that the $\CC$ operator lets
the function type and the computation type commute.

Let us ponder the types of the new constructions so as to get a grip on the
interface that the calculus provides us for dealing with computation
values.

\subsection*{[$\eta$]}

First off, we have the $\eta$ operator. It takes a value of type $\alpha$
and injects it into the type $\FF_E(\alpha)$. The meta-variable $E$ is
free, meaning $\eta$ can take values of type $\alpha$ to type
$\FF_E(\alpha)$ for any $E$. The algebraic intuition would say that elements
of the generator set are valid algebraic expressions independent of the
choice of signature. Computationally, returning a value is always an
option, independently of the available permissions.

\subsection*{[op]}

More complicated computations can be built up by extending existing
computations using the \textbf{operation} construction. Let us have an
effect signature $E$, such that $\typedop{op}{\alpha}{\beta} \in E$. To use
$\op{op}$, we first apply it to a value of the input type $\alpha$ and to a
continuation. The continuation is a function of type $\beta \to
\FF_E(\gamma)$ that accepts a value of the output type $\beta$ (the result
of performing the operation) and chooses in return a computation that
should be pursued next. The return type of our new computation will thus be
the return type $\gamma$ of the computation provided by the
continuation. The continuation's computation and the new extended
computation will also share the same effect signature $E$. This means that
all uses of the operation $\op{op}$ within the created computation have the
same input and output types\footnote{In general, the same operation symbol
  can be used in different computations whose types are indexed by
  different effect signatures.}.

Notice also how the [op] rule is similar to the [var] rule in that it lets
use a symbol with a certain type given that it exists in some kind of
signature/context. The crucial difference is that contexts are components
of judgments whereas effect signatures are components of types. The meaning
of a variable is determined by inspecting the expression in which it occurs
and finding the $\lambda$ that binds it (this is known as \emph{lexical} or
\emph{static binding}). On the other hand, the meaning of an operation in a
computation is determined by evaluating the term in which the computation
appears until the computation becomes the argument of a handler. This
handler will then give meaning to the operation symbol by effectively
substituting it with a suitable interpretation (this kind of \emph{late}
binding is known as \emph{dynamic binding}).

We have now seen how to construct pure computations using $\eta$ and extend
them by adding operations. However, before we go on and start talking about
handlers, we would like to give the algebraic intuition behind [op] as the
algebraic point of view makes explaining the handler rule [$\banana{}$]
easier.

We can see the effect signature as an algebraic signature. Whenever we have
$\typedop{op}{\alpha}{\beta} \in E$, we have an $\alpha$-indexed family of
operators of arity $\beta$. Let's unpack this statement.

\begin{itemize}
\item First, there is the matter of having an indexed family of
  operators. A common example of these is the case of scalar multiplication
  in the algebra of a vector space. A \emph{single-sorted algebraic
    signature} is a set of operation symbols, each of which is given an
  arity (a natural number). For vector addition, the arity is 2, since
  vector addition acts on two vectors (two elements of the
  domain). Scalar multiplication acts on one scalar and one
  vector. However, neither arity 1 nor arity 2 adequately express this. We
  can get around the limitations of a single-sorted signature by
  introducing for every scalar $k$ an operation of arity 1 that corresponds
  to multiplying the vector by $k$. Scalar multiplication is therefore not
  a single operator but a scalar-indexed family of operators.

  The very same strategy is applied here as well. A single operation symbol
  doesn't need to map to a single operator but can instead map to (possibly
  infinitely) many operators indexed by values of some type $\alpha$. For
  example, writing messages to the program's output
  ($\typedop{print}{string}{1}$) can be seen as a string-indexed family of
  unary operators on computations. For every string $s$, we get an operator
  that maps computations $c$ to computations that first print $s$ and then
  continue as $c$.

\item Next, we were speaking about operators of arity $\beta$. The use of a
  type in place of a numerical arity is due to a certain generalization. In
  set theory, natural numbers become sets that have the same cardinality as
  the number they represent ($\left\vert N \right\vert = N$). We can
  therefore conservatively generalize the idea of arity to a set by saying
  that an operator of arity $X$ takes one operand per each element of the
  set $X$. It's a short step from there to using types as arities, wherein
  an operator of arity $\beta$ takes one operand per possible value of type
  $\beta$.

  This will come in very handy in our system. We want our operator
  $\op{op}$ to have as many operands as there are possible values in the
  output type $\beta$. Therefore, we simply say the operator has arity
  $\beta$.

  How do we write down the application of an operator of arity $\beta$ to
  its operands? We can no longer just list out all the operands, since
  types in our calculus may have an unbounded number of inhabitants. We
  will organize operands in \emph{operand clusters}, arity-indexed families
  of operands. We will write them down as functions, using
  $\lambda$-abstraction, from the arity type $\beta$ to some operand type,
  e.g., $\FF_E(\gamma)$.
\end{itemize}

Now we can understand what it means to say that
$\typedop{op}{\alpha}{\beta} \in E$ gives rise to a $\alpha$-indexed family
of operators of arity $\beta$. We apply to $\op{op}$ an index of type
$\alpha$ to get an operator and then we apply that operator to an operand
cluster of type $\beta \to \FF_E(\gamma)$ to get a new expression of type
$\FF_E(\gamma)$.

We suggest visualizing these algebraic expressions as trees. Trees of type
$\FF_E(\alpha)$ consist of leafs containing values of type $\alpha$ and
internal nodes labelled with operations and their parameters. Every
internal node is labelled with some $\typedop{op}{\alpha}{\beta} \in E$ and
with a parameter of type $\alpha$ and it has a cluster of children indexed
by $\beta$.

\subsection*{[$\banana{}$]}

TODO: Write about bananas and cherries here.


\section{Reduction Rules}

We will now present a set of type-preserving rules that we will use to
evaluate/simplify expressions in our language.

\vspace{3mm}

\begin{tabular}{lr}
  $(\lambda x.\ M)\ N \rightarrow$ & [$\beta$] \\
  $M[x/N]$ & \\
  \\
  $[\mathcal{H}\ (OP_i\ M_i)\ldots\ (\eta\ M_\eta)]\ (\eta\ N) \rightarrow$ & [$\mathcal{H}$-$\eta$] \\
  $M_\eta\ N$ & \\
  \\
  $[\mathcal{H}\ (OP_i\ M_i)\ldots\ (\eta\ M_\eta)]\ (OP_i\ N_a\ N_k) \rightarrow$ & [$\mathcal{H}$-$OP$] \\
  $M_i\ N_a\ (\lambda x.\ [\mathcal{H}\ (OP_i\ M_i)\ldots\ (\eta\ M_\eta)]\ (N_k\ x))$ & where $x$ is fresh \\
  \\
  $[\mathcal{H}\ (OP_i\ M_i)\ldots\ (\eta\ M_\eta)]\ (OP\ N_a\ N_k) \rightarrow$ & [$\mathcal{H}$-$OP^*$] \\
  $OP\ N_a\ (\lambda x.\ [\mathcal{H}\ (OP_i\ M_i)\ldots\ (\eta\ M_\eta)]\ (N_k\ x))$ & where $x$ is fresh \\
  & and $OP \notin \{OP_i\}_i$ \\
  \\
  $\mathcal{C}\ (\lambda x.\ \eta\ M) \rightarrow$ & [$\mathcal{C}$-$\eta$] \\
  $\eta\ (\lambda x.\ M)$ & \\
  \\
  $\mathcal{C}\ (\lambda x.\ OP\ M_a\ M_k) \rightarrow$ & [$\mathcal{C}$-$OP$] \\
  $OP\ M_a\ (\lambda y.\ \mathcal{C}\ (\lambda x.\ M_k\ y))$ & where $y$ is fresh \\
  & and $x \notin FV(M_a)$
\end{tabular}

\vspace{3mm}

Besides the standard $\beta$-reduction rule, we have rules that define the
behavior of the handlers formed using $\mathcal{H}$ and of the
$\mathcal{C}$ operator.


\section{Sums and Products}


\section{Common Combinators}

Here we will introduce a collection of generally useful syntactic shortcuts
and combinators for our calculus.

\begin{align*}
  \_ \circ \_ &: (\beta \to \gamma) \to (\alpha \to \beta) \to (\alpha \to \gamma) \\
  f \circ g &= \lambda x.\ f\ (g\ x) \\
  \_^* &: (\alpha \to \mathcal{F}(\beta)) \to (\mathcal{F}(\alpha) \to \mathcal{F}(\beta)) \\
  f^* &= [\mathcal{H}\ (\eta\ f)] \\
  \mathcal{F} &: (\alpha \to \beta) \to (\mathcal{F}(\alpha) \to \mathcal{F}(\beta)) \\
  \mathcal{F} &= \lambda f.\ (\eta \circ f)^* \\
  \_ \hsbind \_ &: \mathcal{F}(\alpha) \to (\alpha \to \mathcal{F}(\beta)) \to \mathcal{F}(\beta) \\
  M \hsbind N &= N^*\ M \\
  \\
  [\mathcal{H}\ (OP_i\ M_i)\ldots] &= [\mathcal{H}\ (OP_i\ M_i)\ldots\ (\eta\ \eta)]
\end{align*}

We will also make use of combinators for function application where one or
more of the arguments are computations.

\begin{align*}
  \_ \apl \_ &: \mathcal{F}(\alpha \to \beta) \to \alpha \to \mathcal{F}(\beta) \\
  F \apl x &= F \hsbind (\lambda x.\ \eta\ (f\ x)) \\
  \_ \apr \_ &: (\alpha \to \beta) \to \mathcal{F}(\alpha) \to \mathcal{F}(\beta) \\
  f \apr X &= X \hsbind (\lambda x.\ \eta\ (f\ x))
\end{align*}
