\bookmarksetup{startatroot}
\chapter{Conclusion}
\label{chap:conclusion}

We start the conclusion of our thesis by summarizing the results we have
obtained (Section~\ref{sec:summary}). We proceed chapter by chapter and
review the material, highlighting the important contributions. We then
examine related work out there and compare it to our approach
(Section~\ref{sec:comparison}). We first speak about the calculus $\calc$
itself, considering the alternatives that exist already and explaining in
which ways $\calc$ is different~(\ref{ssec:comparison-calculus}). Then, we
turn to the linguistic analyses that we presented in our dissertation and
relate them to the closest analogues in semantics
literature~(\ref{ssec:comparison-linguistic}). Finally, we review recent
work which focuses on combining multiple monads or linguistic effects in a
single grammar and see how $\calc$ compares to
those~(\ref{ssec:comparison-combining}). We end our thesis bringing up
directions in which the work presented here can be extended
(Section~\ref{sec:future-work}).

\minitoc


\section{Summary of Results}
\label{sec:summary}

In Part~\ref{part:calculus}, we have introduced $\calc$, a formal calculus
that extends the simply-typed lambda calculus (STLC) with effects and
handlers.

The definition of $\calc$ is given in
Chapter~\ref{chap:definitions}. $\calc$ introduces a new family of types
into STLC, the computation types, and new terms, which are built out of
computation constructors and destructors. We gave a type system to the
calculus which extends that of STLC and a reduction semantics which
combines the STLC $\beta$ and $\eta$ reductions with definitions of the new
function symbols. During the course of the chapter, we maintain two
perspectives on the intended meaning of the terms: computations can be seen
as programs that interact with a system through a selected set of ``system
calls'' (operations) or they can be seen as algebraic expressions built
upon an infinitary algebraic signature.

In Chapter~\ref{chap:examples}, we gave an example of using the $\calc$
calculus. Besides familiarizing the reader with the notation and reductions
of the calculus, the example served as a preview of the kind of language
engineering we would be doing. During the chapter, we developed a
compositional semantics for a simple computing language with errors and
variables. This let us demonstrate the modularity of using our computation
monad, as we could add variables to the language without needing to modify
the semantics of the other constructions.

The main contribution of Part~\ref{part:calculus} lies in
Chapter~\ref{chap:properties}, in which we developed the metatheory of
$\calc$. In Section~\ref{sec:derived-rules}, concepts which are primitive
in some other languages (closed handlers and the $\hsbind$ operator) were
defined within $\calc$ and their typing rules and reduction rules were
derived from those of $\calc$. In Section~\ref{sec:algebraic-properties},
we then connected the calculus to the theory of monads by identifying a
monad in the category in which we interpret $\calc$ with our
\emph{denotational semantics}. In Section~\ref{sec:type-soundness}, we
proved \emph{subject reduction} of $\calc$. This result gives a basic
coherence between the type system of $\calc$ and the reduction semantics,
guaranteeing that types are preserved under reduction. This is complemented
by a limited proof of \emph{progress}, which states that terms which can no
longer be reduced must have a very specific shape. We followed this with
another fundamental property: \emph{strong normalization}. Its proof was
split into two parts: \emph{confluence} (proved in
Section~\ref{sec:confluence}) and \emph{termination} (proved in
Section~\ref{sec:termination}). The proofs of both confluence and
termination proceed by similar strategies: prove the property for the
calculus without $\eta$-reduction by applying a general result and then
extend the property to the complete calculus. In the case of confluence,
the general result is the confluence of orthogonal Combinatory Reduction
Systems~\cite{klop1993combinatory}. In the case of termination, we rely on
two techniques: the termination of the reduction relation of Inductive Data
Type Systems that validate the General Schema~\cite{blanqui2000termination}
and Higher-Order Semantic Labelling~\cite{hamana2007higher}, which lets us
use our denotational semantics to label the terms of our calculus so that
it validates the General Schema.

Andrej Bauer made the analogy that effects and handlers are to delimited
continuations what while loops or if-then-else statements are to
gotos~\cite{bauer2012lambda}. Continuations themselves have proven to be a
useful tool in natural language
semantics~\cite{de2001type,barker2002continuations,shan2005linguistic,de2006towards,barker2006continuations,barker2014continuations}. In
Chapter~\ref{chap:continuations}, we have shown how $\calc$ can simulate
delimited continuations, namely the $\shift$/$\reset$ delimited control
operators. We presented a typed call-by-value lambda calculus with $\shift$
and $\reset$ and have simulated its types and reductions in $\calc$.

In Part\ref{part:natural-language}, we have demonstrated the applications
of $\calc$ to the problem of modelling the meaning of natural language
utterances.

After having reviewed the basics of formal semantics in
Chapter~\ref{chap:intro-fs}, we have shown how computations in $\calc$ can
be used to give a compositional account of several ``non-compositional''
linguistic phenomena in Chapter~\ref{chap:introducing-effects}. We have
described how to introduce computations into a compositional semantics
while preserving the meanings assigned by the semantics in
Section~\ref{sec:lifting-semantics}. We have then presented analyses of
several linguistic phenomena in the framework of $\calc$ computations:
deixis (Section~\ref{sec:deixis}), conventional implicature à la Potts
(Section~\ref{sec:conventional-implicature}) and quantification à la
Montague (Section~\ref{sec:quantification}). We have then explicitly
described the methodology given to find the kinds of analyses that we have
presented in Chapter~\ref{chap:introducing-effects} as to encourage
researchers to develop other analyses in the framework.

We dedicated Chapter~\ref{chap:dynamic-semantics} to a particularly complex
phenomenon: dynamics. In Sections~\ref{sec:drt-as-pl}
and~\ref{sec:banana-drt}, we have shown how a $\calc$ analysis of dynamics
can be extracted from Discourse Representation Theory. This gave us a way
to handle dynamics in $\calc$ as well as strengthen the claim that effects
and handlers are suitable mechanisms for dealing with natural language. We
have also shown how to interpret the $\calc$ computations as the dynamic
propositions of de Groote's Type Theoretical Dynamic Logic
(TTDL)~\cite{de2006towards}. In her
dissertation~\cite{lebedeva2012expression}, Lebedeva extended TTDL with
exceptions to treat presuppositions and in
Section~\ref{sec:presuppositions}, we integrated Lebedeva's analysis of
presupposition into our $\calc$ analysis of dynamics (we compare our
adaptation with the original in the next section,
\ref{ssec:comparison-linguistic}). In Section~\ref{sec:double-negation}, we
considered another extension of TTDL, Double Negation
TTDL~\cite{qian2014accessibility} and we have shown why the kind of
generalization of denotations done in Double Negation TTDL is not amenable
to being analyzed as a side effect in $\calc$.

In Chapter~\ref{chap:composing-effects}, we have supported our claim that
using effects and handlers lets us combine distinct phenomena in a single
grammar. We have started with the dynamic grammar developed in
Chapter~\ref{chap:dynamic-semantics}, repeated in
Sections~\ref{sec:dynamic-kernel} and~\ref{sec:adding-presuppositions}. We
have then extended this grammar with conventional
implicatures~(\ref{sec:adding-conventional-implicature}),
deixis~(\ref{sec:adding-deixis}) and
quantification~(\ref{sec:adding-quantification}) with little to no
modification of the original semantics. We finished the chapter by
sketching out an analysis of restrictive relative clauses and their
interactions with presuppositions in Section~\ref{sec:relative-clauses}.


\section{Comparison with Existing Work}
\label{sec:comparison}

\subsection{Calculus}
\label{ssec:comparison-calculus}

$\calc$ can be compared to several existing calculi and implementations of
effects and handlers:

\begin{itemize}
\item System F (i.e.\ the polymorphic lambda calculus or the second-order
  lambda calculus)

  $\calc$ extends the simply-typed lambda calculus with computation types
  $\FF_E(\alpha)$. Computations are algebraic expressions and as such can
  be expressed as inductive data types.\footnote{An inductive type is a
    recursive type with positive
    constructors. In~\ref{ssec:termination-for-idts}, we have seen that a
    computation type $\FF_E(\alpha)$ has positive constructors $\eta$ and
    $\op{op}$ for every $\op{op} \in E$.} Inductive data types, along with
  the sums and products that we add to the calculus in
  Section~\ref{sec:sums-and-products}, can be expressed in System
  F~\cite{wadler1990recursive}.

  In $\calc$, a computation of type $\FF_E(\alpha)$ can also be given the
  type $\FF_{E \uplus E'}(\alpha)$, where $E \uplus E'$ is an extension of
  $E$. However, in the direct encoding of $\calc$ into System F, for every
  effect signature $E \uplus E'$ that we would like to ascribe to a
  computation, we would end up with a different term. On the other hand, in
  $\calc$ we can keep using the same term. This lets us give a semantics to
  lexical items that does not have to change when new effects are
  introduced into the theory.

\item \emph{Eff}

  The \emph{Eff} language~\cite{bauer2012programming} is an ML-like
  programming language with effects and handlers. Like in ML, effects can
  be freely used within any expression, without any term encoding (we say
  that the calculus is \emph{direct-style}). For this to work correctly,
  the calculus has a fixed evaluation order, which, following ML, is
  call-by-value.

  We have used \emph{Eff} in our first explorations of effects and handlers
  in natural language semantics~\cite{marsik2014algebraic}, benefiting from
  the existing implementation. However, we have found that besides
  call-by-value, call-by-name evaluation is also common, notably on the
  boundaries of lexical items (see~\ref{ssec:cbn-and-cbv}). Call-by-name
  can be simulated in call-by-value by passing around thunks (functions of
  type $1 \to \alpha$ for some $\alpha$). However, in the presence of both
  call-by-name and call-by-value, we have opted for an indirect
  presentation of effects using monads which favors neither call-by-value
  nor call-by-name and that lets us manipulate the order of execution using
  $\hsbind$.

  Finally, we note that \emph{Eff} is a general-purpose programming
  language which includes general recursion (\texttt{let rec}) and
  therefore it is not terminating, contrary to $\calc$.
  
\item $\lambda_{\mathrm{eff}}$

  The $\lambda_{\mathrm{eff}}$ calculus~\cite{kammar2013handlers} is a
  call-by-push-value lambda calculus~\cite{levy1999call} with operations
  and handlers. Call-by-push-value is special in introducing two kinds of
  terms: computations and values. The intuition behind the two is that
  computations \emph{do}, whereas values \emph{are}. Two of the crucial
  things that computations do are to pop values from a stack (that is what
  abstractions do) and to push values to the stack (that is what
  applications do). Therefore, applications and abstractions are considered
  as computations. Furthermore, the function in an application term must be
  a computation term (which is expected to, among other things, pop a value
  from the stack), whereas the argument, which is the value to be pushed to
  the stack, must be a value term.

  This might make it look like that call-by-push-value is like
  call-by-value since all the arguments passed to functions are
  values. However, in true call-by-value, we can use complex expressions as
  arguments and we expect that the reduction system will evaluate the
  arguments down to values before passing them to the function. To do this
  in call-by-push-value, we have to implement this manually by evaluating
  the argument computation down to a value $x$ and then passing the value
  $x$ to the function in question (i.e.\ in $\lambda_{\mathrm{eff}}$ syntax
  $\textbf{let } x \from M \textbf{ in } \ap{F}{x}$). In $\calc$, this
  amounts to the term $M \hsbind F$, where $M : \FF_E(\alpha)$ and
  $F : \alpha \to \FF_E(\beta)$. To implement call-by-name, computations
  can be mapped to values by wrapping them in thunks, which are primitive
  constructs in call-by-push-value (in $\lambda_{\mathrm{eff}}$ syntax
  $\ap{F}{\{M\}}$, where $M$ is a computation and the thunk $\{M\}$ is a
  value). In $\calc$, the corresponding term is $\ap{F}{M}$, where
  $M : \FF_E(\alpha)$ and $F : \FF_E(\alpha) \to \FF_E(\beta)$.

  $\lambda_{\mathrm{eff}}$ presents an intriguing alternative to
  $\calc$. The call-by-push-value calculus is flexible enough to
  accommodate both call-by-name and call-by-value. $\lambda$ abstractions
  and operations are both treated as effects, which might make the
  definition of the $\CC$ operator, which permutes $\lambda$ with
  operations, more intuitive.\footnote{The extra typing rule for the $\CC$
    construction in $\lambda_{\mathrm{eff}}$ would look like this:
    \begin{prooftree}
      \AxiomC{$\Gamma \vdash_E M : A \to C$}
      \UnaryInfC{$\Gamma \vdash_E \ap{\CC}{M} : F (U_\emptyset (A \to C))$}
    \end{prooftree}} $\lambda_{\mathrm{eff}}$ also has a well-developed
  metatheory, developed in~\cite{kammar2013handlers}: it is both confluent
  (due to its reduction relation being deterministic) and terminating
  (thanks to its effect type system).

  $\lambda_{\mathrm{eff}}$ served as an inspiration to the design of
  $\calc$; notably, $\calc$'s effect system is based on that of
  $\lambda_{\mathrm{eff}}$. However, $\calc$ diverges from
  $\lambda_{\mathrm{eff}}$ in that it is a proper extension of the
  simply-typed lambda calculus (STLC): every term, type, typing judgment or
  reduction in STLC is also a term, type, typing judgment or reduction in
  $\calc$. For example, the STLC term $\lam{x}{x}$ is not a
  $\lambda_{\mathrm{eff}}$ term. Its closest counterparts in
  $\lambda_{\mathrm{eff}}$ would be either
  $\vdash_\emptyset \lam{x}{\textbf{return } x} : A \to F(A)$, where $A$ is
  a value type, or $\vdash_E \lam{x}{x!} : U_E(C) \to C$, where $C$ is a
  computation type (a function or an effectful computation). On the other
  hand, in $\calc$, $\vdash \lam{x}{x} : \alpha \to \alpha$ is a valid term
  for any $\alpha$, be it an atomic type such as $o$, a function type such
  as $\iota \to o$ or a computation type such as $\FF_E(o)$.

  The fact that $\calc$ is an extension of STLC motivates its use for two
  reasons. First, STLC is the lingua franca of formal semantics. $\calc$
  already introduces a lot of new notation and the use of effects in
  natural language semantics is not yet ubiquitous. By basing $\calc$ on
  STLC, we narrow the gap between the common practice of formal semantics
  and our use of effects and monads, hopefully making the technique more
  approachable to researchers in the field. Second, the purpose of the
  calculus is to write down computations that produce logical
  representations. By having STLC as a subpart of $\calc$, terms of
  Church's simple type theory (i.e.\ formulas of higher-order logic) are
  already included in our calculus and we can reuse the same notions of
  $\lambda$-abstraction and variables. In $\lambda_{\mathrm{eff}}$, we
  would either need to add constructors for logic formulas (i.e.\ having
  some logic as an object language over the terms of which the meta
  language $\lambda_{\mathrm{eff}}$ would calculate) or use
  call-by-push-value computations in our logical representations.

\item Extensible Effects of Kiselyov et al~\cite{kiselyov2013extensible}
  and other implementations of effect systems in pure functional
  programming languages (Haskell, Idris \ldots)

  Our adoption of a free monad and effect handlers was motivated by the
  paper of Kiselyov, Sabry and Swords on \emph{extensible
    effects}~\cite{kiselyov2013extensible}. The paper presented a Haskell
  library for encoding effectful computations, combining computations with
  diverse effects and interpreting them by composing a series of modular
  interpreters. The library used a free monad (in the style
  of~\cite{swierstra2008data}): a computation is either a pure value
  ($\eta$ in $\calc$) or a request to perform some kind of effect (an
  operation in $\calc$). These requests are then handled by interpreters
  which behave similarly to effect handlers (the authors
  of~\cite{kiselyov2013extensible} also relate handlers to the technique of
  ``extensible denotational language specifications'' published in 1994 by
  Cartwright and Felleisen~\cite{cartwright1994extensible}). The paper
  demonstrated that the approach is more flexible when it comes to
  combining interacting effects than the existing state-of-the-art
  technique of using monad transformers. A more refined version of the
  approach was published in~\cite{kiselyov2015freer} and similar
  implementations of effects and handlers exist also in other pure
  functional programming languages such as
  Idris~\cite{brady2013programming}.

  The extensible effects discipline provides the tools that we would like
  to use to build a modular semantics of natural language. However, we do
  not want our formal semantics to depend on the semantics of a large
  programming language such as Haskell\footnote{The implementations of
    extensible effects in Haskell make use of a wealth of language
    extensions which are not even part of the Haskell standard.} or
  Idris. We created $\calc$ to reap the benefits of extensible effects
  without incurring the complexity of using a language like Haskell as our
  meta language. $\calc$ extends STLC with computation types, two
  constructors ($\eta$ and operations), two destructors (handlers and
  $\cherry$) and the $\CC$ operator. Unlike Haskell, our extension of STLC
  preserves strong normalization.
\end{itemize}


\subsubsection{The Case of the $\CC$ Operator}

A notable feature which distinguishes $\calc$ from all of the
above-mentioned calculi is the $\CC$ operator. The $\CC$ operator was added
relatively late to the $\calc$ calculus as a solution to the following
problem.

\begin{exe}
  \ex A man$_1$ walks in the park. He$_1$ whistles. \label{ex:C-operator}
\end{exe}

In Example~\ref{ex:C-operator}, the noun phrase \emph{a man} introduces a
quantifier ranging over men that takes scope over its continuation
$\lam{x}{\sem{x \text{ walks in the park. He whistles.}}} : \iota \to
\FF_E(o)$.\footnote{In our presentation of dynamic semantics, where
  $\sem{S} = \FF_E(1)$, the type of the continuation would be
  $\iota \to \FF_E(1)$.} The problem is how to combine an effectful
predicate of type $\iota \to \FF_E(o)$ with a quantifier such as
$\exists : (\iota \to o) \to o$. The key insight is that the effects of the
continuation usually do not depend on the individual being talked about. No
matter whether the sentence speaks about Albert, Bill or Charles, the
continuation will proceed the same. The continuation will look into the
context to retrieve the antecedent $x$, which is known to satisfy the
predicate $\obj{man}$ and therefore be eligible for use with a masculine
pronoun. It will then produce the logical formula
$\ap{\obj{walk-in-the-park}}{x} \land \ap{\obj{whistle}}{x}$. The
resolution of the anaphoric pronoun does not depend on the particular
individual being discussed.

DRT is capable of deriving a meaning for Example~\ref{ex:C-operator}
without considering which individual the noun phrase \emph{a man} refers
to. DRT does this by calculating with symbolic representations. The noun
phrase \emph{a man} will introduce a \emph{discourse referent}, a symbolic
object distinct from any individual found in the model. Then the evaluation
of the discourse continues and the anaphoric pronoun can resolve to this
symbolic object. We could do the same in $\calc$. We could state that the
$\iota$ type is not the type of individuals in the model, but the type of
some symbolic objects (discourse referents or variables), and that the type
$o$ is not the type of propositions, but the type of logical
formulas. Assuming that we had an operation $\typedop{gensym}{1}{\iota}$
which could give us fresh variables and that the type of the constructor
for formulas of existential quantification would be
$\exists : \iota \to o \to o$, we could wrap the existential quantifier
over an effectful computation $P : \iota \to \FF_E(o)$ the following way:

\begin{align*}
  &\app{\op{gensym}}{\star}{(\lam{x}{ \\
  &\ap{P}{x} \hsbind (\lam{p}{ \\
  &\etaE{(\app{\exists}{x}{p})}})})}
\end{align*}

By changing the meaning of the type $\iota$ to be the type of symbolic
references, we solve our problem. When we need to evaluate the effects of a
continuation of type $\iota \to \FF_E(o)$, we do not need to know the
precise identity of the individual $\iota$. Instead, we apply the
continuation to a symbolic object which will stand in for any such
individual and then proceed to evaluate the effects of the
continuation. This is the approach that we were using at the beginning of
the project~\cite{marsik2014algebraic}.

However, this approach has several downsides. First, our formal semantics
is contaminated by extra complexity due to the management of variables and
operations like $\op{gensym}$. Second, there is an extra level of
indirection in place. Instead of computing the truth value of a sentence in
some model, we compute a formula and then that formula itself can be
evaluated in a model. Third, binding in logical formulas is no longer
managed by the (meta) calculus. This means that it is easy to $\op{gensym}$
a variable that is supposed to occur in the scope of a quantifier and then
have that variable accidentally project outside of its scope, leading to
the generation of malformed logical formulas.

The $\CC$ operator presents another solution to this problem. If we know
that in a continuation of type $\iota \to \FF_E(o)$, the effects of the
$\FF_E(o)$ do not depend on the $\iota$, we could move them out of the body
of the function. This is exactly what the $\CC$ operator does.\footnote{If
  we are mistaken and the effects do depend on the bound variable, then
  computation gets stuck (instead of producing a malformed logical
  formula).} With it, we can wrap the existential quantifier over the
continuation using the following expression, which we called $\existsr P$
in~\ref{ssec:revising-dynamic-handler}:\footnote{The type of $\exists$ is
  $(\iota \to o) \to o$ and the type of $P$ is $\iota \to \FF_E(o)$.}

$$
  \exists \apr (\ap{\CC}{P})
$$

The inspiration for the $\CC$ operator came from Philippe de Groote's work
on logical relations and
conservativity~\cite{degroote2015conservativity}. Within his work, de
Groote makes use of the following mathematical structure: a type
transformer $\pT$ equipped with three operation $\pU$, $\pBullet$ and $\pC$
with the following types:

\begin{align*}
  \pU &: \alpha \to \pT \alpha \\
  \pBullet &: \pT(\alpha \to \beta) \to \pT \alpha \to \pT \beta \\
  \pC &: (\alpha \to \pT \beta) \to \pT (\alpha \to \beta)
\end{align*}

and obeying the following laws:

\begin{align*}
  (\ap{\pU}{f}) \pBullet (\ap{\pU}{a}) &= \ap{\pU}{(\ap{f}{a})} \\
  \ap{\pC}{(\lam{x}{\ap{\pU}{(\ap{f}{x})}})} &= \ap{\pU}{f}
\end{align*}

If we ignore the $\pC$ operator for the moment and focus only on the $\pT$
type transformer, the $\pU$ and $\pBullet$ operations and the first of the
two laws, we see that we have a weaker version of an applicative
functor. An applicative functor is a functor, which in our category is a
type transformer such as $\pT$ that can also lift functions, equipped with
two operations, $\pure$ and $\circledast$, which have the same types as
$\pU$ and $\pBullet$ respectively, and satisfying the four laws given
in~\ref{ssec:applicative-functor}. The first of the two laws given above is
the homomorphism law~\ref{law:app-homomorphism} of applicative functors. We
note that all three instances of this structure that are used in the
examples of~\cite{degroote2015conservativity} also satisfy all four of the
applicative functor laws.

We now look at the $\pC$ operator, whose type is the same as the $\CC$
operator that we added to $\calc$ (with $\FF_E$ corresponding to
$\pT$). Its behavior is specified only by the second law, which defines the
value of $\pC$ when applied to a pure function (a function of the form
$\lam{x}{\ap{\pU}{(\ap{f}{x})}}$). Our $\CC$ also obeys this law by
including it as the $\CC_\eta$ reduction rule which is part of the
definition of $\CC$:\footnote{The fact that we can take this law and use it
  as a definition of the $\CC$ operator is due to $\eta$ being a
  constructor in $\calc$, $\eta$-headed expressions are not head-reducible
  and therefore can be reliably pattern-matched on.}

$$
\ap{\CC}{(\lam{x}{\ap{\eta}{(\ap{f}{x})}})} \; \to_{\CC_\eta} \;
\etaE{(\lam{x}{\ap{f}{x}})} \; \to_\eta \; \etaE{f}
$$

Our definition of $\CC$ also includes the $\CC_{\op{op}}$ reduction
rule. This extends the definition of $\CC$ to computations which use
operations. However, our definition of $\CC$ is not total, as there are
well-typed arguments for $\CC$ which will cause computation to get
stuck. This correlates with the $\pC$ operator in de Groote's work, where
certain values of the $\CC$ operator are left undefined.

It is interesting that the $\CC$ operator that turned out to be useful in
our work originated in another work whose objective is to capture the way
meanings are extended in formal semantics. In our approach, we address
conservativity\footnote{Conservativity is understood as the notion that
  extending the grammar to cover a new phenomenon should not change the
  meanings which were already correct in the simpler grammar.} in two
ways. First, we show how to lift a simply-typed semantics into a monadic
semantics while preserving the meanings it assigns to sentences in
Section~\ref{sec:lifting-semantics}. Then, in
Chapter~\ref{chap:composing-effects}, we extend a dynamic grammar with
presuppositions~(\ref{sec:adding-presuppositions}), conventional
implicature~(\ref{sec:adding-conventional-implicature}),
deixis~(\ref{sec:adding-deixis}) and
quantification~(\ref{sec:adding-quantification}), and at every step, the
only modifications we make to existing entries is to add handlers for new
effects, which does not affect the meaning of any of the sentences analyzed
before. Furthermore, since for any effect signature $E$, $\FF_E$ is a type
transformer which together with $\eta$ as $\pU$, $\aplr$ as $\pBullet$ and
$\CC$ as $\pC$ has the structure needed
in~\cite{degroote2015conservativity}, de Groote's technique of logical
relations should apply to the kinds of meanings produced by our approach as
well.\footnote{Modulo the partiality of $\CC$.}


\subsection{Linguistic Modelling}
\label{ssec:comparison-linguistic}

Most of the linguistic analyses presented in our thesis are translations of
existing analyses to the $\calc$ framework:

\begin{itemize}
\item Quantification 

  Our technique of using the $\op{scope}$ operation follows approaches that
  propose the use of continuations in natural
  language~\cite{de2001type,barker2002continuations}. The type and the
  semantics of our $\op{scope}$ operation match those of $\shift$, a
  control operator for delimited continuations used to treat quantification
  in~\cite{shan2005linguistic}. Instead of context levels, we use
  quantifier raising to account for scope ambiguity. As in Montague's
  work~\cite{montague1973proper}, we have a syntactic construction rule
  that takes a quantificational noun phrase and uses it to bind an $NP$
  variable within a sentence ($\abs{QR} : QNP \limp (NP \limp S) \limp
  S$). The syntactic type of $\abs{QR}$ is based on the type given to the
  determiners \emph{every} and \emph{some}
  in~\cite{pogodalla2007generalizing}
  ($C_{\text{every}} : N \limp (NP \limp S) \limp S$). By composing
  $\abs{every} : N \limp QNP$ and
  $\abs{QR} : QNP \limp (NP \limp S) \limp S$, we get
  $\lam{N}{\ap{\abs{QR}}{(\ap{\abs{every}}{N})}} : N \limp (NP \limp S)
  \limp S$. In~\ref{ssec:algebraic-quantification}, we have also pointed
  out that a $\calc$ computation invoking a series of $\op{scope}$
  operations is like a nested sequence of quantifiers in Keller
  storage~\cite{keller1988nested}. However, unlike Keller storage (and
  Cooper storage~\cite{cooper1979montague}), the quantifiers cannot be
  retrieved in any order since one could bind a variable in
  another.\footnote{We could imagine adding a rule allowing us to permute
    the order of certain operations, as long as the result of one is not
    bound in the input to the other. However, we would lose confluence of
    our calculus at that point.} Therefore, we solve ambiguity by other
  means ($\abs{QR}$).


\item Conventional Implicature

  Our analysis stems from Potts' dissertation on the logic of conventional
  implicatures~\cite{potts2005logic}. This multidimensional approach to the
  semantics of conventional implicatures was adapted into a monadic
  treatment by Giorgolo, Asudeh et al in
  2011~\cite{giorgolo2011multidimensional,giorgolo2012monads}. Monadic
  treatments of conventional implicatures have also appeared in recent
  ESSLLI courses: the treatment of expressives by Kiselyov and Shan at
  ESSLLI 2013~\cite{kiselyov2010lambda} and by Barker and Bumford at ESSLLI
  2015~\cite{barker2015monads}. The underlying monad in all the monadic
  treatments is the writer monad. The writer monad is parameterized by a
  monoid. Every computation in the monad is accompanied by an element of
  the monoid and composing computations has the effect of combining their
  monoidal components. In the case of the writer monad for conventional
  implicatures, the monoid is some monoid of implicatures, usually the
  conjunctive monoid on propositions. The structure needs to be a monoid
  because we need a neutral element for phrases without any conventional
  implicatures and we need associativity of the monoid to get associativity
  of the monad.

  In the free monad approach, we can characterize the writer monad by a
  single operation which computations can use to communicate elements from
  the monoid. This is what we do in
  Section~\ref{sec:conventional-implicature}, with
  $\typedop{implicate}{o}{1}$ as the operation and
  $\operatorname{accommodate'}$ from~\ref{ssec:algebraic-ci} as the handler
  which defines the monoid ($\land$ as the binary operation in the
  $\op{implicate}$ clause and $\top$ as the neutral element in the $\eta$
  clause). This treatment of conventional implicatures is equivalent to the
  existing monadic analyses.

  However, in Section~\ref{sec:adding-conventional-implicature}, we study
  the interaction of dynamics with conventional implicatures. In our
  approach, the at-issue truth conditions are communicated by the two
  operations $\typedop{assert}{o}{1}$ and
  $\typedop{introduce}{1}{\iota}$. We take our cue from Layered
  DRT~\cite{geurts2003layered} and Projective
  DRT~\cite{venhuizen2013parsimonious} to do the same for conventional
  implicatures: we complement $\typedop{implicate}{o}{1}$ with
  $\typedop{introduce^i}{1}{\iota}$. This way, the monadic structure behind
  conventional implicatures becomes similar to the one used for dynamics
  (continuations with state). Conventional implicatures become updates to
  the context which are not blocked by logical operators such as
  negation. In our system, we get a compositional semantics which can
  account for binding out of appositives, as in Example~\ref{ex:hospital}.

  \begin{exe}
    \exr{ex:hospital} John, who nearly killed a woman$_1$ with his car,
    visited her$_1$ in the hospital.
  \end{exe}


\item Deixis

  In his treatment of indexicals, Kaplan models meanings as
  \emph{characters}, functions from contexts to intensions. Contexts have
  several components, among them the \emph{agent} which is making the
  utterance. In our approach, we add getters for the components of the
  context. In dynamics, we use $\typedop{get}{1}{\gamma}$ to get the
  anaphoric part of the context, and in treating the first-person pronoun,
  we use $\typedop{speaker}{1}{\iota}$ to recover the speaker of the
  utterance.

  We use deixis as the first example in
  Chapter~\ref{chap:introducing-effects}. Wanting to give an example of a
  handler for $\op{speaker}$ that appears within a lexical entry, we turned
  to direct quotations. However, the meanings that we give to sentences
  such as Example~\ref{ex:mary-peter} only express part of the
  meaning. Namely, we capture the fact Peter claims that Mary kissed
  him. Nevertheless, the sentence also entails that Peter uttered the
  sentence ``Mary kissed me''. Therefore, in our analysis, the sentence in
  Example~\ref{ex:mary-peter} would be judged equivalent to the sentence in
  Example~\ref{ex:peter-mary}. The meaning of direct quotation does not
  depend only on the meaning of the quoted clause, but on its exact
  form. Since our system is strictly compositional,\footnote{We are using
    abstract categorial grammars for the syntax-semantics interface.} we
  would need to make it so that the meaning of every sentence is the
  sentence itself. More elaborate treatments of quotation can be found
  in~\cite{geurts2003quotation,potts2007dimensions,shan2010character}.

  \begin{exe}
    \ex Peter said ``Mary kissed me''. \label{ex:mary-peter}
    \ex Peter said ``I was kissed by Mary''. \label{ex:peter-mary}
  \end{exe}

  
\item Dynamics

  Our treatment of dynamics was presented as a reengineering of DRT and
  hence it covers DRT as it is presented in Chapter~1
  of~\cite{kamp1993discourse}. It also originated as a reconstruction of de
  Groote's Type Theoretic Dynamic Logic
  (TTDL)~\cite{de2006towards,lebedeva2012expression} using monads of
  effects and handlers~\cite{marsik2014algebraic}. Therefore, it also
  covers the fragment of TTDL presented in~\cite{de2006towards}
  and~\cite{lebedeva2012expression}.

  Giorgolo and Unger have used the state monad to model DRT-style
  dynamics~\cite{giorgolo2009coreference} and the technique was later
  refined by Unger~\cite{unger2012dynamic}. The most challenging aspect in
  both solutions ends up being the management of accessibility with respect
  to the scope of quantifiers or logical operators such as negation. In the
  former~\cite{giorgolo2009coreference}, the state is a tree data structure
  with a pointer and the denotations manipulate it by appending children to
  the tree and moving the pointer up the tree. In the
  latter~\cite{unger2012dynamic}, the state is a stack of contexts and
  denotations have to allocate new contexts on the stack and clear them
  from the stack when the contexts should no longer be accessible. This
  pattern of encapsulating the dynamic effects of some part of a sentence
  lends itself very well to the handler abstraction. In our treatment of
  dynamics, we use an effect handler ($\BOX$) and avoid the overt
  manipulation of any tree or stack of contexts; our contexts are just sets
  discourse referents propositions.


\item Presuppositions

  In her thesis~\cite{lebedeva2012expression}, Lebedeva gave a
  compositional account of presupposition projection and accommodation by
  introducing exceptions and handlers into TTDL.\@ Our use of effects and
  handlers was strongly influenced by Lebedeva's use of exceptions and so
  our analysis of presuppositions follows the same strategy. However, we
  argue that the resumable nature of effects makes them more appropriate
  for the treatment of presuppositions. We have given a detailed comparison
  of our approach and Lebedeva's original
  in~\ref{ssec:comparison-ttdl}, which we summarize here:

  \begin{itemize}
  \item We preserve strong normalization, even though we have exceptions
    and recursion (in handlers).
  \item We do not licence cataphoric binding from presupposition triggers
    because we can resume on presupposition failure without reevaluating
    the previous parts of discourse.
  \item We reuse the same mechanism we have used to treat dynamicity,
    effects and handlers (i.e.\ the same formal apparatus that projects
    context updates outside of sentence boundaries is used to project
    presuppositions outside of downward entailing contexts).
  \end{itemize}
\end{itemize}


\subsection{Combining Linguistic Effects}
\label{ssec:comparison-combining}

Our work is not the only work aimed at combining different linguistic
effects in a single grammar. During our research, several proposals have
appeared. Below, we relate them to our approach.

\begin{itemize}
 
\item Monad
  Transformers~\cite{charlow2014semantics,barker2015monads}

  Monad transformers map simpler monads into more complex monads with some
  additional structure. If we want to have a monad with enough structure
  for, e.g., state, non-determinism and exceptions, we can take the
  corresponding monad transformers and apply them, one after another, to
  some base monad. Monad transformers were evoked as a possible solution to
  the problem of composing monads by Shan in his first paper on monads for
  natural language semantics~\cite{shan2002monads}. Since then, they have
  been used by Charlow in his dissertation~\cite{charlow2014semantics} and
  featured in Barker and Bumford's ESSLLI 2015
  course~\cite{barker2015monads}.

  To motivate one of the reasons one might prefer effects and handlers
  instead of monad transformers, we will look at an example lexical entry
  from~\cite{charlow2014semantics}:

  $$
  \textsf{\textbf{pro}} \coloneqq \lam{s}{\{ \left< s_\top, s \right> \}}
  $$

  The monads in use here are the state monad to treat discourse updates and
  the set monad to treat indefinites. Meanings in the composite monad can
  depend on some discourse state, they can modify the discourse state and
  they can propose a set of possible readings. The meaning of pronouns,
  $\textsf{\textbf{pro}}$, relies on one of these three capabilities. It
  makes use of the discourse state $s$, in which it looks for the salient
  (``topical'') referent, $s_\top$. Then, to have the correct type and fit
  in with the other definitions, it must also supply the output state and
  the set of possible readings. The output state will be just the input
  state $s$ and the set of possible readings will be the singleton set
  $\{\left< s_\top, s \right>\}$. This is the case even though the pronoun
  has no non-deterministic effect, i.e.\ its structure in the set monad is
  trivial. We contrast this with the entry for pronouns that we use in
  $\calc$:

  $$
  \sem{\abs{she}} = \app{\op{get}}{\star}{(\lam{e}{\etaE{(\ap{\selshe}{e})}})}
  $$

  We use the $\op{get}$ operation to access the discourse state $e$, which
  corresponds to the $\lambda s$ abstraction in the monad transformer
  example. Then we use the $\selshe$ function to retrieve the salient
  referent from the state of discourse $e$, which corresponds to the use of
  the $\__\top$ operator. Finally, we return this referent as the result
  using $\eta$, which corresponds to packing the referent in a pair with
  the output state $s$ and then wrapping it in a singleton set. Note that
  no other effect (such as $\op{introduce}$, which would be the analogue to
  the set monad structure) is mentioned within the lexical entry. If we
  were to extend the grammar with new effects, the interpretation of
  $\sem{\abs{she}}$ would stay the same. On the other hand, if we were to
  add a new monad to the stack of monad transformers in the former example,
  then the semantics of $\textsf{\textbf{pro}}$ would need to be lifted.

  There is a way to avoid lifting when working with monad transformers. We
  characterize every monad transformer by some capabilities/operations it
  gives us and then we write abstract polymorphic terms which can be
  interpreted in different monads provided that they have enough structure
  to interpret the capability/operation. This is the method presented
  in\cite{liang1995monad,jones1995functional} and used in (Haskell)
  libraries implementing monad transformers~\cite{mtl}. However,
  formalizing this method already leads us half of the way towards effect
  and handlers (we write computations using abstract operations and the
  type of the computation indicates the operations that must be
  interpreted).


\item Setting the Monads
  Loose~\cite{charlow2015monads,giorgolo2015natural,charlow2015monadic,charlow2015conventional,charlow2015grammar}

  During his invited lecture at Barker and Bumford's ESSLLI 2015
  course~\cite{barker2015monads,charlow2015monads}, Simon Charlow presented
  another strategy for combining the different monads. It consists of
  adding the $\eta$ and the $\hsbind$ of every monad to the lexicon as type
  shifters. When given a sentence, the grammar then assigns a reading for
  every possible way to glue the meanings of the parts of the sentence in a
  type-sound way. There is therefore no need to define any supermonad to
  serve as the universal glue since the task of gluing together the pieces
  from different monads is up to the parser. The same strategy was also
  adopted by Giorgolo and Asudeh in their ESSLLI 2015
  course~\cite{giorgolo2015natural}. They presented fragments of
  type-logical grammars with different modalities $\diamond_i$ for
  different monads. The logic itself contained inference rules which
  correspond to the monadic primitives (e.g.\ $\eta$ is the rule which lets
  us deduce $\diamond A$ from $A$). Giorgolo and Asudeh further strengthen
  the glue available to the logic by adding distributivity laws: rules that
  can commute certain monadic structures (e.g.\
  $\diamond_i \diamond_j A \to \diamond_j \diamond_i A$).

  In our thesis, instead of going this road, we build a composite (free)
  monad. We do so because of two reasons. First, our investigations are
  motivated by the search for a wide-coverage abstract categorial grammar
  (ACG) with formal semantics. In ACGs, derivations are expressed in an
  abstract grammar, which intuitively corresponds to a level of deep
  syntax. Using a similar approach would force us to introduce semantic
  types into the level of abstract syntax. Furthermore, the logic which
  licences derivations would need to be extended to include monadic types
  (i.e.\ we would need to add modalities to the implicative fragment of
  linear logic used in ACGs), which means we would risk losing the existing
  ACG meta-theory, such as parsing results. Second, the point of using
  $\calc$ is to deal with ``non-compositional'' phenomena in a
  compositional setting. If we rearrange the deep syntax so as to
  facilitate composition, then we could be considered
  cheating.\footnote{Though when it comes to ``non-compositional''
    phenomena, cheating compositionality might actually be the
    methodologically sound approach.}

\item Conservativity via Logical
  Relations~\cite{degroote2015conservativity}

  In~\cite{degroote2015conservativity}, de Groote proves a general
  conservativity result that guarantees the preservation of predicted
  meanings under a class of extensions/embeddings, including those that a
  semanticist might use when introducing a new phenomenon to a
  grammar. While this gives us a way of porting an existing grammar into a
  new framework with different types of interpretation, it does not tell us
  how to add new entries to the ported grammar. As an example, we will take
  the dynamic grammar from~\cite{charlow2014semantics}, which we featured
  in the monad transformer example with the following example:

  $$
  \textsf{\textbf{pro}} \coloneqq \lam{s}{\{ \left< s_\top, s \right> \}}
  $$

  Suppose we want to introduce conventional implicatures (CI) to this
  grammar and we do so by embedding the interpretations into a domain in
  which at-issue meanings are paired with CI contributions. If our
  embedding function satisfies the preconditions
  in~\cite{degroote2015conservativity}, we will have preserved all of the
  existing meanings. Now, we want to add non-restrictive relative clauses
  (NRRCs), which contribute CIs. However, we have to give their meaning in
  a domain with states, sets and conventional implicatures. Below is the
  de-sugared semantics that Charlow gives to the NRRC
  construction~\cite{charlow2015conventional}:

  $$
  \ap{\textsf{\textbf{comma}}}{\textsf{k}} =
  \lam{x}{\lam{s}{\{ \left< x \bullet p, s' \right> \mid \left< p, s' \right> \in \app{\textsf{k}}{x}{s} \}}}
  $$

  The salient part of the entry is that the referent of the appositive is
  $x$ and the contributed CI is $p$, the meaning of the relative clause
  $\textsf{k}$ applied to $x$. However, we also need to handle the state
  and so we abstract over $s$, pass it to $\ap{\textsf{k}}{x}$ to get an
  output state $s'$ and then bundle the output state $s'$ with our result
  $x \bullet p$. Furthermore, there is the set structure, and so we need to
  take into account the fact that $\app{\textsf{k}}{x}{s}$ gives us a set
  with multiple pairs $\left< p, s' \right>$ and we need to give our
  meaning for all of them. The structure of this entry on the set monad
  level and state monad level is trivial since (under this analysis) NRRCs
  have no interesting interaction with discourse state or
  indefiniteness. As the number of phenomena in the grammar grows, we would
  like a way to abstract over the non-relevant parts of the meaning
  structure. This is something that can be seen in the sugared version of
  the semantics that Charlow gives to this
  operator~\cite{degroote2015conservativity}:

  $$
  \ap{\textsf{\textbf{comma}}}{\textsf{k}} = \lam{x}{\textsf{do }
    \begin{aligned}[t]
      & p \from \ap{\textsf{k}}{x} \\
      & \textsf{return: } x \bullet p
    \end{aligned}}
  $$

  as well as in our lexical entry for the same construction:\footnote{The
    entry we see here is simplified from the one
    in~\ref{sec:conventional-implicature} to match that of Charlow. This is
    because in ACGs, we must specify the meaning of the phrase \emph{$X$,
      who $K$,} in terms of the meanings of $X$ and $K$, where $X$ is a
    noun phrase with potentially complex monadic structure and $K$ is a
    sentence missing such a noun phrase (i.e.\ a relative
    clause). Therefore, in the entry in~\ref{sec:conventional-implicature},
    we have to evaluate $X : \FF_E(\iota)$ down to $x : \iota$ and when
    applying $K$ to $x$, we have to wrap $x$ in $\eta$. In Charlow's
    approach, it is not the duty of the lexical entries to manage
    evaluation. Rather, it is the grammar which includes the necessary
    combinators and type shifters which will evaluate $X$ for us.}

  $$
  \sem{\ap{\abs{who}_{\abs{s}}}{\textsf{k}}} = \lam{x}{\begin{aligned}[t]
      &\ap{\textsf{k}}{x} \hsbind (\lam{p}{ \\
      &\app{\op{implicate}}{p}{(\lam{\_}{ \\
      &\etaE{x}})}})
    \end{aligned}}
  $$

  While the conservativity result of de Groote lets us be confident when
  porting a grammar to a different type of interpretation, it does not help
  us when extending it with new entries. It is this problem that is
  addressed by the techniques presented here and in the works of Simon
  Charlow. Specifically, in our setting, we do not view the different
  phenomena in a language as forming a hierarchy in which one extends the
  other.\footnote{There are exceptions, such as our treatment of
    presuppositions, which extends our treatment of dynamics.} Instead, it
  is the case that any lexical entry can use any subset of the phenomena it
  needs. For example, \emph{she} is anaphoric but not quantificational,
  \emph{every man} is quantificational but not anaphoric, \emph{you} is
  neither anaphoric nor quantificational but deictic, \emph{your husband}
  is both deictic and presuppositional but not quantificational\ldots
 

\item Applicative Abstract Categorial
  Grammars~\cite{kiselyov2015applicative,kiselyov2015swing}

  In Subsection~\ref{ssec:applicative-functor}, we spoke about applicative
  functors. Applicative functors generalize monads since every monad is
  also an applicative functor but not vice versa. The structure of an
  applicative gives us two combinators, $\pure : \alpha \to F \alpha$ and
  $\circledast : F (\alpha \to \beta) \to F \alpha \to F \beta$. The
  intuition behind applicative functors vs monads is that applicatives
  embody computations with static control flow whereas monads embody
  computations with dynamic control flow~\cite{lindley2014algebraic}.

  \begin{align*}
    \circledast &: F (\alpha \to \beta) \to F \alpha \to F \beta \\
    \revhsbind\footnotemark &: (\alpha \to F \beta) \to F \alpha \to F \beta
  \end{align*}

  \footnotetext{$\revhsbind$ is $\hsbind$ with its arguments reversed.}

  When combining computations using $\hsbind$, as in the case of a monad,
  we can take a computation $F \alpha$ and chain it with a continuation of
  type $\alpha \to F \beta$, where the computational structure (i.e.\ the
  effects) can vary depending on the value of type $\alpha$. However, when
  combining computations using $\circledast$, as in the case of an
  applicative, we can only take two computations whose computational
  structure is already fixed. 

  Kiselyov argues that since sentence structure is itself also static,
  applicative functors are a good fit~\cite{kiselyov2015applicative}. In
  Section~\ref{sec:lifting-semantics}, we have seen that we can lift the
  semantics of a second-order abstract categorial grammar (ACG) into
  computations using only $\eta$ and $\aplr$, which constitute the
  applicative structure of $\calc$. Second-order ACGs are relevant not only
  because they can be efficiently parsed~\cite{kanazawa2007parsing}, but
  also because they are sufficient to encode mildly context-sensitive
  grammar formalisms such as tree-adjoining grammars~\cite{degroote02}. If
  second-order ACGs are sufficient to model language, then applicative
  functors could be sufficient to model their meanings. The switch to
  actually using applicative functors can then be motivated on the basis of
  the fact that applicatives are composable, unlike
  monads.\footnote{However, the applicative structure that one gets by
    composing two monad transformers is not necessarily the same as the one
    that is obtained by composing the corresponding applicative functors.}

  Kiselyov's applicative abstract categorial grammars (AACGs) exploit
  this. In a second-order ACG, terms of the abstract language are formed
  only by constants and applications. In AACGs, the abstract and object
  languages are defined to be so-called \emph{T-languages}, which admit
  only constants and a binary application operator. Terms of an abstract
  T-language are then homomorphically mapped to $\lambda$-terms that
  produce terms in an object T-language. As in all ACGs, the function type
  of the abstract language corresponds to functions. Furthermore, as in our
  approach, atomic abstract types are mapped to types of the form
  $\FF(\alpha)$, where $\FF$ is some applicative functor and $\alpha$ some
  object-level type.

  The biggest difference between AACGs and our approach is whether we use a
  composition of applicative functors or a free monad of effects and
  handlers. The use of applicative functors relies on the assumption that
  the computational structure (i.e.\ the control flow) in the semantics is
  always static. However, even though the structure of sentences is static,
  we find that the control flow in the semantics can be dynamic. For
  example, if we consider the noun phrase \emph{her car}, then the pronoun
  is a computation of an individual, type $\FF(\iota)$, and the genitive
  construction is a function over individuals that triggers
  presuppositions, type $\iota \to \FF_E(\iota)$. To find the referent of
  this noun phrase, we first have to evaluate the pronoun down to an
  individual $x$ and then apply the meaning of the genitive construction to
  this individual, triggering the presupposition that $x$ owns a car. We
  cannot evaluate the effects of the genitive construction independently of
  the pronoun since we would not know whose car we are presupposing the
  existence of and therefore we cannot combine the two meanings using only
  the $\circledast$ combinator we have in an applicative functor.

\end{itemize}

Note that all of the work on combining linguistic effects that was cited in
this subsection follows the publication of our initial
report~\cite{marsik2014algebraic} from when we were at about a third of the
way into working on this dissertation. That is to say that the motivation
behind this dissertation was not to challenge the work cited in this
subsection, with which it mostly agrees in saying that effects, monads or
some other monad-like structure can be used to combine semantic analyses of
linguistic phenomena. Nevertheless, we believe that the use of effects and
handlers might be interesting to those considering using monad transformers
to build a formal semantics. This is especially the case if one is
considering to do so in the context of an ACG and wants to therefore use a
formal lambda calculus for the semantic calculations. Out of the
alternatives mentioned in this subsection, we found the approach of setting
monads loose and letting the grammar figure out the semantic glue
particularly intriguing as it tries to sidestep the issue of composing the
monads.


\section{Future Work}
\label{sec:future-work}

Future work on $\calc$ could be either focused on the calculus itself or
its linguistic applications. We finish our thesis with a discussion of
both.


\subsection{Future Work on the Calculus}
\label{ssec:future-work-calculus}

\begin{itemize}
\item Adequacy of Denotational Semantics, Observational Equivalence

  In~\ref{ssec:denotational-semantics}, we have given a denotational
  semantics to $\calc$. Even though we first gave the reduction semantics,
  we perceive the denotational semantics as the primary semantics as it
  assigns to the terms of $\calc$ the mathematical objects that we want
  them to stand for. The reduction semantics can then be seen as a
  mechanization of computing with these objects. In
  Chapter~\ref{chap:properties}, we have proven
  Property~\ref{prop:denotation-soundness}, which tells us that our
  reduction semantics is sound w.r.t.\ the denotations. However, the
  converse is not the case (i.e.\ there are terms which the denotational
  semantics considers equal but which are not convertible using our
  reduction rules).

  The denotational semantics is useful for reasoning about $\calc$
  programs. Strengthening the formal link between the denotational
  semantics and the reductions semantics would therefore be most
  welcome. The kind of property we are looking for is known as
  \emph{adequacy} and it states that if a program and a value have the same
  denotation, then the program must be able to reduce to that value. For
  this property to hold, it can be necessary to restrict it only to
  programs and values of \emph{ground} types (types which do not ``hide''
  any more computation, such as functions or continuations). An example of
  such a ground type in $\calc$ would be the Boolean type $2$. Proving
  adequacy for terms of type $2$ would then yield a result about
  \emph{observational equivalence}: if two terms $M$ and $N$ have the same
  denotation, then they are observationally equivalent, i.e.\ there is no
  way to write a Boolean expression in $\calc$ whose value differs when
  switching $M$ for $N$.


\item Commuting Conversions and Extensionality

  During the reduction of one of the examples
  in~\ref{ssec:ambiguous-accommodation}, to simplify the presentation, we
  have made use of a reduction rule for passing $\hsbind$ (a handler) under
  case analysis. The formal rule is given below:
  
  \begin{align*}
      &\ap{\cibanana}{(\case{N_\petitc}{x}{N_\petitl(x)}{y}{N_\petitr(y)})} \\
\to \ &\case{N_\petitc}{x}{\ap{\cibanana}{N_\petitl(x)}}{y}{\ap{\cibanana}{N_\petitr(y)}}
  \end{align*}

  We note that both the redex and the contractum have the same
  denotation. On the other hand, if all the $M$ and $N$ terms are normal,
  then so are both the redex and contractum. Since they are distinct normal
  terms, this means that the redex and the contractum are not convertible
  in $\calc$. In other words, this rule is not derivable in our system
  (even though it is licensed by the denotational semantics).

  For completeness' sake, we might like to have rules such as these in
  $\calc$. They further close the gap between the denotational semantics
  and the reduction semantics and they can be useful when working with the
  calculus, as we have seen in the example
  in~\ref{ssec:ambiguous-accommodation}. Instead of adding a multitude of
  \emph{commuting conversions} such as the one for handlers and sums given
  above, we can also add \emph{extensionality} principles, such as the
  following $\eta$-reduction for sums:

  \begin{align*}
      &\case{M}{x_1}{\subst{N}{x}{\ap{\inl}{x_1}}}{x_2}{\subst{N}{x}{\ap{\inr}{x_2}}} \\
\to \ &\subst{N}{x}{M}
  \end{align*}

  The commuting conversions that commute over sums can then be derived from
  this principle~\cite{balat2004extensional}. It might also be interesting
  to seek what would such an extensionality principle would look like for
  computation types. However, the biggest obstacle is that enriching the
  calculus with new reduction rules puts in jeopardy the established
  meta-theory (most importantly strong normalization, whose proofs are
  usually brittle).


\item Conservativity

  In the summary of the last chapter, Subsection~\ref{ssec:conservativity},
  we have claimed that modifying a grammar by inserting handlers for
  effects which are not used anywhere in the grammar has no effect on the
  semantics. Since this claim plays an important part in motivating the use
  of $\calc$, it would be desirable to formalize and prove
  it.\footnote{This might be one of the useful applications of adequacy of
    our denotational semantics. We can prove that
    $\ap{\banana{\onto{\op{new\_op}}{M}}}{N}$ has the same denotation as
    $N$, provided that $\op{new\_op}$ does not occur in the denotation of
    $N$. By using adequacy, we could translate the equality of denotations
    into a reduction in the reduction semantics.}
  

\item Algebraic Theories of Effects

  Within the subsections titled ``Algebraic Considerations'' in
  Chapters~\ref{chap:introducing-effects} and~\ref{chap:dynamic-semantics},
  we studied the equational theories on computations induced by handlers
  (two computations are considered equal if the handler assigns them the
  same interpretation). Within these chapters, we looked for general
  equalities that might become useful and we used those equalities to
  derive a kind of normal form for the computations. However, the
  normalization assumes that computations are only sequences of operations
  terminated by a return value, i.e.\ they are generated by the grammar
  below:\footnote{This grammar only allows for computations which always
    execute the same sequence of operations, irrespective of any of the
    their outputs. In Lindley's $\lambda_{\textrm{flow}}$
    calculus~\cite{lindley2014algebraic}, such computations would be called
    \emph{arrow} computations~\cite{hughes2000generalising}.}

  $$
    S ::= \app{\op{op}}{M}{(\lam{x}{S})} \mid \etaE{M}
  $$

  Is it possible to derive similar normal forms for computations built
  using a larger grammar (e.g.\ the full $\calc$ language)?

  In the examples of Chapter~\ref{chap:dynamic-semantics}, we have been
  making use of these equalities to simplify terms before passing them to a
  handler. If this practice becomes common, it might be interesting to
  formalize it.

  A good start for both of these problems would be a more thorough
  investigation of Matija Pretnar's dissertation on the logic and handling
  of algebraic effects~\cite{pretnar2010logic}, which introduces a calculus
  of effects, effect theories and handlers.

  
\end{itemize}


\subsection{Future Work on the Linguistic Applications}
\label{ssec:future-work-linguistic}

The plan for future work on the linguistic applications is simple: get more
phenomena, more detail, more interactions.

\begin{itemize}
\item Low-Hanging Fruit

  Intensional meanings are parameterized by some possible world. The kind
  of structure introduced in~\cite{ben2007semantics} and~\cite{de2013note}
  is yet another instance of the reader monad pattern. As with deixis,
  meanings will use a $\typedop{world}{1}{\sigma}$ operation to access the
  current world. Modal operators, which will quantify over possible worlds,
  will then interpret a piece of language while binding the current world
  to some variable (very much like the way we treated direct quotation,
  which was binding the speaker to the quotee).

  There are also phenomena for which there are already analyses in terms of
  monads or (delimited) continuations. An example of this is focus, which
  can be modelled using the pointed powerset monad~\cite{shan2002monads} or
  using delimited control operators~\cite{barker2006continuations}. While
  such phenomena ought not to be difficult to implement in $\calc$, it
  might still take some care and empirical work to ensure that the
  predictions in cases that involve the new phenomenon and the old
  phenomena turn out the way they should.
  
  
\item Better Crossover

  Approaches to semantics that exploit the analogy to computations and
  treat anaphora and scope-taking as side effects run into trouble when it
  comes to inverse scope readings and the crossover constraints. In
  Subsection~\ref{ssec:crossover}, we proposed a solution to the problem
  which decomposes the effect of a quantificational NP into the
  scope-taking, which can happen out of linear order, and the anaphora,
  which must happen in linear order. However, there are still cases in
  which our grammar can violate the crossover constraints, such as when the
  contributor of a discourse referent (e.g.\ the indefinite \emph{a dog})
  hitches a ride in the restrictor of a scope taker (e.g.\ \emph{every
    owner of a dog}), as in Example~\ref{ex:bad-crossover}.

  \begin{exe}
  \exr{ex:bad-crossover}[*]{It$_1$ loves every owner of a dog$_1$.}
  \end{exe}

  Crossover issues with inverse scope readings are an old
  problem~\cite{shan2006explaining} and therefore a solution which improves
  coverage without incurring extra complexity to the rest of the grammar
  would be of great interest.

  
\item More Dynamics --- Double Negation, Modal Subordination and Rhetorical
  Structure

  The treatment of dynamics is easily the most complex application of
  $\calc$. However, anaphora is much more complicated than what we have
  seen in this dissertation. Extending it might thus prove to be a good
  stress test on the level of complexity that is still manageable in
  $\calc$. Directions for extending dynamics include:

  \begin{itemize}
  \item Double negation, which was studied in Qian's
    dissertation~\cite{qian2014accessibility} and which we reviewed in
    Section~\ref{sec:double-negation}. This is very difficult to apply in
    our current implementation of dynamics, since negation uses $\BOX$ to
    encapsulate all of the dynamic effects that take place in its
    argument. However, to have the law of double negation, we would need to
    find a way to break this encapsulation, which is tricky since the
    $\BOX$ handler does encapsulation by throwing away the local structure
    (i.e.\ permanently blocking the dynamic contributions).

  \item Modal subordination, as in Example~\ref{ex:modal-subordination}, is
    another accessibility constraint that was modelled within an extension
    of TTDL in Qian's thesis~\cite{qian2014accessibility}. Furthermore, it
    is a phenomenon that lies at the intersection of two linguistic
    effects, modality and anaphora, which makes it all the more interesting
    for a $\calc$ analysis.

    \begin{exe}
      \ex A wolf$_1$ might walk in. It$_1$ would
      growl. \label{ex:modal-subordination}
    \end{exe}

  \item Rhetorical structure informs anaphoric accessibility via the Right
    Frontier Constraint, as in the famous
    Example~\ref{ex:salmon}~\cite{asher2003logics}.

    \begin{exe}
      \ex[*]{John had a great evening last night. He had a great meal. He
        ate salmon$_1$. He devoured lots of cheese. He then won a dancing
        competition. It$_1$ was pink.} \label{ex:salmon}
    \end{exe}
  \end{itemize}

  It remains to be seen whether the handler abstraction, which turned out
  to be useful to encapsulate the dynamic effects in negation and modelling
  DRT, would be also useful in analyses of accessibility constraints due to
  modal subordination or rhetorical structure.


\item Champollion --- Sentence Meanings as Generalized Quantifiers over Events

  In~\cite{champollion2015interaction}, Lucas Champollion presents a
  semantic fragment in which he combines event semantics with classical
  phenomena of compositional semantics such as quantification or
  negation. His work therefore seems to be an ideal starting point for
  anyone wanting to add events into our $\calc$ grammar. However, the types
  of meanings used within the fragment are quite atypical and therefore it
  would be interesting to see how many of the theories of the other
  phenomena developed in this thesis would be compatible with such a
  semantics.
  
\end{itemize}
