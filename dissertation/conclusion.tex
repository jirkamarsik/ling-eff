\chapter{Conclusion}
\label{chap:conclusion}

\section{Summary of Results}
\label{sec:summary}

In Part~\ref{part:calculus}, we have introduced $\calc$, a formal calculus
that extends the simply-typed lambda calculus (STLC) with effects and
handlers.

The definition of $\calc$ is given in
Chapter~\ref{chap:definitions}. $\calc$ introduces a new family of types
into STLC, the computation types, and new terms, which are built out of
computation constructors and destructors. We gave a type system to the
calculus which extends that of STLC and a reduction semantics which
combines the STLC $\beta$ and $\eta$ reductions with definitions of the new
function symbols. During the course of the chapter, we maintain two
perspectives on the intended meaning of the terms: computations can be seen
as programs that interact with a system through a selected set of ``system
calls'' (operations) or they can be seen as algebraic expressions built
upon an infinitary algebraic signature.

In Chapter~\ref{chap:examples}, we gave an example of using the $\calc$
calculus. Besides familiarizing the reader with the notation and reductions
of the calculus, the example served as a preview of the kind of language
engineering we would be doing. During the chapter, we developed a
compositional semantics for a simple computing language with errors and
variables. This let us demonstrate the modularity of using our computation
monad, as we could add variables to the language without needing to modify
the semantics of the other constructions.

The main contribution of Part~\ref{part:calculus} lies in
Chapter~\ref{chap:properties}, in which we developed the metatheory of
$\calc$. In Section~\ref{sec:derived-rules}, concepts which are primitive
in some other languages (closed handlers and the $\hsbind$ operator) were
defined within $\calc$ and their typing rules and reduction rules were
derived from those of $\calc$. In Section~\ref{sec:algebraic-properties},
we then connected the calculus to the theory of monads by identifying a
monad in the category in which we interpret $\calc$ with our
\emph{denotational semantics}. In Section~\ref{sec:type-soundness}, we
proved \emph{subject reduction} of $\calc$. This result gives a basic
coherence between the type system of $\calc$ and the reduction semantics,
guaranteeing that types are preserved under reduction. This is complemented
by a limited proof of \emph{progress}, which states that terms which can no
longer be reduced must have a very specific shape. We followed this with
another fundamental property: \emph{strong normalization}. Its proof was
split into two parts: \emph{confluence} (proved in
Section~\ref{sec:confluence}) and \emph{termination} (proved in
Section~\ref{sec:termination}). The proofs of both confluence and
termination proceed by similar strategies: prove the property for the
calculus without $\eta$-reduction by applying a general result and then
extend the property to the complete calculus. In the case of confluence,
the general result is the confluence of orthogonal Combinatory Reduction
Systems~\cite{klop1993combinatory}. In the case of termination, we rely on
two techniques: the termination of the reduction relation of Inductive Data
Type Systems that validate the General Schema~\cite{blanqui2000termination}
and Higher-Order Semantic Labelling~\cite{hamana2007higher}, which lets us
use our denotational semantics to label the terms of our calculus so that
it validates the General Schema.

Andrej Bauer made the analogy that effects and handlers are to delimited
continuations what while loops or if-then-else statements are to
gotos~\cite{bauer2012lambda}. Continuations themselves have proven to be a
useful tool in natural language
semantics~\cite{de2001type,barker2002continuations,shan2005linguistic,de2006towards,barker2006continuations,barker2014continuations}. In
Chapter~\ref{chap:continuations}, we have shown how $\calc$ can simulate
delimited continuations, namely the $\shift$/$\reset$ delimited control
operators. We presented a typed call-by-value lambda calculus with $\shift$
and $\reset$ and have simulated its types and reductions in $\calc$.

In Part\ref{part:natural-language}, we have demonstrated the applications
of $\calc$ to the problem of modelling the meaning of natural language
utterances.

After having reviewed the basics of formal semantics in
Chapter~\ref{chap:intro-fs}, we have shown how computations in $\calc$ can
be used to give a compositional account of several ``non-compositional''
linguistic phenomena in Chapter~\ref{chap:introducing-effects}. We have
described how to introduce computations into a compositional semantics
while preserving the meanings assigned by the semantics in
Section~\ref{sec:lifting-semantics}. We have then presented analyses of
several linguistic phenomena in the framework of $\calc$ computations:
deixis (Section~\ref{sec:deixis}), conventional implicature à la Potts
(Section~\ref{sec:conventional-implicature}) and quantification à la
Montague (Section~\ref{sec:quantification}). We have then explicitly
described the methodology given to find the kinds of analyses that we have
presented in Chapter~\ref{chap:introducing-effects} as to encourage
researchers to develop other analyses in the framework.

We dedicated Chapter~\ref{chap:dynamic-semantics} to a particularly complex
phenomenon: dynamics. In Sections~\ref{sec:drt-as-pl}
and~\ref{sec:banana-drt}, we have shown how a $\calc$ analysis of dynamics
can be extracted from Discourse Representation Theory. This gave us a way
to handle dynamics in $\calc$ as well as strengthen the claim that effects
and handlers are suitable mechanisms for dealing with natural language. We
have also shown how to interpret the $\calc$ computations as the dynamic
propositions of de Groote's Type Theoretical Dynamic Logic
(TTDL)~\cite{de2006towards}. In her
dissertation~\cite{lebedeva2012expression}, Lebedeva extended TTDL with
exceptions to treat presuppositions and in
Section~\ref{sec:presuppositions}, we integrated Lebedeva's analysis of
presupposition into our $\calc$ analysis of dynamics (we compare our
adaptation with the original in the next section,
\ref{ssec:comparison-linguistic}). In Section~\ref{sec:double-negation}, we
considered another extension of TTDL, Double Negation
TTDL~\cite{qian2014accessibility} and we have shown why the kind of
generalization of denotations done in Double Negation TTDL is not amenable
to being analyzed as a side effect in $\calc$.

In Chapter~\ref{chap:composing-effects}, we have supported our claim that
using effects and handlers lets us combine distinct phenomena in a single
grammar. We have started with the dynamic grammar developed in
Chapter~\ref{chap:dynamic-semantics}, repeated in
Sections~\ref{sec:dynamic-kernel} and~\ref{sec:adding-presuppositions}. We
have then extended this grammar with conventional
implicatures~(\ref{sec:adding-conventional-implicature}),
deixis~(\ref{sec:adding-deixis}) and
quantification~(\ref{sec:adding-quantification}) with little to no
modification of the original semantics. We finished the chapter by
sketching out an analysis of restrictive relative clauses and their
interactions with presuppositions in Section~\ref{sec:relative-clauses}.


\section{Comparison with Existing Work}
\label{sec:comparison}

\subsection{Calculus}
\label{ssec:comparison-calculus}

$\calc$ can be compared to several existing calculi and implementations of
effects and handlers:

\begin{itemize}
\item System F (i.e.\ the polymorphic lambda calculus or the second-order
  lambda calculus)

  $\calc$ extends the simply-typed lambda calculus with computation types
  $\FF_E(\alpha)$. Computations are algebraic expressions and as such can
  be expressed as inductive data types.\footnote{An inductive type is a
    recursive type with positive
    constructors. In~\ref{ssec:termination-for-idts}, we have seen that a
    computation type $\FF_E(\alpha)$ has positive constructors $\eta$ and
    $\op{op}$ for every $\op{op} \in E$.} Inductive data types, along with
  the sums and products that we add to the calculus in
  Section~\ref{sec:sums-and-products}, can be expressed in System
  F~\cite{wadler1990recursive}.

  In $\calc$, a computation of type $\FF_E(\alpha)$ can also be given the
  type $\FF_{E \uplus E'}(\alpha)$, where $E \uplus E'$ is an extension of
  $E$. However, in the direct encoding of $\calc$ into System F, for every
  effect signature $E \uplus E'$ that we would like to ascribe to a
  computation, we would end up with a different term. On the other hand, in
  $\calc$ we can keep using the same term. This lets us give a semantics to
  lexical items that does not have to change when new effects are
  introduced into the theory.

\item \emph{Eff}

  The \emph{Eff} language~\cite{bauer2012programming} is an ML-like
  programming language with effects and handlers. Like in ML, effects can
  be freely used within any expression, without any term encoding (we say
  that the calculus is \emph{direct-style}). For this to work correctly,
  the calculus has a fixed evaluation order, which, following ML, is
  call-by-value.

  We have used \emph{Eff} in our first explorations of effects and handlers
  in natural language semantics~\cite{marsik2014algebraic}, benefiting from
  the existing implementation. However, we have found that besides
  call-by-value, call-by-name evaluation is also common, notably on the
  boundaries of lexical items (see~\ref{ssec:cbn-and-cbv}). Call-by-name
  can be simulated in call-by-value by passing around thunks (functions of
  type $1 \to \alpha$ for some $\alpha$). However, in the presence of both
  call-by-name and call-by-value, we have opted for an indirect
  presentation of effects using monads which favors neither call-by-value
  nor call-by-name and that lets us manipulate the order of execution using
  $\hsbind$.

  Finally, we note that \emph{Eff} is a general-purpose programming
  language which includes general recursion (\texttt{let rec}) and
  therefore it is not terminating, contrary to $\calc$.
  
\item $\lambda_{\mathrm{eff}}$

  The $\lambda_{\mathrm{eff}}$ calculus~\cite{kammar2013handlers} is a
  call-by-push-value lambda calculus~\cite{levy1999call} with operations
  and handlers. Call-by-push-value is special in introducing two kinds of
  terms: computations and values. The intuition behind the two is that
  computations \emph{do}, whereas values \emph{are}. Two of the crucial
  things that computations do are to pop values from a stack (that is what
  abstractions do) and to push values to the stack (that is what
  applications do). Therefore, applications and abstractions are considered
  as computations. Furthermore, the function in an application term must be
  a computation term (which is expected to, among other things, pop a value
  from the stack), whereas the argument, which is the value to be pushed to
  the stack, must be a value term.

  This might make it look like that call-by-push-value is like
  call-by-value since all the arguments passed to functions are
  values. However, in true call-by-value, we can use complex expressions as
  arguments and we expect that the reduction system will evaluate the
  arguments down to values before passing them to the function. To do this
  in call-by-push-value, we have to implement this manually by evaluating
  the argument computation down to a value $x$ and then passing the value
  $x$ to the function in question (i.e.\ in $\lambda_{\mathrm{eff}}$ syntax
  $\textbf{let } x \from M \textbf{ in } \ap{F}{x}$). In $\calc$, this
  amounts to the term $M \hsbind F$, where $M : \FF_E(\alpha)$ and
  $F : \alpha \to \FF_E(\beta)$. To implement call-by-name, computations
  can be mapped to values by wrapping them in thunks, which are primitive
  constructs in call-by-push-value (in $\lambda_{\mathrm{eff}}$ syntax
  $\ap{F}{\{M\}}$, where $M$ is a computation and the thunk $\{M\}$ is a
  value). In $\calc$, the corresponding term is $\ap{F}{M}$, where
  $M : \FF_E(\alpha)$ and $F : \FF_E(\alpha) \to \FF_E(\beta)$.

  $\lambda_{\mathrm{eff}}$ presents an intriguing alternative to
  $\calc$. The call-by-push-value calculus is flexible enough to
  accommodate both call-by-name and call-by-value. $\lambda$ abstractions
  and operations are both treated as effects, which might make the
  definition of the $\CC$ operator, which permutes $\lambda$ with
  operations, more intuitive.\footnote{The extra typing rule for the $\CC$
    construction in $\lambda_{\mathrm{eff}}$ would look like this:
    \begin{prooftree}
      \AxiomC{$\Gamma \vdash_E M : A \to C$}
      \UnaryInfC{$\Gamma \vdash_E \ap{\CC}{M} : F (U_\emptyset (A \to C))$}
    \end{prooftree}} $\lambda_{\mathrm{eff}}$ also has a well-developed
  metatheory, developed in~\cite{kammar2013handlers}: it is both confluent
  (due to its reduction relation being deterministic) and terminating
  (thanks to its effect type system).

  $\lambda_{\mathrm{eff}}$ served as an inspiration to the design of
  $\calc$; notably, $\calc$'s effect system is based on that of
  $\lambda_{\mathrm{eff}}$. However, $\calc$ diverges from
  $\lambda_{\mathrm{eff}}$ in that it is a proper extension of the
  simply-typed lambda calculus (STLC): every term, type, typing judgment or
  reduction in STLC is also a term, type, typing judgment or reduction in
  $\calc$. For example, the STLC term $\lam{x}{x}$ is not a
  $\lambda_{\mathrm{eff}}$ term. Its closest counterparts in
  $\lambda_{\mathrm{eff}}$ would be either
  $\vdash_\emptyset \lam{x}{\textbf{return } x} : A \to F(A)$, where $A$ is
  a value type, or $\vdash_E \lam{x}{x!} : U_E(C) \to C$, where $C$ is a
  computation type (a function or an effectful computation). On the other
  hand, in $\calc$, $\vdash \lam{x}{x} : \alpha \to \alpha$ is a valid term
  for any $\alpha$, be it an atomic type such as $o$, a function type such
  as $\iota \to o$ or a computation type such as $\FF_E(o)$.

  The fact that $\calc$ is an extension of STLC motivates its use for two
  reasons. First, STLC is the lingua franca of formal semantics. $\calc$
  already introduces a lot of new notation and the use of effects in
  natural language semantics is not yet ubiquitous. By basing $\calc$ on
  STLC, we narrow the gap between the common practice of formal semantics
  and our use of effects and monads, hopefully making the technique more
  approachable to researchers in the field. Second, the purpose of the
  calculus is to write down computations that produce logical
  representations. By having STLC as a subpart of $\calc$, terms of
  Church's simple type theory (i.e.\ formulas of higher-order logic) are
  already included in our calculus and we can reuse the same notions of
  $\lambda$-abstraction and variables. In $\lambda_{\mathrm{eff}}$, we
  would either need to add constructors for logic formulas (i.e.\ having
  some logic as an object language over the terms of which the meta
  language $\lambda_{\mathrm{eff}}$ would calculate) or use
  call-by-push-value computations in our logical representations.

\item Extensible Effects of Kiselyov et al~\cite{kiselyov2013extensible}
  and other implementations of effect systems in pure functional
  programming languages (Haskell, Idris \ldots)

  Our adoption of a free monad and effect handlers was motivated by the
  paper of Kiselyov, Sabry and Swords on \emph{extensible
    effects}~\cite{kiselyov2013extensible}. The paper presented a Haskell
  library for encoding effectful computations, combining computations with
  diverse effects and interpreting them by composing a series of modular
  interpreters. The library used a free monad (in the style
  of~\cite{swierstra2008data}): a computation is either a pure value
  ($\eta$ in $\calc$) or a request to perform some kind of effect (an
  operation in $\calc$). These requests are then handled by interpreters
  which behave similarly to effect handlers (the authors
  of~\cite{kiselyov2013extensible} also relate handlers to the technique of
  ``extensible denotational language specifications'' published in 1994 by
  Cartwright and Felleisen~\cite{cartwright1994extensible}). The paper
  demonstrated that the approach is more flexible when it comes to
  combining interacting effects than the existing state-of-the-art
  technique of using monad transformers. A more refined version of the
  approach was published in~\cite{kiselyov2015freer} and similar
  implementations of effects and handlers exist also in other pure
  functional programming languages such as
  Idris~\cite{brady2013programming}.

  The extensible effects discipline provides the tools that we would like
  to use to build a modular semantics of natural language. However, we do
  not want our formal semantics to depend on the semantics of a large
  programming language such as Haskell\footnote{The implementations of
    extensible effects in Haskell make use of a wealth of language
    extensions which are not even part of the Haskell standard.} or
  Idris. We created $\calc$ to reap the benefits of extensible effects
  without incurring the complexity of using a language like Haskell as our
  meta language. $\calc$ extends STLC with computation types, two
  constructors ($\eta$ and operations), two destructors (handlers and
  $\cherry$) and the $\CC$ operator. Unlike Haskell, our extension of STLC
  preserves strong normalization.
\end{itemize}


\subsubsection{The Case of the $\CC$ Operator}

A notable feature which distinguishes $\calc$ from all of the
above-mentioned calculi is the $\CC$ operator. The $\CC$ operator was added
relatively late to the $\calc$ calculus as a solution to the following
problem.

\begin{exe}
  \ex A man$_1$ walks in the park. He$_1$ whistles. \label{ex:C-operator}
\end{exe}

In Example~\ref{ex:C-operator}, the noun phrase \emph{a man} introduces a
quantifier ranging over men that takes scope over its continuation
$\lam{x}{\sem{x \text{ walks in the park. He whistles.}}} : \iota \to
\FF_E(o)$.\footnote{In our presentation of dynamic semantics, where
  $\sem{S} = \FF_E(1)$, the type of the continuation would be
  $\iota \to \FF_E(1)$.} The problem is how to combine an effectful
predicate of type $\iota \to \FF_E(o)$ with a quantifier such as
$\exists : (\iota \to o) \to o$. The key insight is that the effects of the
continuation usually do not depend on the individual being talked about. No
matter whether the sentence speaks about Albert, Bill or Charles, the
continuation will proceed the same. The continuation will look into the
context to retrieve the antecedent $x$, which is known to satisfy the
predicate $\obj{man}$ and therefore be eligible for use with a masculine
pronoun. It will then produce the logical formula
$\ap{\obj{walk-in-the-park}}{x} \land \ap{\obj{whistle}}{x}$. The
resolution of the anaphoric pronoun does not depend on the particular
individual being discussed.

DRT is capable of deriving a meaning for Example~\ref{ex:C-operator}
without considering which individual the noun phrase \emph{a man} refers
to. DRT does this by calculating with symbolic representations. The noun
phrase \emph{a man} will introduce a \emph{discourse referent}, a symbolic
object distinct from any individual found in the model. Then the evaluation
of the discourse continues and the anaphoric pronoun can resolve to this
symbolic object. We could do the same in $\calc$. We could state that the
$\iota$ type is not the type of individuals in the model, but the type of
some symbolic objects (discourse referents or variables), and that the type
$o$ is not the type of propositions, but the type of logical
formulas. Assuming that we had an operation $\typedop{gensym}{1}{\iota}$
which could give us fresh variables and that the type of the constructor
for formulas of existential quantification would be
$\exists : \iota \to o \to o$, we could wrap the existential quantifier
over an effectful computation $P : \iota \to \FF_E(o)$ the following way:

\begin{align*}
  &\app{\op{gensym}}{\star}{(\lam{x}{ \\
  &\ap{P}{x} \hsbind (\lam{p}{ \\
  &\etaE{(\app{\exists}{x}{p})}})})}
\end{align*}

By changing the meaning of the type $\iota$ to be the type of symbolic
references, we solve our problem. When we need to evaluate the effects of a
continuation of type $\iota \to \FF_E(o)$, we do not need to know the
precise identity of the individual $\iota$. Instead, we apply the
continuation to a symbolic object which will stand in for any such
individual and then proceed to evaluate the effects of the
continuation. This is the approach that we were using at the beginning of
the project~\cite{marsik2014algebraic}.

However, this approach has several downsides. First, our formal semantics
is contaminated by extra complexity due to the management of variables and
operations like $\op{gensym}$. Second, there is an extra level of
indirection in place. Instead of computing the truth value of a sentence in
some model, we compute a formula and then that formula itself can be
evaluated in a model. Third, binding in logical formulas is no longer
managed by the (meta) calculus. This means that it is easy to $\op{gensym}$
a variable that is supposed to occur in the scope of a quantifier and then
have that variable accidentally project outside of its scope, leading to
the generation of malformed logical formulas.

The $\CC$ operator presents another solution to this problem. If we know
that in a continuation of type $\iota \to \FF_E(o)$, the effects of the
$\FF_E(o)$ do not depend on the $\iota$, we could move them out of the body
of the function. This is exactly what the $\CC$ operator does.\footnote{If
  we are mistaken and the effects do depend on the bound variable, then
  computation gets stuck (instead of producing a malformed logical
  formula).} With it, we can wrap the existential quantifier over the
continuation using the following expression, which we called $\existsr P$
in~\ref{ssec:revising-dynamic-handler}:\footnote{The type of $\exists$ is
  $(\iota \to o) \to o$ and the type of $P$ is $\iota \to \FF_E(o)$.}

$$
  \exists \apr (\ap{\CC}{P})
$$

The inspiration for the $\CC$ operator came from Philippe de Groote's work
on logical relations and
conservativity~\cite{degroote2015conservativity}. Within his work, de
Groote makes use of the following mathematical structure: a type
transformer $\pT$ equipped with three operation $\pU$, $\pBullet$ and $\pC$
with the following types:

\begin{align*}
  \pU &: \alpha \to \pT \alpha \\
  \pBullet &: \pT(\alpha \to \beta) \to \pT \alpha \to \pT \beta \\
  \pC &: (\alpha \to \pT \beta) \to \pT (\alpha \to \beta)
\end{align*}

and obeying the following laws:

\begin{align*}
  (\ap{\pU}{f}) \pBullet (\ap{\pU}{a}) &= \ap{\pU}{(\ap{f}{a})} \\
  \ap{\pC}{(\lam{x}{\ap{\pU}{(\ap{f}{x})}})} &= \ap{\pU}{f}
\end{align*}

If we ignore the $\pC$ operator for the moment and focus only on the $\pT$
type transformer, the $\pU$ and $\pBullet$ operations and the first of the
two laws, we see that we have a weaker version of an applicative
functor. An applicative functor is a functor, which in our category is a
type transformer such as $\pT$ that can also lift functions, equipped with
two operations, $\pure$ and $\circledast$, which have the same types as
$\pU$ and $\pBullet$ respectively, and satisfying the four laws given
in~\ref{ssec:applicative-functor}. The first of the two laws given above is
the homomorphism law~\ref{law:app-homomorphism} of applicative functors. We
note that all three instances of this structure that are used in the
examples of~\cite{degroote2015conservativity} also satisfy all four of the
applicative functor laws.

We now look at the $\pC$ operator, whose type is the same as the $\CC$
operator that we added to $\calc$ (with $\FF_E$ corresponding to
$\pT$). Its behavior is specified only by the second law, which defines the
value of $\pC$ when applied to a pure function (a function of the form
$\lam{x}{\ap{\pU}{(\ap{f}{x})}}$). Our $\CC$ also obeys this law by
including it as the $\CC_\eta$ reduction rule which is part of the
definition of $\CC$:\footnote{The fact that we can take this law and use it
  as a definition of the $\CC$ operator is due to $\eta$ being a
  constructor in $\calc$, $\eta$-headed expressions are not head-reducible
  and therefore can be reliably pattern-matched on.}

$$
\ap{\CC}{(\lam{x}{\ap{\eta}{(\ap{f}{x})}})} \; \to_{\CC_\eta} \;
\etaE{(\lam{x}{\ap{f}{x}})} \; \to_\eta \; \etaE{f}
$$

Our definition of $\CC$ also includes the $\CC_{\op{op}}$ reduction
rule. This extends the definition of $\CC$ to computations which use
operations. However, our definition of $\CC$ is not total, as there are
well-typed arguments for $\CC$ which will cause computation to get
stuck. This correlates with the $\pC$ operator in de Groote's work, where
certain values of the $\CC$ operator are left undefined.

It is interesting that the $\CC$ operator that turned out to be useful in
our work originated in another work whose objective is to capture the way
meanings are extended in formal semantics. In our approach, we address
conservativity\footnote{Conservativity is understood as the notion that
  extending the grammar to cover a new phenomenon should not change the
  meanings which were already correct in the simpler grammar.} in two
ways. First, we show how to lift a simply-typed semantics into a monadic
semantics while preserving the meanings it assigns to sentences in
Section~\ref{sec:lifting-semantics}. Then, in
Chapter~\ref{chap:composing-effects}, we extend a dynamic grammar with
presuppositions~(\ref{sec:adding-presuppositions}), conventional
implicature~(\ref{sec:adding-conventional-implicature}),
deixis~(\ref{sec:adding-deixis}) and
quantification~(\ref{sec:adding-quantification}), and at every step, the
only modifications we make to existing entries is to add handlers for new
effects, which does not affect the meaning of any of the sentences analyzed
before. Furthermore, since for any effect signature $E$, $\FF_E$ is a type
transformer which together with $\eta$ as $\pU$, $\aplr$ as $\pBullet$ and
$\CC$ as $\pC$ has the structure needed
in~\cite{degroote2015conservativity}, de Groote's technique of logical
relations should apply to the kinds of meanings produced by our approach as
well.\footnote{Modulo the partiality of $\CC$.}


\subsection{Linguistic Modelling}
\label{ssec:comparison-linguistic}

Most of the linguistic analyses presented in our thesis are translations of
existing analyses to the $\calc$ framework:

\begin{itemize}
\item Quantification 

  Our technique of using the $\op{scope}$ operation follows approaches that
  propose the use of continuations in natural
  language~\cite{de2001type,barker2002continuations}. The type and the
  semantics of our $\op{scope}$ operation match those of $\shift$, a
  control operator for delimited continuations used to treat quantification
  in~\cite{shan2005linguistic}. Instead of context levels, we use
  quantifier raising to account for scope ambiguity. As in Montague's
  work~\cite{montague1973proper}, we have a syntactic construction rule
  that takes a quantificational noun phrase and uses it to bind an $NP$
  variable within a sentence ($\abs{QR} : QNP \limp (NP \limp S) \limp
  S$). The syntactic type of $\abs{QR}$ is based on the type given to the
  determiners \emph{every} and \emph{some}
  in~\cite{pogodalla2007generalizing}
  ($C_{\text{every}} : N \limp (NP \limp S) \limp S$). By composing
  $\abs{every} : N \limp QNP$ and
  $\abs{QR} : QNP \limp (NP \limp S) \limp S$, we get
  $\lam{N}{\ap{\abs{QR}}{(\ap{\abs{every}}{N})}} : N \limp (NP \limp S)
  \limp S$. In~\ref{ssec:algebraic-quantification}, we have also pointed
  out that a $\calc$ computation invoking a series of $\op{scope}$
  operations is like a nested sequence of quantifiers in Keller
  storage~\cite{keller1988nested}. However, unlike Keller storage (and
  Cooper storage~\cite{cooper1979montague}), the quantifiers cannot be
  retrieved in any order since one could bind a variable in
  another.\footnote{We could imagine adding a rule allowing us to permute
    the order of certain operations, as long as the result of one is not
    bound in the input to the other. However, we would lose confluence of
    our calculus at that point.} Therefore, we solve ambiguity by other
  means ($\abs{QR}$).

\item Conventional implicature

  Our analysis stems from Potts' dissertation on the logic of conventional
  implicatures~\cite{potts2005logic}. This multidimensional approach to the
  semantics of conventional implicatures was adapted into a monadic
  treatment by Giorgolo, Asudeh et al in
  2011~\cite{giorgolo2011multidimensional,giorgolo2012monads}. Monadic
  treatments of conventional implicatures have also appeared in recent
  ESSLLI courses: the treatment of expressives by Kiselyov and Shan at
  ESSLLI 2013~\cite{kiselyov2010lambda} and by Barker and Bumford at ESSLLI
  2015. The underlying monad in all the monadic treatments is the writer
  monad. 
\end{itemize}


\section{Future Work}
\label{sec:future-work}
