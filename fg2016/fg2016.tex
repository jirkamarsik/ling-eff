\documentclass{llncs}

\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{bussproofs}
\usepackage{stmaryrd}
\usepackage{gb4e}
\noautomath
\usepackage{url}
\usepackage{hyperref}
\usepackage{subcaption}

\include{definitions}


%
\begin{document}
%
\title{Calculus of Effects and Handlers \\ for Natural Language Semantics}
%
\author{Anonymous Copy}
%
\institute{Anonymous Copy}

\maketitle              % typeset the title of the contribution

\begin{abstract}
\dots
\keywords{\dots}
\end{abstract}
%

\section{Introduction}
\label{sec:introduction}

The prevailing methodology of formal semantics is compositionality in the
sense of Frege: denotations of complex phrases are functions of the
denotations of their immediate constituents. However, several phenomena
have been identified that challenge this notion of
compositionality. Examples include anaphora, presupposition,
quantification, deixis and conventional implicature. In all of these
examples, simple models of denotation (i.e.\ noun phrases are individuals,
sentences are truth-values) run into complications as the denotations can
depend on external values (anaphora, deixis) or on something which is not
an immediate constituent (presupposition, quantification, conventional
implicature).

Among the solutions to these challenges, we find (at least) two types of
solutions. First, we have those that relax the condition of
compositionality. Notably, the denotation of a complex phrase is no longer
a \emph{function} per se of the denotations of its immediate
subconstituents. Rather, it is some other formally defined
process\footnote{This kind of distinction is the same distinction between a
  mathematical function and a function in a programming language, which
  might have all kinds of side effects and therefore not be an actual
  function.}. Examples of this approach include:

\begin{itemize}
\item the incremental algorithm used to build discourse representation
  structures in DRT, as presented in~\cite{kamp1993discourse}
\item the $\lambda\mu$ calculus used in~\cite{de2001type} to analyse
  quantification since, due to the lack of confluence, function terms no
  longer denote functions over simple denotations
\item the use of exceptions and exception handlers
  in~\cite{lebedeva2012expression} to model presuppositions in an otherwise
  compositional framework
\item the parsetree interpretation step in the logic of conventional
  implicatures of~\cite{potts2005logic} that builds the denotation of a
  sentence by extracting implicatures from the denotations of all of its
  subparts (including the non-immediate ones)
\end{itemize}

The other approach is to enrich the denotations so that they are
parameterized by the external information they need to obtain and contain
whatever internal information they need to provide to their
superconstituents. Here are some examples of this style:

\begin{itemize}
\item any kind of semantic indices (e.g.\ the speaker and addressee for
  deixis, the current world for modality), since they amount to saying that
  a phrase denotes an indexed set of simpler meanings
\item the continuized semantics of~\cite{barker2002continuations} for
  quantification in which denotations are functions of their own
  continuations
  \begin{itemize}
  \item and more generally, any semantics using type raising or generalized
    quantifiers for noun phrase denotations
  \end{itemize}
\item the dynamic denotations of~\cite{de2006towards} that are functions of
  the common ground and their continuation
\item compositional event semantics, such as the one
  in~\cite{qian2011event}, that shift the denotations of sentences from
  truth-values to predicates on events
\end{itemize}

We want to find a common language in which we could express the above
techniques. Our inspiration comes from computer science. There, a concept
known as \emph{monad} has been used:
\begin{itemize}
\item in denotational semantics to give the domain of interpretation for
  programming languages that involve side effects~\cite{moggi1991notions}.
\item in functional programming to emulate programming with side effects
  via term-level encodings of effectful programs~\cite{wadler1992essence}.
\end{itemize}
These two principal applications of monads align with the two approaches we
have seen above. One where we change our calculus so it no longer defines
pure functions (e.g.\ is non-deterministic, stateful or throws exceptions)
and the one where we use use a pure calculus to manipulate terms
(denotations) that encode some interaction (e.g.\ dynamicity, continuations
or event predication).

Monad is a term from category-theory. Its meaning is relative to some
category. For us, this will always be the category whose objects are types
and whose arrows are functions between different types. A monad is formed
by a functor and a pair of natural transformations that satisfy certain
laws. In our case, this means that a monad is some type wrapper (the
functor part) and some combinators (the natural transformations) that
follow some basic laws. To give an example of this, we can think of the
functor $T(\alpha) = (\alpha \to o) \to o$ together with combinators such
as the type raising $\eta(x) = \lam{P}{\ap{P}{x}}$ as a monad of
quantification.

The usefulness of monads in natural language semantics has been discovered
by Shan in 2002~\cite{shan2002monads}\footnote{Side effects are to
  programming languages what pragmatics are to natural languages: they both
  study how expressions interact with the worlds of their users. It might
  then come as no surprise that phenomena such as anaphora, presupposition,
  deixis and conventional implicature yield a monadic description.}. Since
then, the problem that remained was how to compose several different monads
in a single solution. Marlow used the popular method of monad
morphisms\footnote{Also known as monad transformers in functional
  programming.} to combine several monads in his
dissertation~\cite{charlow2014semantics}. Giorgolo and Asudeh have used
distributive laws between monads to combine
monads~\cite{giorgolo2015natural}, while Kiselyov has eschewed monads
altogether in favor of applicative functors which enjoy easy composability
\cite{kiselyov2015applicative}.

Our approach follows the recent trend in adopting effects and handlers to
combine side effects~\cite{bauer2012programming,kammar2013handlers} and to
encode effectful programs in pure functional programming
languages~\cite{kiselyov2013extensible,brady2013programming}.

The idea is that we can represent each of the relevant monads using an
algebra. We can then combine the algebras by summing up their
signatures. The resulting algebra will serve as a universal representation
format for terms built from any of the source algebras and closed on
substitution. Then, we will build modular interpreters that will give
meanings to the operators of the algebras in terms of individuals,
truth-values and functions.

In Sect.~\ref{sec:definition}, we will introduce a formal calculus for
working with the algebraic terms that will use in our linguistic
denotations. In Sect.~\ref{sec:phenomena}, we will incrementally build up a
fragment involving several of the linguistic phenomena and see the calculus
in action. Before we conclude in Sect.~\ref{sec:conclusion}, we will also
discuss some of the formal properties of the calculus in
Sect.~\ref{sec:properties}.


\section{Definition of the Calculus}
\label{sec:definition}

Having sketched the general idea behind our calculus in the introduction,
we will now turn our attention to the specifics. We start by defining the
syntactic constructions used to build the terms of our language.

\subsection{Terms}
\label{ssec:terms}

First off, let $\XX$ be a set of variables, $\Sigma$ a typed signature and
$\EE$ a set of operation symbols. In the definition below, we will let $M$,
$N$\ldots range over terms, $x$, $y$, $z$\ldots range over variables from
$\XX$, $c$, $d$\ldots range over the names constants from $\Sigma$ and
$\op{op}$, $\op{op}_i$\ldots range over the operation symbols in $\EE$.

The terms of our language are comprised of the following:

\begin{align*}
  M, N ::= &\ \lam{x}{M} & \mbox{[abstraction]} \\
   | \, &\ \ap{M}{N} & \mbox{[application]} \\
   | \, &\ x & \mbox{[variable]} \\
   | \, &\ c & \mbox{[constant]} \\
   | \, &\ \app{\op{op}}{M_{\mathrm{p}}}{(\lam{x}{M_{\mathrm{c}}})} & \mbox{[operation]} \\
   | \, &\ \ap{\eta}{M} & \mbox{[injection]} \\
   | \, &\ \ap{\cdbanana}{N} & \mbox{[handler]} \\
   | \, &\ \ap{\cherry}{M} & \mbox{[extraction]} \\
   | \, &\ \ap{\CC}{M} & \mbox{[exchange]} 
\end{align*}

The first four constructions --- abstraction, application, variables and
constants --- come directly from the simply-typed lambda calculus (STLC)
with constants.

The next four deal with the algebraic expressions used to encode
computations. Let us sketch the behaviors of these four kinds of
expressions.

The operation $\op{op}$ and injection $\eta$ expressions will serve as the
constructors for our algebraic expressions. Algebraic expressions are
usually formed by operation symbols and then variables at the
leaves. Instead of variables, our algebraic expressions use terms from our
calculus for ther leaves. The $\eta$ constructor can thus take an ordinary
term from our calculus and make it a leaf node in an algebraic
expression. The operation symbols $\op{op}$ are then the operations of the
algebra.

The other three expression types correspond to functions over algebraic
expressions.
\begin{itemize}
\item The most useful is the handler $\banana{}$\footnote{Pronounced
  ``banana''. See~\cite{meijer1991functional} for the introduction of
  banana brackets.}. It acts as an iterator for the type of algebraic
  expressions. The terms $M_1$,\ldots,$M_n$ and $M_\eta$ in $\cdbanana$ are
  the clauses for the constructors $\op{op}_1$,\ldots,$\op{op}_n$ and
  $\eta$, respectively. We will use handlers to define interpretations of
  operation symbols in algebraic expressions.
\item The cherry $\cherry$ operator allows us to extract terms out of
  algebraic expressions. If an algebraic expression is of the form
  $\ap{\eta}{M}$, applying $\cherry$ to it will yield $M$.
\item The exchange operator $\CC$ permits a kind of commutation between the
  $\lambda$ binder and the operation symbols. We will see its use later.
\end{itemize}


\subsection{Types}
\label{ssec:types}

We now give a syntax for the types of our calculus alongside with a typing
relation. In the grammar below, $\alpha$, $\beta$, $\gamma$\ldots range
over types, $\nu$ ranges over atomic types from some set $\TT$ and $E$,
$E'$\ldots ranges over effect signatures (introduced below).

The types of our language consist of:

\begin{align*}
  \alpha, \beta, \gamma ::= &\ \alpha \to \beta & \mbox{[function]} \\
   | \, &\ \nu & \mbox{[atom]} \\
   | \, &\ \FF_E(\alpha) & \mbox{[computation]}
\end{align*}

The only novelty here is the $\FF_E(\alpha)$ computation type. This is the
type of algebraic expressions whose leaves are terms of type $\alpha$ and
whose operation symbols come from the effect signature $E$. We call them
\emph{computation types} and we call terms of these types
\emph{computations} because our algebraic expressions will always represent
some kind of effectful program.

\emph{Effect signatures} are similar to typing contexts. They are partial
mappings from the set of operation symbols $\EE$ to pairs of types. We will
write the elements of effect signatures the following way ---
$\typedop{op}{\alpha}{\beta} \in E$ means that $E$ maps $\op{op}$ to the
pair of types $\alpha$ and $\beta$\footnote{The two types $\alpha$ and
  $\beta$ are to be seen as the operation's \emph{input} and \emph{output}
  types, respectively.}. When dealing with effect signatures, we will often
make use of the disjoint union operator $\uplus$. The term $E_1 \uplus E_2$
serves as a constraint demanding that the domains of $E_1$ and $E_2$ be
disjoint and at the same time it denotes the effect signature that is the
union of $E_1$ and $E_2$.

The typing rules are presented in Figure~\ref{fig:types}.

\newcommand{\handlerrule}{
 \begin{prooftree}
  \AxiomC{$E = \{\typedopg{\op{op}_i}{\alpha_i}{\beta_i}\}_{i \in I} \uplus E_{\mathrm{f}}$}
  \noLine
  \def\extraVskip{0pt}
  \UnaryInfC{$E' = E'' \uplus E_{\mathrm{f}}$}
  \noLine
  \UnaryInfC{$[\Gamma \vdash M_i : \alpha_i \to (\beta_i \to
    \FF_{E'}(\delta)) \to \FF_{E'}(\delta)]_{i \in I}$}
  \noLine
  \UnaryInfC{$\Gamma \vdash M_\eta : \gamma \to \FF_{E'}(\delta)$}
  \noLine
  \UnaryInfC{$\Gamma \vdash N : \FF_{E}(\gamma)$}
  \def\extraVskip{2pt}
  \RightLabel{[$\banana{}$]}
  \UnaryInfC{$\Gamma \vdash \ap{\cibanana}{N} : \FF_{E'}(\delta)$}
 \end{prooftree}}

\begin{figure}
  \def\labelSpacing{4pt}

  \begin{subfigure}{.5\textwidth}
   \begin{prooftree}
    \AxiomC{$\Gamma, x : \alpha \vdash M : \beta$}
    \RightLabel{[abs]}
    \UnaryInfC{$\Gamma \vdash \lam{x}{M} : \alpha \to \beta$}
   \end{prooftree}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
   \begin{prooftree}
    \AxiomC{$\Gamma \vdash M : \alpha \to \beta$}
    \AxiomC{$\Gamma \vdash N : \alpha$}
    \RightLabel{[app]}
    \BinaryInfC{$\Gamma \vdash M N : \beta$}
   \end{prooftree}
  \end{subfigure}

  \vspace{2mm}
 
  \begin{subfigure}{.5\textwidth}
   \begin{prooftree}
    \AxiomC{$x : \alpha \in \Gamma$}
    \RightLabel{[var]}
    \UnaryInfC{$\Gamma \vdash x : \alpha$}
   \end{prooftree}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
   \begin{prooftree}
    \AxiomC{$c : \alpha \in \Sigma$}
    \RightLabel{[const]}
    \UnaryInfC{$\Gamma \vdash c : \alpha$}
   \end{prooftree}
  \end{subfigure}

  \vspace{6mm}

  \begin{subfigure}{.5\textwidth}
   \begin{prooftree}
    \AxiomC{$\Gamma \vdash M : \alpha$}
    \RightLabel{[$\eta$]}
    \UnaryInfC{$\Gamma \vdash \ap{\eta}{M} : \FF_E(\alpha)$}
   \end{prooftree}
  \end{subfigure}
  \begin{subfigure}{.5\textwidth}
   \begin{prooftree}
    \AxiomC{$\Gamma \vdash M_{\mathrm{p}} : \alpha$}
    \AxiomC{$\Gamma, x : \beta \vdash M_{\mathrm{c}} : \FF_E(\gamma)$}
    \def\extraVskip{0pt}
    \noLine
    \BinaryInfC{$\typedop{op}{\alpha}{\beta} \in E$}
    \def\extraVskip{2pt}
    \RightLabel{[op]}
    \UnaryInfC{$\Gamma \vdash \app{\op{op}}{M_{\mathrm{p}}}{(\lam{x}{M_{\mathrm{c}}})} : \FF_E(\gamma)$}
   \end{prooftree}
  \end{subfigure}

  \vspace{3mm}

  \hspace{-1.5cm}
  \begin{subfigure}{.5\textwidth}
   \begin{prooftree}
    \AxiomC{$\Gamma \vdash M : \FF_\emptyset(\alpha)$}
    \RightLabel{[$\cherry$]}
    \UnaryInfC{$\Gamma \vdash \ap{\cherry}{M} : \alpha$}
   \end{prooftree}
  \end{subfigure}
  \hspace{1cm}
  \begin{subfigure}{.5\textwidth}
   \handlerrule
  \end{subfigure}

  \vspace{6mm}

  \begin{subfigure}{\textwidth}
   \begin{prooftree}
    \AxiomC{$\Gamma \vdash M : \alpha \to \FF_E(\beta)$}
    \RightLabel{[$\CC$]}
    \UnaryInfC{$\Gamma \vdash \ap{\CC}{M} : \FF_E(\alpha \to \beta)$}
   \end{prooftree}
  \end{subfigure}

  \caption{\label{fig:types}Typing rules for our calculus.}
\end{figure}

The typing rules mirror the syntax of terms. Again, the first four rules
come from STLC.\@ The [$\eta$] and [$\cherry$] rules are self-explanatory
and so we will focus on the [$\op{op}$], [$\banana{}$] and [$\CC$] rules.

\subsubsection*{[$\op{op}$]}

To use an operation $\typedop{op}{\alpha}{\beta}$, we provide the input
parameter $M_{\mathrm{p}} : \alpha$ and a continuation
$\lam{x}{M_{\mathrm{c}}} : \beta \to \FF_E(\gamma)$ which expects the
output of type $\beta$. The resulting term has the same type as the body of
the continuation, $\FF_E(\gamma)$.

Before, we have spoken of terms of type $\FF_E(\gamma)$ as of algebraic
expressions generated by the terms of type $\gamma$ and the operators in
the effect signature $E$. However, having seen the typing rule for
operation terms, it might not be obvious how such a term represents an
algebraic expression. Traditionally, algebraic signatures map operation
symbols to arities, which are natural numbers. Our effect signatures map
each operation symbol to a pair of types $\alpha \rightarrowtail \beta$.
\begin{itemize}
\item We can explain $\alpha$ by analogy to the single-sorted algebra of
  vector spaces. In a single-sorted vector space algebra, scalar
  multiplication is viewed as a unary operation parameterized by some
  scalar. So technically, there is a different unary operation for each
  scalar. All of our operations are similarly parameterized and $\alpha$ is
  the type of that parameter.
\item The type $\beta$ expresses the arity of the operator. When we say
  that an operator has arity $\beta$, where $\beta$ is a type, we mean that
  it takes one operand for every value of $\beta$
  \cite{pretnar2010logic}. We can also think of the operator as taking one
  operand with $x : \beta$ as a free variable.
\end{itemize}

We can look at the algebraic expression
$\app{\op{op}}{M_{\mathrm{p}}}{(\lam{x}{M_{\mathrm{c}}})}$ as a description
of a program that:
\begin{itemize}
\item interacts with its context by some operator called $\op{op}$
\item to which it provides the input $M_{\mathrm{p}}$
\item and from which it expects to receive an output of type $\beta$
\item which it will then bind as the variable $x$ and continue as the
  program described by $M_{\mathrm{c}}$.
\end{itemize}

\subsubsection*{[$\banana{}$]}

The banana brackets describe iterators/catamorphisms\footnote{These are
  similar to recursors/paramorphisms. See~\cite{meijer1991functional} for
  the difference. Catamorphisms are also known as folds and the common
  higher-order function \emph{fold} found in functional programming
  languages is actually the iterator/catamorphism for lists.}. In the
typing rule, $E$ is the input's signature, $E'$ is the output's signature,
$\gamma$ is the input's leaf type and $\delta$ is the output's leaf
type. $E$ is decomposed into the operation that our iterator will actually
interpret, the other operations form a residual signature
$E_{\mathrm{f}}$. The output signature will then still contain the
uninterpreted operations $E_{\mathrm{f}}$ combined with any operations
$E''$ that our interpretation might introduce.

\subsubsection*{[$\CC$]}

We said before that the $\CC$ function will let us commute $\lambda$ and
operations. Here, we see that on the type level, this corresponds to
commuting the $\FF_E(\_)$ and the $\alpha \to \_$ type constructors.

\subsection{Reduction Rules}
\label{ssec:reductions}

We will now finally give a semantics to our calculus. The semantics will be
given in the form of a reduction relation on terms. Even though the point
of the calculus is to talk about effects, the reduction semantics will not
be based on any fixed evaluation order; any subterm that is a redex can be
reduced in any context. The reduction rules are given in
Fig.~\ref{fig:reductions}.

\begin{figure}
  \centering
  \begin{tabular}{lr}
  $\ap{(\lam{x}{M})}{N} \to$ & rule $\beta$ \\
  $\subst{M}{x}{N}$ & \\
  \\
  $\lam{x}{\ap{M}{x}} \to$ & rule $\eta$ \\
  $M$ & where $x \notin \FV(M)$ \\
  \\
  $\ap{\cibanana}{(\ap{\eta}{N})} \to$ & rule $\banana{\eta}$ \\
  $\ap{M_\eta}{N}$ & \\
  \\
  $\ap{\cibanana}{(\ap{\ap{\op{op}_j}{N_{\mathrm{p}}}}{(\lam{x}{N_{\mathrm{c}}})})} \to$ & rule $\banana{\op{op}}$ \\
  $\ap{M_j}{\ap{N_{\mathrm{p}}}{(\lam{x}{\ap{\cibanana}{N_{\mathrm{c}}}})}}$
  & where $j \in I$ \\
  & and $x \notin \FV((M_i)_{i \in I}, M_\eta)$ \\
  \\
  $\ap{\cibanana}{(\ap{\ap{\op{op}_j}{N_{\mathrm{p}}}}{(\lam{x}{N_{\mathrm{c}}})})} \to$ & rule $\banana{\op{op}'}$ \\
  $\ap{\op{op}_j}{\ap{N_{\mathrm{p}}}{(\lam{x}{\ap{\cibanana}{N_{\mathrm{c}}}})}}$
  & where $j \notin I$ \\
  & and $x \notin \FV((M_i)_{i \in I}, M_\eta)$ \\
  \\
  $\ap{\cherry}{(\ap{\eta}{M})} \to$ & rule $\cherry$ \\
  $M$ & \\
  \\
  $\ap{\CC}{(\lam{x}{\ap{\eta}{M}})} \to$ & rule $\CC_\eta$ \\
  $\ap{\eta}{(\lam{x}{M})}$ & \\
  \\
  $\ap{\CC}{(\lam{x}{\ap{\ap{\op{op}}{M_{\mathrm{p}}}}{(\lam{y}{M_{\mathrm{c}}})}})} \to$ & rule $\CC_\op{op}$ \\
  $\ap{\ap{\op{op}}{M_{\mathrm{p}}}}{(\lam{y}{\ap{\CC}{(\lam{x}{M_{\mathrm{c}}})}})}$
  & where $x \notin \FV(M_{\mathrm{p}})$
  \end{tabular}
  
  \caption{\label{fig:reductions} The reduction rules of our calculus.}
\end{figure}

We have the $\beta$ and $\eta$ rules. By no coincidence, they are the same
rules as the ones found in STLC.\@ The rest are function definitions for
$\banana{}$, $\cherry$ and $\CC$.

By looking at the definition of $\banana{}$, we see that it as a standard
iterator. It recursively replaces every occurrence of the constructors
$\op{op}_j$ and $\eta$ with $M_j$ and $M_\eta$, respectively.

The $\CC$ function recursively swaps [$\ap{\CC}{(\lam{x}{}}$] with
[$\app{\op{op}}{M_{\mathrm{p}}}{(\lam{y}{}}$] using the $\CC_{\op{op}}$
rule. When it finally hits the $\eta$ constructor, it swaps [$(\lam{x}$]
with [$\eta$] and terminates. Note that the constraint that $x \notin
\FV(M_{\mathrm{p}})$ cannot be dismissed by renaming of bound variables. If
the parameter to $M_{\mathrm{p}}$ contains a free occurrence of $x$ that
cannot be reduced away, the evaluation of $\CC$ will get stuck. $\CC$ is
thus a partial function: it is only applicable when none of the operations
being commuted with the $\lambda$-binder actually depend on the bound
variable.

\subsection{Common Combinators}
\label{ssec:common-combinators}

When showing off the calculus in the next Section, the following
combinators will be helpful. First, we define a sequencing operator. The
operator $\hsbind$, called bind\footnote{This is the same bind as the one
  used to define monads in functional programming languages.}, replaces all
the $\alpha$-typed leaves of a $\FF_E(\alpha)$-typed expression with
$\FF_E(\beta)$-typed expressions. More intuitively, $M \hsbind N$ is the
program that first runs $M$ to get its result $x$ and then continues as the
program $\ap{N}{x}$.

\begin{align*}
  \_ \hsbind \_ &: \FF_E(\alpha) \to (\alpha \to \FF_E(\beta)) \to \FF_E(\beta) \\
  M \hsbind N &= \ap{\banana{\onto{\eta}{N}}}{M}
\end{align*}

Using $\hsbind$, we can define the following variations on the application
operator.

\begin{align*}
  \_ \apl \_ &: \FF_E(\alpha \to \beta) \to \alpha \to \FF_E(\beta) \\
  F \apl x &= F \hsbind (\lam{f}{\ap{\eta}{(\ap{f}{x})}}) \\
  \_ \apr \_ &: (\alpha \to \beta) \to \FF_E(\alpha) \to \FF_E(\beta) \\
  f \apr X &= X \hsbind (\lam{x}{\ap{\eta}{(\ap{f}{x})}}) \\
  \_ \aplr \_ &: \FF_E(\alpha \to \beta) \to \FF_E(\alpha) \to \FF_E(\beta) \\
  F \aplr X &= F \hsbind (\lam{f}{X \hsbind (\lam{x}{\ap{\eta}{(\ap{f}{x})}})})
\end{align*}

All of these operators associate to the left, so $f \apr X \aplr Y$ should
be read as $(f \apr X) \aplr Y$.


\section{Linguistic Phenomena as Effects}
\label{sec:phenomena}

\subsection{Deixis}
\label{ssec:deixis}

We will now try to use this calculus to do some semantics. Here is our
tectogrammar in an abstract categorial grammar
presentation~\cite{de2001towards}.

\begin{align*}
  \abs{John}, \abs{Mary}, \abs{me} &: NP \\
  \abs{loves} &: NP \limp NP \limp S
\end{align*}

And here is our semantics.

\begin{align*}
  \lex{John}{\etaE{\obj{j}}} \\
  \lex{Mary}{\etaE{\obj{m}}} \\
  \lex{me}{\app{\op{speaker}}{\star}{(\lam{x}{\etaE{x}})}} \\
  \lex{loves}{\lam{O S}{{\obj{love}} \apr S \aplr O}}
\end{align*}

This, and all the semantics we will see in this paper, satisfies a
homomorphism condition that whenever $M : \tau$ then $\sem{M} :
\sem{\tau}$. In our case, $\sem{NP} = \FF_E(\iota)$ and $\sem{S} =
\FF_E(o)$, where $\iota$ and $o$ are the types of individuals and
propositions, respectively. Of $E$, we assume that
$\typedop{speaker}{1}{\iota} \in E$, since that is the type of
$\op{speaker}$ used in our semantics\footnote{$1$ is the unit type whose
  only element is written as $\star$.}.

With this fragment, we can give meanings to trivial sentences like:

\begin{exe}
  \ex John loves Mary. \label{ex:trivial}
  \ex Mary loves me. \label{ex:deixis}
\end{exe}

whose meanings we can calculate as:

\begin{align}
  \sem{\app{\abs{loves}}{\abs{Mary}}{\abs{John}}} & \tto 
  \etaE{(\app{\obj{love}}{\obj{j}}{\obj{m}})} \\
  \sem{\app{\abs{loves}}{\abs{me}}{\abs{Mary}}} & \tto
  \app{\op{speaker}}{\star}{(\lam{x}{\app{\obj{love}}{\obj{m}}{x}})}
\end{align}

The meaning of~\eqref{ex:trivial} is a proposition of type $o$ wrapped in
$\eta$, i.e.\ something that we can interpret in a model. As for the
meaning of~\eqref{ex:deixis}, the $\op{speaker}$ operator has propagated
from the $\abs{me}$ lexical entry up to the meaning of the wholse
setence. We now have an algebraic expression having as operands the
propositions $\app{\obj{love}}{\obj{m}}{x}$ for all possible $x :
\iota$. In order to get a single proposition which is to be seen as the
truth-conditional meaning of the sentence and which can be evaluated in a
model, we will need to fix the speaker. We will do so by defining an
interpreting handler.

\begin{align*}
  \withSpeaker &: \iota \to \FF_{\{\typedop{speaker}{1}{\iota}\} \uplus
    E}(\alpha) \to \FF_E(\alpha) \\
  \withSpeaker &= \lam{s M}{\ap{\banana{\onto{\op{speaker}}{(\lam{x k}{\ap{k}{s}})}}}{M}}
\end{align*}

NB: We omitted the $\eta$ clause in the banana brackets above. In such
cases, we say there is a default clause $\onto{\eta}{(\lam{x}{\etaE{x}})}$.

$$
  \app{\withSpeaker}{s}{\sem{\app{\abs{loves}}{\abs{me}}{\abs{Mary}}}} \tto
  \etaE{(\app{\obj{love}}{\obj{m}}{s})}
$$

So far, we could have done the same by introducing a constant named
$\obj{me}$ to stand in for the speaker. However, since handlers are part of
our object language, we can include them in lexical entries. With this, we
can handle phenomena such as direct (quoted) speech, that rebinds the
current speaker in a certain scope.

\begin{align*}
  \abs{said}_{\abs{is}} &: S \limp NP \limp S \\
  \abs{said}_{\abs{ds}} &: S \limp NP \limp S
\end{align*}

Those are our new syntactic constructors: one for the indirect speech use
of \emph{said} and the other for the direct speech use (their surface
realizations would differ typographically or phonologically). Let us give
them some semantics.

\begin{align*}
  \sem{\abs{said}_{\abs{is}}} &= \lam{C S}{\obj{say} \apr S \aplr C} \\
                             &= \lam{C S}{S \hsbind (\lam{s}{\ap{\obj{say}}{s} \apr C})} \\
  \sem{\abs{said}_{\abs{ds}}} &= \lam{C S}{S \hsbind (\lam{s}{\ap{\obj{say}}{s} \apr (\app{\withSpeaker}{s}{C})})}
\end{align*}

Here we elaborated the entry for indirect speech so it is easier to compare
with the one for direct speech, which just adds a use of the $\withSpeaker$
operator.

\begin{exe}
  \ex John said Mary loves me. \label{ex:indirect-speech}
  \ex John said, ``Mary loves me''. \label{ex:direct-speech}
\end{exe}

\begin{align}
  \sem{\app{\abs{said}_{\abs{is}}}{(\app{\abs{loves}}{\abs{me}}{\abs{Mary}})}{\abs{John}}}
  & \tto \app{\op{speaker}}{\star}{(\lam{x}{\etaE{(\app{\obj{say}}{\obj{j}}{(\app{\obj{love}}{\obj{m}}{x})})}})} \\
  \sem{\app{\abs{said}_{\abs{ds}}}{(\app{\abs{loves}}{\abs{me}}{\abs{Mary}})}{\abs{John}}}
  & \tto \etaE{(\app{\obj{say}}{\obj{j}}{(\app{\obj{love}}{\obj{m}}{\obj{j}})})}
\end{align}

The meaning of sentence~\eqref{ex:indirect-speech} depends on the speaker
(as testified by the use of the $\op{speaker}$ operator) whereas
in~\eqref{ex:direct-speech}, this dependence has been eliminated due to the
use of direct speech.

\subsection{Quantification}
\label{ssec:quantification}

Now we turn our attetion to quantificational noun phrases.

\begin{align*}
  \abs{every}, \abs{a} &: N \limp NP \\
  \abs{man}, \abs{woman} &: N
\end{align*}

Let $\circ : o \to o \to o$ be a binary operator on propositions. We define
the following syntax for the same operator lifted to computations of propositions.

\begin{align*}
  \_ \mathop{\overline{\circ}} \_ &: \FF_E(o) \to \FF_E(o) \to \FF_E(o) \\
  M \mathop{\overline{\circ}} N &= (\lam{m n}{m \circ n}) \apr M \aplr N
\end{align*}

\begin{align*}
  \lex{every}{\lam{N}{\app{\op{scope}}{(\lam{c}{\forall \apr
          (\ap{\CC}{(\lam{x}{(N \apl x) \dimpl (\ap{c}{x})})})})}{(\lam{x}{\etaE{x}})}}} \\
  \lex{a}{\lam{N}{\app{\op{scope}}{(\lam{c}{\exists \apr
          (\ap{\CC}{(\lam{x}{(N \apl x) \dand (\ap{c}{x})})})})}{(\lam{x}{\etaE{x}})}}} \\
  \lex{man}{\etaE{\obj{man}}} \\
  \lex{woman}{\etaE{\obj{woman}}}
\end{align*}

The entries for $\abs{every}$ and $\abs{a}$ might seem
intimidating. However, if we ignore the $\apr$, the $\CC$, the $\apl$ and
the overline on the logical operator, we get the familiar generalized
quantifiers. These decorations are the plumbing that takes care of the
order of evaluation.

While the terms that use the $\op{scope}$ operator, the handler that
interprets is as simple as can be.

\begin{align*}
  \SI &= \lam{M}{\ap{\banana{\onto{\op{scope}}{(\lam{c k}{\ap{c}{k}})}}}{M}}
\end{align*}

Same as with $\withSpeaker$, $\SI$ will also be used in lexical items. By
interpreting the $\op{scope}$ operation in a particular place, we
effectively determine the scope of the quantifier. Hence the name of $\SI$,
short for Scope Island. If we want to model clause boundaries as scope
islands, we can do so by inserting $\SI$ in the lexical entries of clause
constructors (in our case, the verbs).

\begin{align*}
  \sem{\abs{loves}} &:= \lam{O S}{\ap{\SI}{(\app{\sem{\abs{loves}}}{O}{S})}} \\
  \sem{\abs{said}_{\abs{is}}} &:= \lam{C S}{\ap{\SI}{(\app{\sem{\abs{said}_{\abs{is}}}}{C}{S})}} \\
  \sem{\abs{said}_{\abs{ds}}} &:= \lam{C S}{\ap{\SI}{(\app{\sem{\abs{said}_{\abs{ds}}}}{C}{S})}}
\end{align*}

The denotations used on the right-hand side of these revised definitions
stand for the denotations given to them previously.

\begin{exe}
  \ex Every man loves a woman.
  \ex John said every woman loves me.
  \ex John said, ``Every woman loves me''.
\end{exe}

For the sentences above, we derive:

\begin{align}
  & \sem{\app{\abs{loves}}{(\ap{\abs{a}}{\abs{woman}})}{(\ap{\abs{every}}{\abs{man}})}} \nonumber \\
  & \tto \etaE{(\forall x.\ \ap{\obj{man}}{x} \to (\exists y.\ \ap{\obj{woman}}{y} \land \app{\obj{love}}{x}{y}))} \\
  & \app{\withSpeaker}{s}{\sem{\app{\abs{said}_{\abs{is}}}{(\app{\abs{loves}}{\abs{me}}{(\ap{\abs{every}}{\abs{woman}})})}{\abs{John}}}} \nonumber \\
  & \tto \etaE{(\app{\obj{say}}{\obj{j}}{(\forall x.\ \ap{\obj{woman}}{x} \to \app{\obj{love}}{x}{s})})} \\
  & \sem{\app{\abs{said}_{\abs{ds}}}{(\app{\abs{loves}}{\abs{me}}{(\ap{\abs{every}}{\abs{woman}})})}{\abs{John}}} \nonumber \\
  & \tto \etaE{(\app{\obj{say}}{\obj{j}}{(\forall x.\ \ap{\obj{woman}}{x} \to \app{\obj{love}}{x}{\obj{j}})})}
\end{align}

The calculus offers us enough flexibility to model the semantics. We might
choose to relax the constraint that clauses are scope islands by keeping
the old entries for verbs that do not use the $\SI$ handler. We might then
want to add the $\SI$ handler to the lexical entry of
$\abs{said}_{\abs{ds}}$ next to the $\withSpeaker$ handler so that
quantifiers cannot escape quoted expressions. We might also allow for
inverse scope readings by, e.g., providing entries for transitive verbs
that evaluate their arguments right-to-left (though then we would have to
watch out for crossover effects if we were to add dynamicity).

\subsection{Conventional Implicatures}
\label{ssec:ci}

We will add one last phenomenon to our growing fragment and those will be
conventional implicatures, as analyzed by
Potts~\cite{potts2005logic}. Specifically, we will focus on nominal
appositives.

\begin{align*}
  \abs{appos} &: NP \limp NP \limp NP \\
  \abs{best-friend} &: NP \limp NP
\end{align*}

\begin{align*}
  \lex{appos}{\lam{X Y}{X \hsbind (\lam{x}{\ap{\SI}{(\etaE{x}
          \mathbin{\overline{=}} Y)} \hsbind
        (\lam{i}{\app{\op{implicate}}{i}{(\lam{z}{\etaE{x}})}})})}} \\
  \lex{best-friend}{\lam{X}{\obj{best-friend} \apr X}}
\end{align*}

In the denotation of the nominal appositive construction, $\abs{appos}$, we
first evaluate the head noun phrase $X : \sem{NP}$ to find its referent $x
: \iota$. We then want to implicate that $x$ is equal to the referent of
$Y$, the term $\etaE{x} \mathbin{\overline{=}} Y$ (note the line over $=$)
is the term that computes that referent and gives us the proposition we
want. We also want to state that no quantifier from within the appositive
$Y$ should escape into the matrix clause and so we wrap this computation in
the $\SI$ handler to establish a scope island. Finally, we pass this
proposition as an argument to $\op{implicate}$ and we return $x$ as the
referent of the entire noun phrase.

The point of the $\op{implicate}$ is to smuggle out non-at-issue content
outside the scope of any logical operators. The contribution of an
appositive should survive, e.g., logical negation\footnote{In our limited
  fragment, we will only see it crawl out of a quantifier.}. The place
where we will accomodate the implicated truth-conditions will be determined
by the use of the following handler:

\begin{align*}
  \accomodate &: \FF_{\{\typedop{implicate}{o}{1}\} \uplus E}(o) \to \FF_E(o) \\
  \accomodate &= \lam{M}{\ap{\banana{\onto{\op{implicate}}{(\lam{i
            k}{\etaE{i} \dand \ap{k}{\star}})}}}{M}}
\end{align*}

\begin{exe}
  \ex John, my best friend, loves every woman.
  \ex Mary, everyone's best friend, loves John.
\end{exe}

\begin{align}
  & \app{\withSpeaker}{s}{(\ap{\accomodate}{\sem{\app{\abs{loves}}{(\ap{\abs{every}}{\abs{woman}})}{(\app{\abs{appos}}{\abs{John}}{(\ap{\abs{best-friend}}{\abs{me}})})}}})} \nonumber \\
  & \tto \etaE{((\obj{j} = \ap{\obj{best-friend}}{s}) \land (\forall x.\ \ap{\obj{woman}}{x} \to \app{\obj{love}}{\obj{j}}{x}))} \\
  & \app{\withSpeaker}{s}{(\ap{\accomodate}{\sem{\app{\abs{loves}}{\abs{John}}{(\app{\abs{appos}}{\abs{Mary}}{(\ap{\abs{best-friend}}{\abs{everyone}})})}}})} \nonumber \\
  & \tto \etaE{((\forall x.\ \obj{m} = \ap{\obj{best-friend}}{x}) \land (\app{\obj{love}}{\obj{m}}{\obj{j}}))}
\end{align}

Note that when adding appositives and their conventional implicatures, we
did not have to modify any of the existing denotations in our fragment. In
the new denotations, the only old effect that we mention is the
$\op{scope}$ effect that we handle with $\SI$ to make a claim that
appositives form scope islands.

Furthermore, adding quantification did not change the denotation of
existing noun phrases. We did not have to type raise $\sem{\abs{John}}$ and
$\sem{\abs{me}}$. We only added $\SI$ to existing places in the grammar
which we wanted to act as scope islands.

The goal of our calculus is to enable the creation of semantic lexicons
with a high degree of separation of concerns. In this section, we have seen
how it can be done for one particular fragment.


\section{Properties of the Calculus}
\label{sec:properties}


\section{Conclusion}
\label{sec:conclusion}

%
% ---- Bibliography ----
%
\bibliographystyle{splncs03}
\bibliography{references}

\end{document}
